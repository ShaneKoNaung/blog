[
  {
    "objectID": "posts/pandas/2.2-Data_Preprocessing.html",
    "href": "posts/pandas/2.2-Data_Preprocessing.html",
    "title": "Data Preprocessing with Pandas",
    "section": "",
    "text": "Notes for D2L Chapter 2 Section 2"
  },
  {
    "objectID": "posts/pandas/2.2-Data_Preprocessing.html#data-preparation",
    "href": "posts/pandas/2.2-Data_Preprocessing.html#data-preparation",
    "title": "Data Preprocessing with Pandas",
    "section": "Data Preparation",
    "text": "Data Preparation\nIn supervised learning, we train models to predict a target value, given some sets of input values. Our first step in preprocessing the dataset is to separate out columns corresponding to input and columns corresponding to target values. We can select columns either\n- by name or\n- by using integer-location based indexing (iloc)\n\ninputs, targets = data.iloc[:, 0:2], data.iloc[:, 2]\nprint(f\"Inputs \\n {inputs}\\n\")\nprint(f\"Targets \\n {targets}\")\n\nInputs \n    NumRooms RoofType\n0       NaN      NaN\n1       2.0      NaN\n2       4.0    Slate\n3       NaN      NaN\n\nTargets \n 0    127500\n1    106000\n2    178100\n3    140000\nName: Price, dtype: int64\n\n\nPandas replaced all CSV entries with value NA with a special NaN ( not a number ) value. These are called “missing values”. Depending on the context, missing values can be handled either via imputation or deletion.\n\nImputation : replace missing values with estimates of their values\nDeletion : simply discard the columns or rows that contain missing values\n\nFor categorical input fields like “RoofType”, we can treat NaN as a category.\nWe can use pd.get_dummies() to convert this column into two separate columns.”RootType” column is now separated into “RoofType_Slate” and “RootType_nan”.\n\nprint('Before Imputation')\nprint(inputs)\n\nprint(\"\\nAfter Category Imputation\")\ninputs = pd.get_dummies(inputs, columns=['RoofType'], dummy_na=True, dtype=int)\nprint(inputs)\n\nBefore Imputation\n   NumRooms RoofType\n0       NaN      NaN\n1       2.0      NaN\n2       4.0    Slate\n3       NaN      NaN\n\nAfter Category Imputation\n   NumRooms  RoofType_Slate  RoofType_nan\n0       NaN               0             1\n1       2.0               0             1\n2       4.0               1             0\n3       NaN               0             1\n\n\nFor numerical missing value, one common way is to replace NaN entries with the mean value of the corresponding column.\n\ninputs = inputs.fillna(inputs.mean())\nprint(inputs)\n\n   NumRooms  RoofType_Slate  RoofType_nan\n0       3.0               0             1\n1       2.0               0             1\n2       4.0               1             0\n3       3.0               0             1"
  },
  {
    "objectID": "posts/pandas/2.2-Data_Preprocessing.html#conversion-to-the-tensor-format",
    "href": "posts/pandas/2.2-Data_Preprocessing.html#conversion-to-the-tensor-format",
    "title": "Data Preprocessing with Pandas",
    "section": "Conversion to the tensor format",
    "text": "Conversion to the tensor format\nNow that we have separated inputs and targets columns, we can load them into a tensor.\n\nimport torch\n\nX, y = torch.tensor(inputs.values), torch.tensor(targets.values)\nX, y\n\n(tensor([[3., 0., 1.],\n         [2., 0., 1.],\n         [4., 1., 0.],\n         [3., 0., 1.]], dtype=torch.float64),\n tensor([127500, 106000, 178100, 140000]))\n\n\nNotes for 10 minutes to pands"
  },
  {
    "objectID": "posts/pandas/2.2-Data_Preprocessing.html#object-creation",
    "href": "posts/pandas/2.2-Data_Preprocessing.html#object-creation",
    "title": "Data Preprocessing with Pandas",
    "section": "Object Creation",
    "text": "Object Creation\nCreating a Series by passing a list of values\n\ns = pd.Series([1, 3, 5, np.nan, 6, 8])\n\nCreating a DataFrame by passing a NumPy array, with a datetime index using date_range()\n\n1 + 1\n\n2\n\n\n\ndates = pd.date_range(\"20230719\", periods=6)\n\nprint(dates)\n\nDatetimeIndex(['2023-07-19', '2023-07-20', '2023-07-21', '2023-07-22',\n               '2023-07-23', '2023-07-24'],\n              dtype='datetime64[ns]', freq='D')\n\n\n\ndf = pd.DataFrame(np.random.randn(6,4), index=dates, columns=list(\"ABCD\"))\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2023-07-19\n-0.882153\n-0.519798\n1.191473\n-0.365119\n\n\n2023-07-20\n-0.743235\n-1.251854\n-1.297950\n0.837160\n\n\n2023-07-21\n-1.441845\n1.072803\n1.595452\n0.279867\n\n\n2023-07-22\n0.837404\n-1.051493\n-0.894736\n0.117740\n\n\n2023-07-23\n0.110210\n-0.240610\n1.276668\n-0.659641\n\n\n2023-07-24\n0.542348\n-1.661006\n0.360272\n-0.452072\n\n\n\n\n\n\n\nCreating a DataFrame by passing a dictionary of objects\n\ndf2 = pd.DataFrame(\n    {\n        \"A\" : 1.,\n        \"B\" : pd.Timestamp(\"20230721\"),\n        \"C\" : pd.Series(1, index=list(range(6)), dtype=\"float32\"),\n        \"D\" : np.array([3] * 6, dtype=\"int32\"),\n        \"E\" : pd.Categorical([\"test\", \"train\",\"test\", \"train\", \"test\", \"train\"]),\n        \"F\" : \"foo\",\n    }\n)\n\ndf2\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\n\n\n\n\n0\n1.0\n2023-07-21\n1.0\n3\ntest\nfoo\n\n\n1\n1.0\n2023-07-21\n1.0\n3\ntrain\nfoo\n\n\n2\n1.0\n2023-07-21\n1.0\n3\ntest\nfoo\n\n\n3\n1.0\n2023-07-21\n1.0\n3\ntrain\nfoo\n\n\n4\n1.0\n2023-07-21\n1.0\n3\ntest\nfoo\n\n\n5\n1.0\n2023-07-21\n1.0\n3\ntrain\nfoo\n\n\n\n\n\n\n\n\ndf2.dtypes\n\nA           float64\nB    datetime64[ns]\nC           float32\nD             int32\nE          category\nF            object\ndtype: object"
  },
  {
    "objectID": "posts/pandas/2.2-Data_Preprocessing.html#viewing-data",
    "href": "posts/pandas/2.2-Data_Preprocessing.html#viewing-data",
    "title": "Data Preprocessing with Pandas",
    "section": "Viewing Data",
    "text": "Viewing Data\n\nDataFrame.head() : to view top rows\nDataFrame.tail() : to view bottom rows\n\n\ndf.head()\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2023-07-19\n-0.882153\n-0.519798\n1.191473\n-0.365119\n\n\n2023-07-20\n-0.743235\n-1.251854\n-1.297950\n0.837160\n\n\n2023-07-21\n-1.441845\n1.072803\n1.595452\n0.279867\n\n\n2023-07-22\n0.837404\n-1.051493\n-0.894736\n0.117740\n\n\n2023-07-23\n0.110210\n-0.240610\n1.276668\n-0.659641\n\n\n\n\n\n\n\n\ndf.tail(3)\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2023-07-22\n0.837404\n-1.051493\n-0.894736\n0.117740\n\n\n2023-07-23\n0.110210\n-0.240610\n1.276668\n-0.659641\n\n\n2023-07-24\n0.542348\n-1.661006\n0.360272\n-0.452072\n\n\n\n\n\n\n\n\ndf.index\n\nDatetimeIndex(['2023-07-19', '2023-07-20', '2023-07-21', '2023-07-22',\n               '2023-07-23', '2023-07-24'],\n              dtype='datetime64[ns]', freq='D')\n\n\n\ndf.columns\n\nIndex(['A', 'B', 'C', 'D'], dtype='object')\n\n\nDataFrame.to_numpy() gives NumPy representation of the underlying data.\nIt can be expensive operation if DataFrame has columns with different data types.\nNumPy arrays have only one dtype for entire array, while Pandas DataFrames have one dtype per column.\nWhen we call DataFrame.to_numpy(), pandas will find the Numpy dtype that can hold all of the dtypes in the DataFrame, which probably might end up being object.\n\n\n\n\n\n\nSide Note about %time and %%time\n\n\n\n\n\n%time measures execution time of the next line.\n%%time measures execution time of the whole cell.\nDifference between %time and %%time\n\n\n\n\n%%time\ndf.to_numpy()\n\nCPU times: user 23 µs, sys: 14 µs, total: 37 µs\nWall time: 42.9 µs\n\n\narray([[-0.88215262, -0.51979766,  1.19147342, -0.36511911],\n       [-0.74323485, -1.25185408, -1.29795001,  0.83716014],\n       [-1.44184489,  1.07280347,  1.59545244,  0.27986697],\n       [ 0.83740402, -1.05149342, -0.89473616,  0.11773951],\n       [ 0.11020985, -0.24061037,  1.27666783, -0.65964116],\n       [ 0.54234772, -1.66100567,  0.36027161, -0.45207181]])\n\n\n\n%%time\ndf2.to_numpy()\n\nCPU times: user 552 µs, sys: 327 µs, total: 879 µs\nWall time: 915 µs\n\n\narray([[1.0, Timestamp('2023-07-21 00:00:00'), 1.0, 3, 'test', 'foo'],\n       [1.0, Timestamp('2023-07-21 00:00:00'), 1.0, 3, 'train', 'foo'],\n       [1.0, Timestamp('2023-07-21 00:00:00'), 1.0, 3, 'test', 'foo'],\n       [1.0, Timestamp('2023-07-21 00:00:00'), 1.0, 3, 'train', 'foo'],\n       [1.0, Timestamp('2023-07-21 00:00:00'), 1.0, 3, 'test', 'foo'],\n       [1.0, Timestamp('2023-07-21 00:00:00'), 1.0, 3, 'train', 'foo']],\n      dtype=object)\n\n\nDataFrame.to_numpy() does not include the index or column labels in the output.\ndescribe() shows a quick statistics summary of data.\n\ndf.describe()\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\ncount\n6.000000\n6.000000\n6.000000\n6.000000\n\n\nmean\n-0.262878\n-0.608660\n0.371863\n-0.040344\n\n\nstd\n0.894697\n0.968288\n1.214931\n0.557865\n\n\nmin\n-1.441845\n-1.661006\n-1.297950\n-0.659641\n\n\n25%\n-0.847423\n-1.201764\n-0.580984\n-0.430334\n\n\n50%\n-0.316512\n-0.785646\n0.775873\n-0.123690\n\n\n75%\n0.434313\n-0.310407\n1.255369\n0.239335\n\n\nmax\n0.837404\n1.072803\n1.595452\n0.837160\n\n\n\n\n\n\n\nTransposing data\n\ndf.T\n\n\n\n\n\n\n\n\n2023-07-19\n2023-07-20\n2023-07-21\n2023-07-22\n2023-07-23\n2023-07-24\n\n\n\n\nA\n-0.882153\n-0.743235\n-1.441845\n0.837404\n0.110210\n0.542348\n\n\nB\n-0.519798\n-1.251854\n1.072803\n-1.051493\n-0.240610\n-1.661006\n\n\nC\n1.191473\n-1.297950\n1.595452\n-0.894736\n1.276668\n0.360272\n\n\nD\n-0.365119\n0.837160\n0.279867\n0.117740\n-0.659641\n-0.452072\n\n\n\n\n\n\n\nDataFrame.sort_index() sorts by an index.\n\ndf.sort_index(axis=1, ascending=False)\n\n\n\n\n\n\n\n\nD\nC\nB\nA\n\n\n\n\n2023-07-19\n-0.365119\n1.191473\n-0.519798\n-0.882153\n\n\n2023-07-20\n0.837160\n-1.297950\n-1.251854\n-0.743235\n\n\n2023-07-21\n0.279867\n1.595452\n1.072803\n-1.441845\n\n\n2023-07-22\n0.117740\n-0.894736\n-1.051493\n0.837404\n\n\n2023-07-23\n-0.659641\n1.276668\n-0.240610\n0.110210\n\n\n2023-07-24\n-0.452072\n0.360272\n-1.661006\n0.542348\n\n\n\n\n\n\n\nDataFrame.sort_values() sorts by values.\n\ndf.sort_values(by=\"D\")\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2023-07-23\n0.110210\n-0.240610\n1.276668\n-0.659641\n\n\n2023-07-24\n0.542348\n-1.661006\n0.360272\n-0.452072\n\n\n2023-07-19\n-0.882153\n-0.519798\n1.191473\n-0.365119\n\n\n2023-07-22\n0.837404\n-1.051493\n-0.894736\n0.117740\n\n\n2023-07-21\n-1.441845\n1.072803\n1.595452\n0.279867\n\n\n2023-07-20\n-0.743235\n-1.251854\n-1.297950\n0.837160"
  },
  {
    "objectID": "posts/pandas/2.2-Data_Preprocessing.html#selection",
    "href": "posts/pandas/2.2-Data_Preprocessing.html#selection",
    "title": "Data Preprocessing with Pandas",
    "section": "Selection",
    "text": "Selection\n\nGetting\nSelecting a single column - df[“A”] - df.A\n\ndf[\"A\"]\n\n2023-07-19   -0.882153\n2023-07-20   -0.743235\n2023-07-21   -1.441845\n2023-07-22    0.837404\n2023-07-23    0.110210\n2023-07-24    0.542348\nFreq: D, Name: A, dtype: float64\n\n\nSelecting via [] (__getitem__)\n\ndf[0:3]\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2023-07-19\n-0.882153\n-0.519798\n1.191473\n-0.365119\n\n\n2023-07-20\n-0.743235\n-1.251854\n-1.297950\n0.837160\n\n\n2023-07-21\n-1.441845\n1.072803\n1.595452\n0.279867\n\n\n\n\n\n\n\n\ndf[\"20230719\":\"20230722\"]\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2023-07-19\n-0.882153\n-0.519798\n1.191473\n-0.365119\n\n\n2023-07-20\n-0.743235\n-1.251854\n-1.297950\n0.837160\n\n\n2023-07-21\n-1.441845\n1.072803\n1.595452\n0.279867\n\n\n2023-07-22\n0.837404\n-1.051493\n-0.894736\n0.117740\n\n\n\n\n\n\n\n\n\nSelection by label\nFor getting a cross section using a label:\n\ndf.loc[dates[0]]\n\nA   -0.882153\nB   -0.519798\nC    1.191473\nD   -0.365119\nName: 2023-07-19 00:00:00, dtype: float64\n\n\nSelecting on a multi-axis by label\n\ndf.loc[:, [\"A\", \"B\"]]\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n2023-07-19\n-0.882153\n-0.519798\n\n\n2023-07-20\n-0.743235\n-1.251854\n\n\n2023-07-21\n-1.441845\n1.072803\n\n\n2023-07-22\n0.837404\n-1.051493\n\n\n2023-07-23\n0.110210\n-0.240610\n\n\n2023-07-24\n0.542348\n-1.661006\n\n\n\n\n\n\n\nshowing label slicing, both endpoints are included\n\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2023-07-19\n-0.882153\n-0.519798\n1.191473\n-0.365119\n\n\n2023-07-20\n-0.743235\n-1.251854\n-1.297950\n0.837160\n\n\n2023-07-21\n-1.441845\n1.072803\n1.595452\n0.279867\n\n\n2023-07-22\n0.837404\n-1.051493\n-0.894736\n0.117740\n\n\n2023-07-23\n0.110210\n-0.240610\n1.276668\n-0.659641\n\n\n2023-07-24\n0.542348\n-1.661006\n0.360272\n-0.452072\n\n\n\n\n\n\n\n\ndf.loc[\"20230719\" : \"20230722\", [\"A\", \"B\"]]\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n2023-07-19\n-0.882153\n-0.519798\n\n\n2023-07-20\n-0.743235\n-1.251854\n\n\n2023-07-21\n-1.441845\n1.072803\n\n\n2023-07-22\n0.837404\n-1.051493\n\n\n\n\n\n\n\nReduction in the dimensions of the returned object\n\ndf.loc[\"20230722\", [\"A\", \"B\"]]\n\nA    0.837404\nB   -1.051493\nName: 2023-07-22 00:00:00, dtype: float64\n\n\nFor getting a scalar value\n\n%%time\ndf.loc[\"20230722\", \"A\"]\n\nCPU times: user 215 µs, sys: 124 µs, total: 339 µs\nWall time: 373 µs\n\n\n0.8374040236752622\n\n\nFor getting fast access to scalar, same as the previous method\n\n%%time\ndf.at[\"20230722\", \"A\"]\n\nCPU times: user 116 µs, sys: 68 µs, total: 184 µs\nWall time: 205 µs\n\n\n0.8374040236752622\n\n\n\n\nSelection by position\nSelect via the position of the passed integers\n\ndf.iloc[3]\n\nA    0.837404\nB   -1.051493\nC   -0.894736\nD    0.117740\nName: 2023-07-22 00:00:00, dtype: float64\n\n\n\ndf.iloc[3:5, 0:2]\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n2023-07-22\n0.837404\n-1.051493\n\n\n2023-07-23\n0.110210\n-0.240610\n\n\n\n\n\n\n\nby list of integer positions, similar to NumPy\n\ndf.iloc[[1,2,4], [0,2]]\n\n\n\n\n\n\n\n\nA\nC\n\n\n\n\n2023-07-20\n-0.743235\n-1.297950\n\n\n2023-07-21\n-1.441845\n1.595452\n\n\n2023-07-23\n0.110210\n1.276668\n\n\n\n\n\n\n\nFor Slicing rows explicitly\n\ndf.iloc[1:3, :]\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2023-07-20\n-0.743235\n-1.251854\n-1.297950\n0.837160\n\n\n2023-07-21\n-1.441845\n1.072803\n1.595452\n0.279867\n\n\n\n\n\n\n\nFor slicing columns, explicitly\n\ndf.iloc[:, 1:3]\n\n\n\n\n\n\n\n\nB\nC\n\n\n\n\n2023-07-19\n-0.519798\n1.191473\n\n\n2023-07-20\n-1.251854\n-1.297950\n\n\n2023-07-21\n1.072803\n1.595452\n\n\n2023-07-22\n-1.051493\n-0.894736\n\n\n2023-07-23\n-0.240610\n1.276668\n\n\n2023-07-24\n-1.661006\n0.360272\n\n\n\n\n\n\n\nFor getting a value explicitly\n\n%time df.iloc[1,1]\n\nCPU times: user 74 µs, sys: 42 µs, total: 116 µs\nWall time: 128 µs\n\n\n-1.251854084728132\n\n\nFor getting fast access,\n\n%time df.iat[1,1]\n\nCPU times: user 109 µs, sys: 0 ns, total: 109 µs\nWall time: 121 µs\n\n\n-1.251854084728132\n\n\n\n\nBoolean Indexing\nUsing single column’s values to select data\n\ndf[df[\"B\"] &gt; 0]\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2023-07-21\n-1.441845\n1.072803\n1.595452\n0.279867\n\n\n\n\n\n\n\nSelecting values from a DataFrame where a boolean condition is met:\n\ndf[df &gt; 0]\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2023-07-19\nNaN\nNaN\n1.191473\nNaN\n\n\n2023-07-20\nNaN\nNaN\nNaN\n0.837160\n\n\n2023-07-21\nNaN\n1.072803\n1.595452\n0.279867\n\n\n2023-07-22\n0.837404\nNaN\nNaN\n0.117740\n\n\n2023-07-23\n0.110210\nNaN\n1.276668\nNaN\n\n\n2023-07-24\n0.542348\nNaN\n0.360272\nNaN\n\n\n\n\n\n\n\nUsing isin() method for filtering\n\ndf2 = df.copy()\n\ndf2[\"E\"] = [\"one\", \"one\", \"two\", \"three\", \"four\", \"three\"]\n\ndf2\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n2023-07-19\n-0.882153\n-0.519798\n1.191473\n-0.365119\none\n\n\n2023-07-20\n-0.743235\n-1.251854\n-1.297950\n0.837160\none\n\n\n2023-07-21\n-1.441845\n1.072803\n1.595452\n0.279867\ntwo\n\n\n2023-07-22\n0.837404\n-1.051493\n-0.894736\n0.117740\nthree\n\n\n2023-07-23\n0.110210\n-0.240610\n1.276668\n-0.659641\nfour\n\n\n2023-07-24\n0.542348\n-1.661006\n0.360272\n-0.452072\nthree\n\n\n\n\n\n\n\n\ndf2[df2[\"E\"].isin([\"two\", \"four\"])]\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n2023-07-21\n-1.441845\n1.072803\n1.595452\n0.279867\ntwo\n\n\n2023-07-23\n0.110210\n-0.240610\n1.276668\n-0.659641\nfour\n\n\n\n\n\n\n\n\n\nSetting\nSetting a new column automatically aligns the data by the indexes\n\ns1 = pd.Series([1,2,3,4,5,6], index=pd.date_range(\"20230719\", periods=6))\ns1\n\n2023-07-19    1\n2023-07-20    2\n2023-07-21    3\n2023-07-22    4\n2023-07-23    5\n2023-07-24    6\nFreq: D, dtype: int64\n\n\n\ndf[\"F\"] = s1\n\nSetting values by label\n\ndf.at[dates[0], \"A\"] = 0\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nF\n\n\n\n\n2023-07-19\n0.000000\n-0.519798\n1.191473\n-0.365119\n1\n\n\n2023-07-20\n-0.743235\n-1.251854\n-1.297950\n0.837160\n2\n\n\n2023-07-21\n-1.441845\n1.072803\n1.595452\n0.279867\n3\n\n\n2023-07-22\n0.837404\n-1.051493\n-0.894736\n0.117740\n4\n\n\n2023-07-23\n0.110210\n-0.240610\n1.276668\n-0.659641\n5\n\n\n2023-07-24\n0.542348\n-1.661006\n0.360272\n-0.452072\n6\n\n\n\n\n\n\n\nSetting values by position\n\ndf.iat[0, 1] = 0\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nF\n\n\n\n\n2023-07-19\n0.000000\n0.000000\n1.191473\n-0.365119\n1\n\n\n2023-07-20\n-0.743235\n-1.251854\n-1.297950\n0.837160\n2\n\n\n2023-07-21\n-1.441845\n1.072803\n1.595452\n0.279867\n3\n\n\n2023-07-22\n0.837404\n-1.051493\n-0.894736\n0.117740\n4\n\n\n2023-07-23\n0.110210\n-0.240610\n1.276668\n-0.659641\n5\n\n\n2023-07-24\n0.542348\n-1.661006\n0.360272\n-0.452072\n6\n\n\n\n\n\n\n\nSetting by assigning with a NumPy array\n\ndf.loc[:, \"D\"] = np.array([5] * len(df))\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nF\n\n\n\n\n2023-07-19\n0.000000\n0.000000\n1.191473\n5.0\n1\n\n\n2023-07-20\n-0.743235\n-1.251854\n-1.297950\n5.0\n2\n\n\n2023-07-21\n-1.441845\n1.072803\n1.595452\n5.0\n3\n\n\n2023-07-22\n0.837404\n-1.051493\n-0.894736\n5.0\n4\n\n\n2023-07-23\n0.110210\n-0.240610\n1.276668\n5.0\n5\n\n\n2023-07-24\n0.542348\n-1.661006\n0.360272\n5.0\n6\n\n\n\n\n\n\n\nA where operation with setting\n\ndf2 = df.copy()\n\ndf2[df2 &gt; 0] = -df2\n\ndf2\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nF\n\n\n\n\n2023-07-19\n0.000000\n0.000000\n-1.191473\n-5.0\n-1\n\n\n2023-07-20\n-0.743235\n-1.251854\n-1.297950\n-5.0\n-2\n\n\n2023-07-21\n-1.441845\n-1.072803\n-1.595452\n-5.0\n-3\n\n\n2023-07-22\n-0.837404\n-1.051493\n-0.894736\n-5.0\n-4\n\n\n2023-07-23\n-0.110210\n-0.240610\n-1.276668\n-5.0\n-5\n\n\n2023-07-24\n-0.542348\n-1.661006\n-0.360272\n-5.0\n-6"
  },
  {
    "objectID": "posts/pandas/2.2-Data_Preprocessing.html#missing-data",
    "href": "posts/pandas/2.2-Data_Preprocessing.html#missing-data",
    "title": "Data Preprocessing with Pandas",
    "section": "Missing data",
    "text": "Missing data\npandas primarily uses the value np.nan to represent missing data.\nReindexing allows us to change/add/delete the index on a specified axis.\n\ndf1 = df.reindex(index=dates[:4], columns=list(df.columns) + [\"E\"])\ndf1.loc[dates[0]: dates[1], \"E\"] = 1\n\ndf1\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nF\nE\n\n\n\n\n2023-07-19\n0.000000\n0.000000\n1.191473\n5.0\n1\n1.0\n\n\n2023-07-20\n-0.743235\n-1.251854\n-1.297950\n5.0\n2\n1.0\n\n\n2023-07-21\n-1.441845\n1.072803\n1.595452\n5.0\n3\nNaN\n\n\n2023-07-22\n0.837404\n-1.051493\n-0.894736\n5.0\n4\nNaN\n\n\n\n\n\n\n\nDataFrame.dropna() drops any rows that have missing data\n\ndf1.dropna(how=\"any\")\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nF\nE\n\n\n\n\n2023-07-19\n0.000000\n0.000000\n1.191473\n5.0\n1\n1.0\n\n\n2023-07-20\n-0.743235\n-1.251854\n-1.297950\n5.0\n2\n1.0\n\n\n\n\n\n\n\nDataFrame.fillna() fills missing data\n\ndf1.fillna(value=5)\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nF\nE\n\n\n\n\n2023-07-19\n0.000000\n0.000000\n1.191473\n5.0\n1\n1.0\n\n\n2023-07-20\n-0.743235\n-1.251854\n-1.297950\n5.0\n2\n1.0\n\n\n2023-07-21\n-1.441845\n1.072803\n1.595452\n5.0\n3\n5.0\n\n\n2023-07-22\n0.837404\n-1.051493\n-0.894736\n5.0\n4\n5.0\n\n\n\n\n\n\n\nisna() gets the boolean mask where values are nan.\n\npd.isna(df1)\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nF\nE\n\n\n\n\n2023-07-19\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2023-07-20\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2023-07-21\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n2023-07-22\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue"
  },
  {
    "objectID": "posts/pandas/2.2-Data_Preprocessing.html#operations",
    "href": "posts/pandas/2.2-Data_Preprocessing.html#operations",
    "title": "Data Preprocessing with Pandas",
    "section": "Operations",
    "text": "Operations\n\nStats\nOperations in general exclude missing data.\n\ndf.mean()\n\nA   -0.115853\nB   -0.522027\nC    0.371863\nD    5.000000\nF    3.500000\ndtype: float64\n\n\n\ndf.mean(1)\n\n2023-07-19    1.438295\n2023-07-20    0.741392\n2023-07-21    1.845282\n2023-07-22    1.578235\n2023-07-23    2.229253\n2023-07-24    2.048323\nFreq: D, dtype: float64\n\n\nFor opearting with objects that have different dimensionality and need alignment, pandas automatically broadcasts along the specified dimension.\n\ndates\n\nDatetimeIndex(['2023-07-19', '2023-07-20', '2023-07-21', '2023-07-22',\n               '2023-07-23', '2023-07-24'],\n              dtype='datetime64[ns]', freq='D')\n\n\nshift : Shift index by desired number of time frequency increments.\nThis method is for shifting the values of datetime-like indexes by a specified time increment a given number of times.\n\ns = pd.Series([1, 3, 5, np.nan, 6, 8], index=dates).shift(2)\n\ns\n\n2023-07-19    NaN\n2023-07-20    NaN\n2023-07-21    1.0\n2023-07-22    3.0\n2023-07-23    5.0\n2023-07-24    NaN\nFreq: D, dtype: float64\n\n\n\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nF\n\n\n\n\n2023-07-19\n0.000000\n0.000000\n1.191473\n5.0\n1\n\n\n2023-07-20\n-0.743235\n-1.251854\n-1.297950\n5.0\n2\n\n\n2023-07-21\n-1.441845\n1.072803\n1.595452\n5.0\n3\n\n\n2023-07-22\n0.837404\n-1.051493\n-0.894736\n5.0\n4\n\n\n2023-07-23\n0.110210\n-0.240610\n1.276668\n5.0\n5\n\n\n2023-07-24\n0.542348\n-1.661006\n0.360272\n5.0\n6\n\n\n\n\n\n\n\n\ndf.sub(s, axis=\"index\")\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nF\n\n\n\n\n2023-07-19\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2023-07-20\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2023-07-21\n-2.441845\n0.072803\n0.595452\n4.0\n2.0\n\n\n2023-07-22\n-2.162596\n-4.051493\n-3.894736\n2.0\n1.0\n\n\n2023-07-23\n-4.889790\n-5.240610\n-3.723332\n0.0\n0.0\n\n\n2023-07-24\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n\nApply\n\ndf.apply(np.cumsum)\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nF\n\n\n\n\n2023-07-19\n0.000000\n0.000000\n1.191473\n5.0\n1\n\n\n2023-07-20\n-0.743235\n-1.251854\n-0.106477\n10.0\n3\n\n\n2023-07-21\n-2.185080\n-0.179051\n1.488976\n15.0\n6\n\n\n2023-07-22\n-1.347676\n-1.230544\n0.594240\n20.0\n10\n\n\n2023-07-23\n-1.237466\n-1.471154\n1.870908\n25.0\n15\n\n\n2023-07-24\n-0.695118\n-3.132160\n2.231179\n30.0\n21\n\n\n\n\n\n\n\n\n\nHistogramming\n\ns = pd.Series(np.random.randint(0, 7, size=10))\n\ns \n\n0    3\n1    0\n2    6\n3    5\n4    1\n5    1\n6    0\n7    1\n8    2\n9    5\ndtype: int64\n\n\n\ns.value_counts()\n\n1    3\n0    2\n5    2\n3    1\n6    1\n2    1\nName: count, dtype: int64\n\n\n\n\nString Methods\n\ns = pd.Series([\"A\", \"B\", \"C\", \"Aaba\", \"Baca\", np.nan, \"CABA\", \"dog\", \"cat\"])\n\ns.str.lower()\n\n0       a\n1       b\n2       c\n3    aaba\n4    baca\n5     NaN\n6    caba\n7     dog\n8     cat\ndtype: object"
  },
  {
    "objectID": "posts/pandas/2.2-Data_Preprocessing.html#merge",
    "href": "posts/pandas/2.2-Data_Preprocessing.html#merge",
    "title": "Data Preprocessing with Pandas",
    "section": "Merge",
    "text": "Merge\n\nConcat\n\ndf = pd.DataFrame(np.random.randn(10, 4))\ndf\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n0.372769\n-0.346441\n-1.706328\n-1.004275\n\n\n1\n-1.511273\n-1.057781\n-1.078231\n0.936687\n\n\n2\n1.236382\n-1.434123\n0.502913\n-0.629513\n\n\n3\n0.885381\n-0.593661\n1.997468\n0.678717\n\n\n4\n-1.434716\n-1.697377\n0.585448\n-0.327443\n\n\n5\n-0.437827\n0.298824\n0.162045\n0.719636\n\n\n6\n-0.215078\n-0.115372\n-0.219809\n1.326141\n\n\n7\n-0.177327\n-0.165627\n-0.789062\n-0.389858\n\n\n8\n0.151773\n-0.342665\n-0.031968\n-0.457642\n\n\n9\n-0.248350\n-0.547784\n0.429245\n-1.644430\n\n\n\n\n\n\n\n\n# break it into pieces\npieces = [df[:3], df[3:7], df[7:]]\npd.concat(pieces)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n0.372769\n-0.346441\n-1.706328\n-1.004275\n\n\n1\n-1.511273\n-1.057781\n-1.078231\n0.936687\n\n\n2\n1.236382\n-1.434123\n0.502913\n-0.629513\n\n\n3\n0.885381\n-0.593661\n1.997468\n0.678717\n\n\n4\n-1.434716\n-1.697377\n0.585448\n-0.327443\n\n\n5\n-0.437827\n0.298824\n0.162045\n0.719636\n\n\n6\n-0.215078\n-0.115372\n-0.219809\n1.326141\n\n\n7\n-0.177327\n-0.165627\n-0.789062\n-0.389858\n\n\n8\n0.151773\n-0.342665\n-0.031968\n-0.457642\n\n\n9\n-0.248350\n-0.547784\n0.429245\n-1.644430\n\n\n\n\n\n\n\n\n\n\n\n\n\nSide Note about Adding\n\n\n\n\n\nAdding a column to a DataFrame is relatively fast. However, adding a row requires a copy and may be expensive.\nRecommand passing a pre-built list of records to the DataFrame constructor instead of building a DataFrame by iteratively appending records to it.\n\n\n\n\n\nJoin\nmerge enables SQL style join types along specific columns.\n\nleft = pd.DataFrame({\"key\" : [\"foo\", \"foo\"], \"lval\" : [1, 2]})\nright = pd.DataFrame({\"key\" : [\"foo\", \"foo\"], \"rval\" : [4, 5]})\n\n\nleft\n\n\n\n\n\n\n\n\nkey\nlval\n\n\n\n\n0\nfoo\n1\n\n\n1\nfoo\n2\n\n\n\n\n\n\n\n\nright\n\n\n\n\n\n\n\n\nkey\nrval\n\n\n\n\n0\nfoo\n4\n\n\n1\nfoo\n5\n\n\n\n\n\n\n\n\npd.merge(left, right, on=\"key\")\n\n\n\n\n\n\n\n\nkey\nlval\nrval\n\n\n\n\n0\nfoo\n1\n4\n\n\n1\nfoo\n1\n5\n\n\n2\nfoo\n2\n4\n\n\n3\nfoo\n2\n5\n\n\n\n\n\n\n\n\nleft = pd.DataFrame({\"key\" : [\"foo\", \"bar\"], \"lval\" : [1, 2]})\nright = pd.DataFrame({\"key\" : [\"foo\", \"bar\"], \"rval\" : [4, 5]})\n\n\nleft\n\n\n\n\n\n\n\n\nkey\nlval\n\n\n\n\n0\nfoo\n1\n\n\n1\nbar\n2\n\n\n\n\n\n\n\n\nright\n\n\n\n\n\n\n\n\nkey\nrval\n\n\n\n\n0\nfoo\n4\n\n\n1\nbar\n5\n\n\n\n\n\n\n\n\npd.merge(left, right, on=\"key\")\n\n\n\n\n\n\n\n\nkey\nlval\nrval\n\n\n\n\n0\nfoo\n1\n4\n\n\n1\nbar\n2\n5"
  },
  {
    "objectID": "posts/pandas/2.2-Data_Preprocessing.html#grouping",
    "href": "posts/pandas/2.2-Data_Preprocessing.html#grouping",
    "title": "Data Preprocessing with Pandas",
    "section": "Grouping",
    "text": "Grouping\nGroup by : a process involving one or more of the following steps: - Splitting - Applying - Combining\n\ndf = pd.DataFrame(\n{\n    \"A\": [\"foo\", \"bar\", \"foo\", \"bar\", \"foo\", \"bar\", \"foo\", \"foo\"],\n    \"B\": [\"one\", \"one\", \"two\", \"three\", \"two\", \"two\", \"one\", \"three\"],\n    \"C\": np.random.randn(8),\n    \"D\": np.random.randn(8),\n})\n\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\nfoo\none\n0.426356\n0.853049\n\n\n1\nbar\none\n0.891177\n0.000043\n\n\n2\nfoo\ntwo\n-0.731032\n1.247819\n\n\n3\nbar\nthree\n1.137938\n0.469195\n\n\n4\nfoo\ntwo\n-0.367243\n-1.080192\n\n\n5\nbar\ntwo\n0.663225\n0.036520\n\n\n6\nfoo\none\n-0.145269\n-2.061565\n\n\n7\nfoo\nthree\n-0.957018\n-1.726286\n\n\n\n\n\n\n\n\ndf.groupby(\"A\")[[\"C\", \"D\"]].sum()\n\n\n\n\n\n\n\n\nC\nD\n\n\nA\n\n\n\n\n\n\nbar\n2.692340\n0.505758\n\n\nfoo\n-1.774205\n-2.767175\n\n\n\n\n\n\n\n\ndf.groupby([\"B\", \"A\"]).sum()\n\n\n\n\n\n\n\n\n\nC\nD\n\n\nB\nA\n\n\n\n\n\n\none\nbar\n0.891177\n0.000043\n\n\nfoo\n0.281087\n-1.208516\n\n\nthree\nbar\n1.137938\n0.469195\n\n\nfoo\n-0.957018\n-1.726286\n\n\ntwo\nbar\n0.663225\n0.036520\n\n\nfoo\n-1.098275\n0.167627"
  },
  {
    "objectID": "posts/pandas/2.2-Data_Preprocessing.html#reshaping",
    "href": "posts/pandas/2.2-Data_Preprocessing.html#reshaping",
    "title": "Data Preprocessing with Pandas",
    "section": "Reshaping",
    "text": "Reshaping\n\ntuples = list(\n    zip(\n        [\"bar\", \"bar\", \"baz\", \"baz\", \"foo\", \"foo\", \"qux\", \"qux\"],\n        [\"one\", \"two\", \"one\", \"two\", \"one\", \"two\", \"one\", \"two\"],\n    )\n)\n\nindex = pd.MultiIndex.from_tuples(tuples, names=[\"first\", \"second\"])\n\ndf = pd.DataFrame(np.random.randn(8, 2), index=index, columns=[\"A\", \"B\"])\n\ndf2 = df[:4]\n\ndf2\n\n\n\n\n\n\n\n\n\nA\nB\n\n\nfirst\nsecond\n\n\n\n\n\n\nbar\none\n0.893822\n1.314928\n\n\ntwo\n-0.679436\n-0.266448\n\n\nbaz\none\n-1.105568\n1.354243\n\n\ntwo\n-0.095790\n0.052857\n\n\n\n\n\n\n\nstack() method compresses a level in the DataFrame’s columns\n\nstacked = df2.stack()\nstacked\n\nfirst  second   \nbar    one     A    0.893822\n               B    1.314928\n       two     A   -0.679436\n               B   -0.266448\nbaz    one     A   -1.105568\n               B    1.354243\n       two     A   -0.095790\n               B    0.052857\ndtype: float64\n\n\n\nstacked.unstack()\n\n\n\n\n\n\n\n\n\nA\nB\n\n\nfirst\nsecond\n\n\n\n\n\n\nbar\none\n0.893822\n1.314928\n\n\ntwo\n-0.679436\n-0.266448\n\n\nbaz\none\n-1.105568\n1.354243\n\n\ntwo\n-0.095790\n0.052857\n\n\n\n\n\n\n\n\nstacked.unstack(1)\n\n\n\n\n\n\n\n\nsecond\none\ntwo\n\n\nfirst\n\n\n\n\n\n\n\nbar\nA\n0.893822\n-0.679436\n\n\nB\n1.314928\n-0.266448\n\n\nbaz\nA\n-1.105568\n-0.095790\n\n\nB\n1.354243\n0.052857\n\n\n\n\n\n\n\n\nstacked.unstack(0)\n\n\n\n\n\n\n\n\nfirst\nbar\nbaz\n\n\nsecond\n\n\n\n\n\n\n\none\nA\n0.893822\n-1.105568\n\n\nB\n1.314928\n1.354243\n\n\ntwo\nA\n-0.679436\n-0.095790\n\n\nB\n-0.266448\n0.052857\n\n\n\n\n\n\n\n\nPivot tables\n\ndf = pd.DataFrame(\n{\n    \"A\": [\"one\", \"one\", \"two\", \"three\"] * 3,\n    \"B\": [\"A\", \"B\", \"C\"] * 4,\n    \"C\": [\"foo\", \"foo\", \"foo\", \"bar\", \"bar\", \"bar\"] * 2,\n    \"D\": np.random.randn(12),\n    \"E\": np.random.randn(12),\n}\n)\n\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n0\none\nA\nfoo\n0.938341\n-0.804973\n\n\n1\none\nB\nfoo\n1.188957\n-0.461107\n\n\n2\ntwo\nC\nfoo\n0.605981\n-0.442691\n\n\n3\nthree\nA\nbar\n-2.038343\n0.779995\n\n\n4\none\nB\nbar\n-1.638382\n-0.257908\n\n\n5\none\nC\nbar\n-0.532004\n0.802269\n\n\n6\ntwo\nA\nfoo\n-1.511639\n-0.005113\n\n\n7\nthree\nB\nfoo\n-0.329843\n0.715273\n\n\n8\none\nC\nfoo\n0.030269\n0.024303\n\n\n9\none\nA\nbar\n0.371548\n-0.906586\n\n\n10\ntwo\nB\nbar\n-0.127591\n0.508265\n\n\n11\nthree\nC\nbar\n-1.179578\n-0.862304\n\n\n\n\n\n\n\n\npd.pivot_table(df, values=\"D\", index=[\"A\", \"B\"], columns=\"C\")\n\n\n\n\n\n\n\n\nC\nbar\nfoo\n\n\nA\nB\n\n\n\n\n\n\none\nA\n0.371548\n0.938341\n\n\nB\n-1.638382\n1.188957\n\n\nC\n-0.532004\n0.030269\n\n\nthree\nA\n-2.038343\nNaN\n\n\nB\nNaN\n-0.329843\n\n\nC\n-1.179578\nNaN\n\n\ntwo\nA\nNaN\n-1.511639\n\n\nB\n-0.127591\nNaN\n\n\nC\nNaN\n0.605981"
  },
  {
    "objectID": "posts/pandas/2.2-Data_Preprocessing.html#time-series",
    "href": "posts/pandas/2.2-Data_Preprocessing.html#time-series",
    "title": "Data Preprocessing with Pandas",
    "section": "Time series",
    "text": "Time series\n\nrng = pd.date_range(\"1/1/2012\", periods=100, freq=\"S\")\n\nts = pd.Series(np.random.randint(0, 500, len(rng)), index=rng)\n\nts.resample(\"5Min\").sum()\n\n2012-01-01    24024\nFreq: 5T, dtype: int64\n\n\n\nts_utc = ts.tz_localize(\"UTC\")\nts_utc\n\n2012-01-01 00:00:00+00:00    367\n2012-01-01 00:00:01+00:00    448\n2012-01-01 00:00:02+00:00    208\n2012-01-01 00:00:03+00:00     72\n2012-01-01 00:00:04+00:00    290\n                            ... \n2012-01-01 00:01:35+00:00    249\n2012-01-01 00:01:36+00:00    352\n2012-01-01 00:01:37+00:00    393\n2012-01-01 00:01:38+00:00     60\n2012-01-01 00:01:39+00:00    460\nFreq: S, Length: 100, dtype: int64\n\n\n\nts_utc.tz_convert(\"US/Eastern\")\n\n2011-12-31 19:00:00-05:00    367\n2011-12-31 19:00:01-05:00    448\n2011-12-31 19:00:02-05:00    208\n2011-12-31 19:00:03-05:00     72\n2011-12-31 19:00:04-05:00    290\n                            ... \n2011-12-31 19:01:35-05:00    249\n2011-12-31 19:01:36-05:00    352\n2011-12-31 19:01:37-05:00    393\n2011-12-31 19:01:38-05:00     60\n2011-12-31 19:01:39-05:00    460\nFreq: S, Length: 100, dtype: int64\n\n\n\nrng = pd.date_range(\"1/1/2012\", periods=5, freq=\"M\")\n\nts = pd.Series(np.random.randn(len(rng)) , index=rng)\nts\n\n2012-01-31   -1.068932\n2012-02-29   -0.237446\n2012-03-31    0.436204\n2012-04-30   -0.254945\n2012-05-31    0.911485\nFreq: M, dtype: float64\n\n\n\nps = ts.to_period()\nps\n\n2012-01   -1.068932\n2012-02   -0.237446\n2012-03    0.436204\n2012-04   -0.254945\n2012-05    0.911485\nFreq: M, dtype: float64\n\n\n\nps.to_timestamp()\n\n2012-01-01   -1.068932\n2012-02-01   -0.237446\n2012-03-01    0.436204\n2012-04-01   -0.254945\n2012-05-01    0.911485\nFreq: MS, dtype: float64\n\n\n\nprng = pd.period_range(\"1990Q1\", \"2000Q4\", freq=\"Q-NOV\")\nts = pd.Series(np.random.randn(len(prng)), prng)\n\nts.index = (prng.asfreq('M', 'e') + 1).asfreq('H', 's') + 9\n\nts.head()\n\n1990-03-01 09:00   -1.267412\n1990-06-01 09:00    1.438059\n1990-09-01 09:00   -0.937723\n1990-12-01 09:00    0.727735\n1991-03-01 09:00    0.653211\nFreq: H, dtype: float64"
  },
  {
    "objectID": "posts/pandas/2.2-Data_Preprocessing.html#categoricals",
    "href": "posts/pandas/2.2-Data_Preprocessing.html#categoricals",
    "title": "Data Preprocessing with Pandas",
    "section": "Categoricals",
    "text": "Categoricals\n\ndf = pd.DataFrame(\n    {\"id\": [1, 2, 3, 4, 5, 6], \"raw_grade\": [\"a\", \"b\", \"b\", \"a\", \"a\", \"e\"]}\n)\n\n\ndf[\"grade\"] = df[\"raw_grade\"].astype(\"category\")\ndf[\"grade\"]\n\n0    a\n1    b\n2    b\n3    a\n4    a\n5    e\nName: grade, dtype: category\nCategories (3, object): ['a', 'b', 'e']\n\n\n\nnew_categories = [\"very good\", \"good\", \"very bad\"]\n\ndf[\"grade\"] = df[\"grade\"].cat.rename_categories(new_categories)\ndf[\"grade\"]\n\n0    very good\n1         good\n2         good\n3    very good\n4    very good\n5     very bad\nName: grade, dtype: category\nCategories (3, object): ['very good', 'good', 'very bad']\n\n\n\ndf[\"grade\"] = df[\"grade\"].cat.set_categories(\n    [\"very bad\", \"bad\", \"medium\", \"good\", \"very good\"]\n)\n\ndf[\"grade\"]\n\n0    very good\n1         good\n2         good\n3    very good\n4    very good\n5     very bad\nName: grade, dtype: category\nCategories (5, object): ['very bad', 'bad', 'medium', 'good', 'very good']\n\n\n\ndf.sort_values(by=\"grade\")\n\n\n\n\n\n\n\n\nid\nraw_grade\ngrade\n\n\n\n\n5\n6\ne\nvery bad\n\n\n1\n2\nb\ngood\n\n\n2\n3\nb\ngood\n\n\n0\n1\na\nvery good\n\n\n3\n4\na\nvery good\n\n\n4\n5\na\nvery good\n\n\n\n\n\n\n\n\ndf.groupby(\"grade\").size()\n\ngrade\nvery bad     1\nbad          0\nmedium       0\ngood         2\nvery good    3\ndtype: int64"
  },
  {
    "objectID": "posts/pandas/2.2-Data_Preprocessing.html#plotting",
    "href": "posts/pandas/2.2-Data_Preprocessing.html#plotting",
    "title": "Data Preprocessing with Pandas",
    "section": "Plotting",
    "text": "Plotting\n\nimport matplotlib.pyplot as plt\n\nplt.close(\"all\")\n\n\nts = pd.Series(np.random.randn(1000), index=pd.date_range(\"1/1/2000\", periods=1000))\n\nts = ts.cumsum()\n\nts.plot()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\ndf = pd.DataFrame(\n    np.random.randn(1000, 4), index=ts.index, columns=[\"A\", \"B\", \"C\", \"D\"]\n)\n\ndf = df.cumsum()\n\nplt.figure()\n\ndf.plot()\n\nplt.legend(loc=\"best\")\n\n&lt;matplotlib.legend.Legend at 0x7ff91a3f3be0&gt;\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;"
  },
  {
    "objectID": "posts/pandas/2.2-Data_Preprocessing.html#importing-and-exporting-data",
    "href": "posts/pandas/2.2-Data_Preprocessing.html#importing-and-exporting-data",
    "title": "Data Preprocessing with Pandas",
    "section": "Importing and exporting data",
    "text": "Importing and exporting data\n\ndf.to_csv(\"foo.csv\")\n\n\npd.read_csv(\"foo.csv\")\n\n\n\n\n\n\n\n\nUnnamed: 0\nA\nB\nC\nD\n\n\n\n\n0\n2000-01-01\n0.306614\n1.025331\n-0.479480\n1.586562\n\n\n1\n2000-01-02\n1.431916\n2.847032\n-0.952482\n1.391025\n\n\n2\n2000-01-03\n0.511170\n2.542530\n-0.373599\n2.282406\n\n\n3\n2000-01-04\n1.114214\n3.759945\n-1.010507\n1.543108\n\n\n4\n2000-01-05\n0.781823\n3.868184\n-2.182871\n0.458372\n\n\n...\n...\n...\n...\n...\n...\n\n\n995\n2002-09-22\n26.756206\n-18.838221\n-18.800867\n12.305153\n\n\n996\n2002-09-23\n26.462995\n-18.795508\n-18.760409\n14.663530\n\n\n997\n2002-09-24\n26.412155\n-18.294482\n-19.939892\n13.161555\n\n\n998\n2002-09-25\n26.166936\n-18.565965\n-19.302396\n13.245883\n\n\n999\n2002-09-26\n25.349671\n-19.351925\n-19.871699\n13.841760\n\n\n\n\n1000 rows × 5 columns\n\n\n\n\nHDF5\n\ndf.to_hdf(\"foo.h5\", \"df\")\n\n\npd.read_hdf(\"foo.h5\", \"df\")\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2000-01-01\n0.306614\n1.025331\n-0.479480\n1.586562\n\n\n2000-01-02\n1.431916\n2.847032\n-0.952482\n1.391025\n\n\n2000-01-03\n0.511170\n2.542530\n-0.373599\n2.282406\n\n\n2000-01-04\n1.114214\n3.759945\n-1.010507\n1.543108\n\n\n2000-01-05\n0.781823\n3.868184\n-2.182871\n0.458372\n\n\n...\n...\n...\n...\n...\n\n\n2002-09-22\n26.756206\n-18.838221\n-18.800867\n12.305153\n\n\n2002-09-23\n26.462995\n-18.795508\n-18.760409\n14.663530\n\n\n2002-09-24\n26.412155\n-18.294482\n-19.939892\n13.161555\n\n\n2002-09-25\n26.166936\n-18.565965\n-19.302396\n13.245883\n\n\n2002-09-26\n25.349671\n-19.351925\n-19.871699\n13.841760\n\n\n\n\n1000 rows × 4 columns\n\n\n\n\n\nExcel\n\ndf.to_excel(\"foo.xlsx\", sheet_name=\"Sheet1\")\n\n\npd.read_excel(\"foo.xlsx\", \"Sheet1\", index_col=None, na_values=[\"NA\"])\n\n\n\n\n\n\n\n\nUnnamed: 0\nA\nB\nC\nD\n\n\n\n\n0\n2000-01-01\n0.306614\n1.025331\n-0.479480\n1.586562\n\n\n1\n2000-01-02\n1.431916\n2.847032\n-0.952482\n1.391025\n\n\n2\n2000-01-03\n0.511170\n2.542530\n-0.373599\n2.282406\n\n\n3\n2000-01-04\n1.114214\n3.759945\n-1.010507\n1.543108\n\n\n4\n2000-01-05\n0.781823\n3.868184\n-2.182871\n0.458372\n\n\n...\n...\n...\n...\n...\n...\n\n\n995\n2002-09-22\n26.756206\n-18.838221\n-18.800867\n12.305153\n\n\n996\n2002-09-23\n26.462995\n-18.795508\n-18.760409\n14.663530\n\n\n997\n2002-09-24\n26.412155\n-18.294482\n-19.939892\n13.161555\n\n\n998\n2002-09-25\n26.166936\n-18.565965\n-19.302396\n13.245883\n\n\n999\n2002-09-26\n25.349671\n-19.351925\n-19.871699\n13.841760\n\n\n\n\n1000 rows × 5 columns"
  },
  {
    "objectID": "posts/linear_algebra/2.3-Linear_Algebra.html",
    "href": "posts/linear_algebra/2.3-Linear_Algebra.html",
    "title": "Gentle Introduction to Linear Algebra",
    "section": "",
    "text": "Notes for Chapter 2 Section 3 of Dive into Deep Learning\nGentle introduction to the most essential concepts, starting from scalar arithmetic and ramping up to matrix multiplication.\n\nimport torch\n\n\n\nScalars are numbers.\nTo convert degree Fahrenheit to Celsius, we can use this equation \\(c = 5/9(f - 32)\\). In this case, 5,9 and 32 are scalars. The variables f and c are unknown scalars.\nScalars are denoted by lower-cased letters (e.g., x, y and z) and the space of all (continuous) real-valued scalars by \\(R\\).\n\\(x \\in R\\) is formal way to say x is real value scalar.\nThe symbol \\(\\in\\) denote memebership of a set.\n\nx = torch.tensor(3.)\ny = torch.tensor(2.)\n\n\nx + y, x * y, x / y, x ** y\n\n(tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))\n\n\n\n\n\nThink of vectors as fixed-length arrays of scalars.\nWhen vectors represent examples from real world datasets, their values hold some real world significance.\n\n\n\n\n\n\n\n\nModel\nVector\nVector components\n\n\n\n\nPredicting risk of a loan defaulting\neach applicants\ntheir incomes,length of employment, number of previous defaults.\n\n\nheart attack risk\neach patient\nmost recent vital signs, cholesterol levels, minutes of exercise per day.\n\n\n\nVectors are denoted by bold lowercase letters (e.g., x, y and z)\n\nx = torch.arange(3)\nx\n\ntensor([0, 1, 2])\n\n\nWe can refer to an element of a vector by using a subscript.For example, x2 is the second element of x which in this case is 1.\nDimensionality of a vector is denoted by $ x R^n $. In code, it is the length of the vector\n\nprint(f\"Dimension of vector x is {len(x)} or {x.shape}\")\n\nDimension of vector x is 3 or torch.Size([3])\n\n\nThe word “dimension” can mean both the number of axes and the number of elements along an axis. To avoid confusion, we will use order to refer to the number of axes and dimensionality to refer to the number of components.\n\n\n\n\nScalars are 0th order tensors.\nVectors are 1st order tensors.\nMatrics are 2nd order tensors.\n\nMatrices are denoted by bold uppercase letters (e.g., X, Y and Z).\nWe will represent them in code by tensors with two axes.\nThe expression \\(A \\in R^{mxn}\\) indicates that the matrix A contains \\(m \\times n\\) real-valued scalars and it has m rows and n columns.\nIf m == n, we call them square matrix.\n\nA = torch.arange(6).reshape(3,2)\nA\n\ntensor([[0, 1],\n        [2, 3],\n        [4, 5]])\n\n\nTranspose : when we exchange a martrix’s rows and columns, the result is called transpose.\nA transpose : \\(A^T\\)\nIf $ A = B^T $, $ aij = bji $ for all i and j.\nThe transpose of an \\(m \\times n\\) matrix is an \\(n \\times m\\) matrix.\n\nA, A.T\n\n(tensor([[0, 1],\n         [2, 3],\n         [4, 5]]),\n tensor([[0, 2, 4],\n         [1, 3, 5]]))\n\n\nSymmetric matrices are a subset of square matrices that are equal to their own transposes.\nIf \\(A == A^T\\), \\(A\\) is a symmetric matrix.\n\nA = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\n\nA == A.T\n\ntensor([[True, True, True],\n        [True, True, True],\n        [True, True, True]])\n\n\n\n\n\nTensors are a generic way to describe extensions to nth-order arrays.\nTensors will become more important when working with images. Each image arrives as 3rd-order tensors with axes corresponding to height, width and channel.\n\ntorch.arange(24).reshape(2,3,4)\n\ntensor([[[ 0,  1,  2,  3],\n         [ 4,  5,  6,  7],\n         [ 8,  9, 10, 11]],\n\n        [[12, 13, 14, 15],\n         [16, 17, 18, 19],\n         [20, 21, 22, 23]]])\n\n\n\n\n\nElementwise operations produce outputs that have the same shape.\n\nA = torch.arange(6, dtype=torch.float32).reshape(2,3)\nB = A.clone()\nA, A+B\n\n(tensor([[0., 1., 2.],\n         [3., 4., 5.]]),\n tensor([[ 0.,  2.,  4.],\n         [ 6.,  8., 10.]]))\n\n\nHadamard Product : The elementwise product of two matrices. \\[A,B \\in R^{mxn}\\]\n\nA * B\n\ntensor([[ 0.,  1.,  4.],\n        [ 9., 16., 25.]])\n\n\nAdding or Multiplying a scalar to a tensor also produces a result with the same shape.\n\na = 2\nX = torch.arange(24).reshape(2,3,4)\na + X , (a * X).shape\n\n(tensor([[[ 2,  3,  4,  5],\n          [ 6,  7,  8,  9],\n          [10, 11, 12, 13]],\n \n         [[14, 15, 16, 17],\n          [18, 19, 20, 21],\n          [22, 23, 24, 25]]]),\n torch.Size([2, 3, 4]))\n\n\n\n\n\nSum of a tensor’s elements. \\[\\Sigma_{i=1}^n x_i\\]\n\nx = torch.arange(3, dtype=torch.float32)\nx, x.sum()\n\n(tensor([0., 1., 2.]), tensor(3.))\n\n\nTo sum over the elements of tensors with arbitrary shape, we simply sum over all of its axes.\n\\[ \\Sigma^m_{i=1} \\Sigma^n_{j=1} a_{ij} \\]\n\nA.shape, A.sum()\n\n(torch.Size([2, 3]), tensor(15.))\n\n\nBy default, sum function reduces a tensor along all of its axes, eventually produce a scalar.\nWe can specify the axes along which the tensor should be reduced.\nsum(axis=0) reduces the tensor along its rows.\n\nA.shape, A.sum(axis=0)\n\n(torch.Size([2, 3]), tensor([3., 5., 7.]))\n\n\nsum(axis=1) reduces the tensor along its column.\n\nA.shape, A.sum(axis=1)\n\n(torch.Size([2, 3]), tensor([ 3., 12.]))\n\n\nReducing a matrix along both row and column is the same as default sum.\n\nA.sum(axis=[0,1]) == A.sum()\n\ntensor(True)\n\n\nmean or average : we get mean by dividing the sum by the total number of elements.\n\nA.mean(), A.sum() / A.numel()\n\n(tensor(2.5000), tensor(2.5000))\n\n\nWe can also calculate mean along specific axis.\n\nA.mean(axis=0), A.sum(axis=0)/ A.shape[0]\n\n(tensor([1.5000, 2.5000, 3.5000]), tensor([1.5000, 2.5000, 3.5000]))\n\n\n\n\n\nSometimes, it is useful to keep the number of axes unchanged when invoking sum or mean.\nThis matters when we want to use the broadcast mechanism.\n\nsum_A = A.sum(axis=1, keepdims=True)\nsum_A, sum_A.shape\n\n(tensor([[ 3.],\n         [12.]]),\n torch.Size([2, 1]))\n\n\nFor example, we can now divide A by sum_A with broadcasting to create a matrix where each row sums up to 1.\n\nA / sum_A\n\ntensor([[0.0000, 0.3333, 0.6667],\n        [0.2500, 0.3333, 0.4167]])\n\n\nWe can calculate cummulative sum of elements by using cumsum.\n\nA.cumsum(axis=0)\n\ntensor([[0., 1., 2.],\n        [3., 5., 7.]])\n\n\n\n\n\nDot Product\nGiven two vectors \\(x,y \\in R^d\\), their dot product \\(x^Ty\\) is a sum over the product of the elements at the same position:\\[ x^Ty = \\Sigma^d_{i=1} x_iy_i\\]\n\ny = torch.ones(3, dtype=torch.float32)\nx, y, torch.dot(x,y)\n\n(tensor([0., 1., 2.]), tensor([1., 1., 1.]), tensor(3.))\n\n\nAlso, we can calculate the dot product of two vectors by performing elementwise multiplication followed by a sum.\n\ntorch.sum(x * y)\n\ntensor(3.)\n\n\nDot product are useful in a wide range of contexts.\n\nGiven some set of values, denoted by vector \\(x \\in R^n\\) and a set of weights, \\(w \\in R^n\\), the weighted sum of the values in \\(x\\) accroding to the weights \\(w\\) could be expressed as the dot product \\(x^Tw\\).\nWhen the weights are non-negative and sum to one, i.e,( $ ^n_{i=1} w_i = 1$ ), the dot product expresses a weighted average.\nAfter normalizing two vectors to have unit length, the dot product expresses the cosine of the angle between them.\n\n\n\n\nTo understand matrix - vector product, let’s start by visualizing $ m n $ matrix \\(A\\) in terms of its row vectors.\n\\[A =\n\\begin{pmatrix}\na^T_1 \\\\\na^T_2 \\\\\n. \\\\\n. \\\\\n. \\\\\na^T_n\n\\end{pmatrix}\\]\nEach \\(a^T\\) is a row vector representing \\(i^{th}\\) row of matrix \\(A\\).\nThe matrix-vector product is a column vector of length \\(m\\), whose \\(i^{th}\\) element is the dot product \\(a^T_ix\\).\n\\[Ax =\n\\begin{pmatrix}\na^T_1 \\\\\na^T_2 \\\\\n. \\\\\n. \\\\\n. \\\\\na^T_n\n\\end{pmatrix} x  = \\begin{pmatrix}\na^T_1 x\\\\\na^T_2 x\\\\\n. \\\\\n. \\\\\n. \\\\\na^T_n x\n\\end{pmatrix}  \\]\nWe can think of multiplication with a matrix \\(A \\in R^{mxn}\\) as a transformation that projects vectors from \\(R^n\\) to \\(R^m\\) .\nIn code, we can use mv function.\nThe column dimension of A (length along axis 1) must be the same as the dimension of x (its length).\nWe can also use @ opeartor.\n\nA.shape, x.shape, torch.mv(A, x), A@x\n\n(torch.Size([2, 3]), torch.Size([3]), tensor([ 5., 14.]), tensor([ 5., 14.]))\n\n\n\n\n\nWe can think of matrix-matrix multiplication \\(AB\\) as performing \\(m\\) matrix-vector products pr \\(m x n\\) dot products and stitching the results together to from an \\(nxm\\) matrix.\n\nB = torch.ones(3,4)\ntorch.mm(A, B), A@B\n\n(tensor([[ 3.,  3.,  3.,  3.],\n         [12., 12., 12., 12.]]),\n tensor([[ 3.,  3.,  3.,  3.],\n         [12., 12., 12., 12.]]))\n\n\n\n\n\nNorm of a vector tells us how big it is.\nA norm is a function \\(||.||\\) that maps a vector to a scalar and satisfies the following three properties:\n\nGiven any vector \\(x\\), if we scale the vector by a scalar \\(\\alpha \\in R\\), its norm scales accrodingly. \\[ || \\alpha x||= |\\alpha|||x||\\].\nFor any vectors \\(x\\) and \\(y\\): norms satisfy the triangle inequality: \\[||x + y|| &lt;= ||x|| + ||y||\\]\nThe norm of a vector is nonnegative and it only vanishes if the vector is zero: \\[||x|| &gt; 0\\] for all \\(x \\neq 0\\).\n\n\\(l_2\\) norm measures the (Euclidean) length of a vector. \\[||x||_2 = \\sqrt {\\Sigma^n_{i=1} x_i^2} \\]\nThe method norm calculates the \\(l_2\\) norm.\n\nu = torch.tensor([3., -4.])\ntorch.norm(u)\n\ntensor(5.)\n\n\n\\(l_1\\) norm is call Manhattan distance. The \\(l_1\\) norm sums the absolute values of a vector’s elements:\\[||x||_1 = \\Sigma^n_{i=1}|x_i|.\\]\nCompared to \\(l_2\\) norm, it is less sensitive to outliers.\n\ntorch.abs(u).sum()\n\ntensor(7.)\n\n\nBoth \\(l_1\\) and \\(l_2\\) are special case of the more general \\(l_p\\) normas: \\[ ||x||_p = {(\\Sigma^n_{i=1} |x_i|^p})^{1/p} \\]\nMatrices’ norms are more complicated. Matrices can be viewed both 1. as collections of individual entries and 2. as objects that operate on vectors and transform them into other vectors\nFrobenius norm\nThe square root of the sum of the squares of a matrix’s elements. \\[ \\sqrt {\\Sigma^m_{i=1}\\Sigma^n_{j=1} x_{ij}^2} \\]\n\ntorch.norm(torch.ones((4,9)))\n\ntensor(6.)\n\n\nSome Intuition about why these concepts are useful\nIn deep learning, we are often trying to solve optimization problems * Maximize the probabiltiy assigned to observed data * Maximize the revenue associated with a recommender system * Minimize the distance between predictions and the ground-truth observations * Minimize the distance between the representation of photos of the same person while Maximizing the distance between the representation of photos of different people.\nThese distances, which constitute the objectives of deep learning algorithms, are often expressed as norms.\n\n\n\n\nScalars, vectors, matrices and tensor are the basic mathematical objects used in linear algebra and have zero, one, two and arbitrary number of axes, respectively.\nTensors can be sliced or reduced along specified axes via indexing, or operations such as sum or mean.\nElementwise products are called Hadamard products. By contrast, dot products, matrix-vector product and matrix-matrix products are not elementwise operations and in general return objects that have different shapes than the operands.\nCompared to Hadamard products, matrix-matrix products are longer to compute (cubic rather than quadratic time).\nNorms capture various notions of magnitude of a vector, and are commonly applied to the difference of two vectors to measure their distance\nCommon vector norms include the \\(l_1\\) and \\(l_2\\) norms, and common matrix norms include the spectral and Frobenius norms."
  },
  {
    "objectID": "posts/linear_algebra/2.3-Linear_Algebra.html#scalars",
    "href": "posts/linear_algebra/2.3-Linear_Algebra.html#scalars",
    "title": "Gentle Introduction to Linear Algebra",
    "section": "",
    "text": "Scalars are numbers.\nTo convert degree Fahrenheit to Celsius, we can use this equation \\(c = 5/9(f - 32)\\). In this case, 5,9 and 32 are scalars. The variables f and c are unknown scalars.\nScalars are denoted by lower-cased letters (e.g., x, y and z) and the space of all (continuous) real-valued scalars by \\(R\\).\n\\(x \\in R\\) is formal way to say x is real value scalar.\nThe symbol \\(\\in\\) denote memebership of a set.\n\nx = torch.tensor(3.)\ny = torch.tensor(2.)\n\n\nx + y, x * y, x / y, x ** y\n\n(tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))"
  },
  {
    "objectID": "posts/linear_algebra/2.3-Linear_Algebra.html#vectors",
    "href": "posts/linear_algebra/2.3-Linear_Algebra.html#vectors",
    "title": "Gentle Introduction to Linear Algebra",
    "section": "",
    "text": "Think of vectors as fixed-length arrays of scalars.\nWhen vectors represent examples from real world datasets, their values hold some real world significance.\n\n\n\n\n\n\n\n\nModel\nVector\nVector components\n\n\n\n\nPredicting risk of a loan defaulting\neach applicants\ntheir incomes,length of employment, number of previous defaults.\n\n\nheart attack risk\neach patient\nmost recent vital signs, cholesterol levels, minutes of exercise per day.\n\n\n\nVectors are denoted by bold lowercase letters (e.g., x, y and z)\n\nx = torch.arange(3)\nx\n\ntensor([0, 1, 2])\n\n\nWe can refer to an element of a vector by using a subscript.For example, x2 is the second element of x which in this case is 1.\nDimensionality of a vector is denoted by $ x R^n $. In code, it is the length of the vector\n\nprint(f\"Dimension of vector x is {len(x)} or {x.shape}\")\n\nDimension of vector x is 3 or torch.Size([3])\n\n\nThe word “dimension” can mean both the number of axes and the number of elements along an axis. To avoid confusion, we will use order to refer to the number of axes and dimensionality to refer to the number of components."
  },
  {
    "objectID": "posts/linear_algebra/2.3-Linear_Algebra.html#matrices",
    "href": "posts/linear_algebra/2.3-Linear_Algebra.html#matrices",
    "title": "Gentle Introduction to Linear Algebra",
    "section": "",
    "text": "Scalars are 0th order tensors.\nVectors are 1st order tensors.\nMatrics are 2nd order tensors.\n\nMatrices are denoted by bold uppercase letters (e.g., X, Y and Z).\nWe will represent them in code by tensors with two axes.\nThe expression \\(A \\in R^{mxn}\\) indicates that the matrix A contains \\(m \\times n\\) real-valued scalars and it has m rows and n columns.\nIf m == n, we call them square matrix.\n\nA = torch.arange(6).reshape(3,2)\nA\n\ntensor([[0, 1],\n        [2, 3],\n        [4, 5]])\n\n\nTranspose : when we exchange a martrix’s rows and columns, the result is called transpose.\nA transpose : \\(A^T\\)\nIf $ A = B^T $, $ aij = bji $ for all i and j.\nThe transpose of an \\(m \\times n\\) matrix is an \\(n \\times m\\) matrix.\n\nA, A.T\n\n(tensor([[0, 1],\n         [2, 3],\n         [4, 5]]),\n tensor([[0, 2, 4],\n         [1, 3, 5]]))\n\n\nSymmetric matrices are a subset of square matrices that are equal to their own transposes.\nIf \\(A == A^T\\), \\(A\\) is a symmetric matrix.\n\nA = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\n\nA == A.T\n\ntensor([[True, True, True],\n        [True, True, True],\n        [True, True, True]])"
  },
  {
    "objectID": "posts/linear_algebra/2.3-Linear_Algebra.html#tensors",
    "href": "posts/linear_algebra/2.3-Linear_Algebra.html#tensors",
    "title": "Gentle Introduction to Linear Algebra",
    "section": "",
    "text": "Tensors are a generic way to describe extensions to nth-order arrays.\nTensors will become more important when working with images. Each image arrives as 3rd-order tensors with axes corresponding to height, width and channel.\n\ntorch.arange(24).reshape(2,3,4)\n\ntensor([[[ 0,  1,  2,  3],\n         [ 4,  5,  6,  7],\n         [ 8,  9, 10, 11]],\n\n        [[12, 13, 14, 15],\n         [16, 17, 18, 19],\n         [20, 21, 22, 23]]])"
  },
  {
    "objectID": "posts/linear_algebra/2.3-Linear_Algebra.html#basic-properties-of-tensor-arithmetic",
    "href": "posts/linear_algebra/2.3-Linear_Algebra.html#basic-properties-of-tensor-arithmetic",
    "title": "Gentle Introduction to Linear Algebra",
    "section": "",
    "text": "Elementwise operations produce outputs that have the same shape.\n\nA = torch.arange(6, dtype=torch.float32).reshape(2,3)\nB = A.clone()\nA, A+B\n\n(tensor([[0., 1., 2.],\n         [3., 4., 5.]]),\n tensor([[ 0.,  2.,  4.],\n         [ 6.,  8., 10.]]))\n\n\nHadamard Product : The elementwise product of two matrices. \\[A,B \\in R^{mxn}\\]\n\nA * B\n\ntensor([[ 0.,  1.,  4.],\n        [ 9., 16., 25.]])\n\n\nAdding or Multiplying a scalar to a tensor also produces a result with the same shape.\n\na = 2\nX = torch.arange(24).reshape(2,3,4)\na + X , (a * X).shape\n\n(tensor([[[ 2,  3,  4,  5],\n          [ 6,  7,  8,  9],\n          [10, 11, 12, 13]],\n \n         [[14, 15, 16, 17],\n          [18, 19, 20, 21],\n          [22, 23, 24, 25]]]),\n torch.Size([2, 3, 4]))"
  },
  {
    "objectID": "posts/linear_algebra/2.3-Linear_Algebra.html#reduction",
    "href": "posts/linear_algebra/2.3-Linear_Algebra.html#reduction",
    "title": "Gentle Introduction to Linear Algebra",
    "section": "",
    "text": "Sum of a tensor’s elements. \\[\\Sigma_{i=1}^n x_i\\]\n\nx = torch.arange(3, dtype=torch.float32)\nx, x.sum()\n\n(tensor([0., 1., 2.]), tensor(3.))\n\n\nTo sum over the elements of tensors with arbitrary shape, we simply sum over all of its axes.\n\\[ \\Sigma^m_{i=1} \\Sigma^n_{j=1} a_{ij} \\]\n\nA.shape, A.sum()\n\n(torch.Size([2, 3]), tensor(15.))\n\n\nBy default, sum function reduces a tensor along all of its axes, eventually produce a scalar.\nWe can specify the axes along which the tensor should be reduced.\nsum(axis=0) reduces the tensor along its rows.\n\nA.shape, A.sum(axis=0)\n\n(torch.Size([2, 3]), tensor([3., 5., 7.]))\n\n\nsum(axis=1) reduces the tensor along its column.\n\nA.shape, A.sum(axis=1)\n\n(torch.Size([2, 3]), tensor([ 3., 12.]))\n\n\nReducing a matrix along both row and column is the same as default sum.\n\nA.sum(axis=[0,1]) == A.sum()\n\ntensor(True)\n\n\nmean or average : we get mean by dividing the sum by the total number of elements.\n\nA.mean(), A.sum() / A.numel()\n\n(tensor(2.5000), tensor(2.5000))\n\n\nWe can also calculate mean along specific axis.\n\nA.mean(axis=0), A.sum(axis=0)/ A.shape[0]\n\n(tensor([1.5000, 2.5000, 3.5000]), tensor([1.5000, 2.5000, 3.5000]))"
  },
  {
    "objectID": "posts/linear_algebra/2.3-Linear_Algebra.html#non-reduction-sum",
    "href": "posts/linear_algebra/2.3-Linear_Algebra.html#non-reduction-sum",
    "title": "Gentle Introduction to Linear Algebra",
    "section": "",
    "text": "Sometimes, it is useful to keep the number of axes unchanged when invoking sum or mean.\nThis matters when we want to use the broadcast mechanism.\n\nsum_A = A.sum(axis=1, keepdims=True)\nsum_A, sum_A.shape\n\n(tensor([[ 3.],\n         [12.]]),\n torch.Size([2, 1]))\n\n\nFor example, we can now divide A by sum_A with broadcasting to create a matrix where each row sums up to 1.\n\nA / sum_A\n\ntensor([[0.0000, 0.3333, 0.6667],\n        [0.2500, 0.3333, 0.4167]])\n\n\nWe can calculate cummulative sum of elements by using cumsum.\n\nA.cumsum(axis=0)\n\ntensor([[0., 1., 2.],\n        [3., 5., 7.]])"
  },
  {
    "objectID": "posts/linear_algebra/2.3-Linear_Algebra.html#dot-product",
    "href": "posts/linear_algebra/2.3-Linear_Algebra.html#dot-product",
    "title": "Gentle Introduction to Linear Algebra",
    "section": "",
    "text": "Dot Product\nGiven two vectors \\(x,y \\in R^d\\), their dot product \\(x^Ty\\) is a sum over the product of the elements at the same position:\\[ x^Ty = \\Sigma^d_{i=1} x_iy_i\\]\n\ny = torch.ones(3, dtype=torch.float32)\nx, y, torch.dot(x,y)\n\n(tensor([0., 1., 2.]), tensor([1., 1., 1.]), tensor(3.))\n\n\nAlso, we can calculate the dot product of two vectors by performing elementwise multiplication followed by a sum.\n\ntorch.sum(x * y)\n\ntensor(3.)\n\n\nDot product are useful in a wide range of contexts.\n\nGiven some set of values, denoted by vector \\(x \\in R^n\\) and a set of weights, \\(w \\in R^n\\), the weighted sum of the values in \\(x\\) accroding to the weights \\(w\\) could be expressed as the dot product \\(x^Tw\\).\nWhen the weights are non-negative and sum to one, i.e,( $ ^n_{i=1} w_i = 1$ ), the dot product expresses a weighted average.\nAfter normalizing two vectors to have unit length, the dot product expresses the cosine of the angle between them."
  },
  {
    "objectID": "posts/linear_algebra/2.3-Linear_Algebra.html#matrix-vector-products",
    "href": "posts/linear_algebra/2.3-Linear_Algebra.html#matrix-vector-products",
    "title": "Gentle Introduction to Linear Algebra",
    "section": "",
    "text": "To understand matrix - vector product, let’s start by visualizing $ m n $ matrix \\(A\\) in terms of its row vectors.\n\\[A =\n\\begin{pmatrix}\na^T_1 \\\\\na^T_2 \\\\\n. \\\\\n. \\\\\n. \\\\\na^T_n\n\\end{pmatrix}\\]\nEach \\(a^T\\) is a row vector representing \\(i^{th}\\) row of matrix \\(A\\).\nThe matrix-vector product is a column vector of length \\(m\\), whose \\(i^{th}\\) element is the dot product \\(a^T_ix\\).\n\\[Ax =\n\\begin{pmatrix}\na^T_1 \\\\\na^T_2 \\\\\n. \\\\\n. \\\\\n. \\\\\na^T_n\n\\end{pmatrix} x  = \\begin{pmatrix}\na^T_1 x\\\\\na^T_2 x\\\\\n. \\\\\n. \\\\\n. \\\\\na^T_n x\n\\end{pmatrix}  \\]\nWe can think of multiplication with a matrix \\(A \\in R^{mxn}\\) as a transformation that projects vectors from \\(R^n\\) to \\(R^m\\) .\nIn code, we can use mv function.\nThe column dimension of A (length along axis 1) must be the same as the dimension of x (its length).\nWe can also use @ opeartor.\n\nA.shape, x.shape, torch.mv(A, x), A@x\n\n(torch.Size([2, 3]), torch.Size([3]), tensor([ 5., 14.]), tensor([ 5., 14.]))"
  },
  {
    "objectID": "posts/linear_algebra/2.3-Linear_Algebra.html#matrix-matrix-multiplication",
    "href": "posts/linear_algebra/2.3-Linear_Algebra.html#matrix-matrix-multiplication",
    "title": "Gentle Introduction to Linear Algebra",
    "section": "",
    "text": "We can think of matrix-matrix multiplication \\(AB\\) as performing \\(m\\) matrix-vector products pr \\(m x n\\) dot products and stitching the results together to from an \\(nxm\\) matrix.\n\nB = torch.ones(3,4)\ntorch.mm(A, B), A@B\n\n(tensor([[ 3.,  3.,  3.,  3.],\n         [12., 12., 12., 12.]]),\n tensor([[ 3.,  3.,  3.,  3.],\n         [12., 12., 12., 12.]]))"
  },
  {
    "objectID": "posts/linear_algebra/2.3-Linear_Algebra.html#norms",
    "href": "posts/linear_algebra/2.3-Linear_Algebra.html#norms",
    "title": "Gentle Introduction to Linear Algebra",
    "section": "",
    "text": "Norm of a vector tells us how big it is.\nA norm is a function \\(||.||\\) that maps a vector to a scalar and satisfies the following three properties:\n\nGiven any vector \\(x\\), if we scale the vector by a scalar \\(\\alpha \\in R\\), its norm scales accrodingly. \\[ || \\alpha x||= |\\alpha|||x||\\].\nFor any vectors \\(x\\) and \\(y\\): norms satisfy the triangle inequality: \\[||x + y|| &lt;= ||x|| + ||y||\\]\nThe norm of a vector is nonnegative and it only vanishes if the vector is zero: \\[||x|| &gt; 0\\] for all \\(x \\neq 0\\).\n\n\\(l_2\\) norm measures the (Euclidean) length of a vector. \\[||x||_2 = \\sqrt {\\Sigma^n_{i=1} x_i^2} \\]\nThe method norm calculates the \\(l_2\\) norm.\n\nu = torch.tensor([3., -4.])\ntorch.norm(u)\n\ntensor(5.)\n\n\n\\(l_1\\) norm is call Manhattan distance. The \\(l_1\\) norm sums the absolute values of a vector’s elements:\\[||x||_1 = \\Sigma^n_{i=1}|x_i|.\\]\nCompared to \\(l_2\\) norm, it is less sensitive to outliers.\n\ntorch.abs(u).sum()\n\ntensor(7.)\n\n\nBoth \\(l_1\\) and \\(l_2\\) are special case of the more general \\(l_p\\) normas: \\[ ||x||_p = {(\\Sigma^n_{i=1} |x_i|^p})^{1/p} \\]\nMatrices’ norms are more complicated. Matrices can be viewed both 1. as collections of individual entries and 2. as objects that operate on vectors and transform them into other vectors\nFrobenius norm\nThe square root of the sum of the squares of a matrix’s elements. \\[ \\sqrt {\\Sigma^m_{i=1}\\Sigma^n_{j=1} x_{ij}^2} \\]\n\ntorch.norm(torch.ones((4,9)))\n\ntensor(6.)\n\n\nSome Intuition about why these concepts are useful\nIn deep learning, we are often trying to solve optimization problems * Maximize the probabiltiy assigned to observed data * Maximize the revenue associated with a recommender system * Minimize the distance between predictions and the ground-truth observations * Minimize the distance between the representation of photos of the same person while Maximizing the distance between the representation of photos of different people.\nThese distances, which constitute the objectives of deep learning algorithms, are often expressed as norms."
  },
  {
    "objectID": "posts/linear_algebra/2.3-Linear_Algebra.html#recap",
    "href": "posts/linear_algebra/2.3-Linear_Algebra.html#recap",
    "title": "Gentle Introduction to Linear Algebra",
    "section": "",
    "text": "Scalars, vectors, matrices and tensor are the basic mathematical objects used in linear algebra and have zero, one, two and arbitrary number of axes, respectively.\nTensors can be sliced or reduced along specified axes via indexing, or operations such as sum or mean.\nElementwise products are called Hadamard products. By contrast, dot products, matrix-vector product and matrix-matrix products are not elementwise operations and in general return objects that have different shapes than the operands.\nCompared to Hadamard products, matrix-matrix products are longer to compute (cubic rather than quadratic time).\nNorms capture various notions of magnitude of a vector, and are commonly applied to the difference of two vectors to measure their distance\nCommon vector norms include the \\(l_1\\) and \\(l_2\\) norms, and common matrix norms include the spectral and Frobenius norms."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "Gentle Introduction to Linear Algebra\n\n\n\n\n\n\n\nlinear algebra\n\n\nd2l\n\n\n\n\n\n\n\n\n\n\n\nJul 21, 2023\n\n\nShane Ko Naung\n\n\n\n\n\n\n  \n\n\n\n\nData Preprocessing with Pandas\n\n\n\n\n\n\n\npandas\n\n\nd2l\n\n\n\n\n\n\n\n\n\n\n\nJul 20, 2023\n\n\nShane Ko Naung\n\n\n\n\n\n\n  \n\n\n\n\nData Manipulation with Pytorch\n\n\n\n\n\n\n\npytorch\n\n\nd2l\n\n\n\n\n\n\n\n\n\n\n\nJul 19, 2023\n\n\nShane Ko Naung\n\n\n\n\n\n\n  \n\n\n\n\nExperiment Tracking with MLflow\n\n\n\n\n\n\n\nmlops\n\n\nmlflow\n\n\n\n\n\n\n\n\n\n\n\nJul 17, 2023\n\n\nShane Ko Naung\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "These are just my notes about MLOps, Machine Learning, Deep Learning and Python.\nMy Name is Shane Ko Naung.I am a machine learning engineer from Myanmar."
  },
  {
    "objectID": "posts/mlflow/experiment_tracking.html",
    "href": "posts/mlflow/experiment_tracking.html",
    "title": "Experiment Tracking with MLflow",
    "section": "",
    "text": "In this tutorial, we are going to train a simple regression model. While training a model, we are going to use an experiment tracking tool called mlflow.\nWhat is mlflow?\nMLflow is a open-source experiment tracking tool. We can use mlflow to track experiments, experiment runs, artifacts related to experiment runs. MLflow has five components:\n- MLflow Tracking\n- MLflow Models\n- MLflow Model Registry\n- MLflow Projects\n- MLflow Recipes\nWe are only going to use Tracking, Models and Model Registry here. You can see the rest here in the mlflow docs.\nThis blog or notebook is the notes for week 2 ( experiment tracking ) lectures of the datatalks mlops zoomcamp.\n\n\nWe can install mlflow using pip or conda.\n\n# !pip install mlflow \n\n# or \n\n# !conda install -c conda-forge mlflow\n\n\n\n\nHere we are going to import a list of libraries that we need for this tutorial.\n\n!python --version\n\nPython 3.9.0\n\n\n\nfrom fastdownload import download_url\nfrom pathlib import Path\nimport pandas as pd\n\nfrom sklearn.linear_model import Lasso, LinearRegression\nfrom sklearn.metrics import mean_squared_error \n\nimport mlflow\nfrom mlflow.models.signature import infer_signature\n\n\n\n\nBefore starting any training or data preprocessing, we start by setting tracking uri and experiment for this mlflow experiment. Tracking uri can be\n- localhost\n- localhost with SQlite\n- localhost with tracking server\n- remote tracking server, backend and artifact stores\nIn this tutorial, we will start with localhost option and we will also use remote tracking server option in the second half of this tutorial.\n\n\n\nIn a localhost setting, the backend and artifact store share a local folder called ./mlruns.\n\nmlflow.set_tracking_uri(\"mlruns\")\nmlflow.set_experiment(\"Experiment-1\")\n\n2023/07/17 18:58:07 INFO mlflow.tracking.fluent: Experiment with name 'Experiment-1' does not exist. Creating a new experiment.\n\n\n&lt;Experiment: artifact_location='/home/shane/mlops-practice/experiment-tracking/mlruns/280918629490989571', creation_time=1689596887482, experiment_id='280918629490989571', last_update_time=1689596887482, lifecycle_stage='active', name='Experiment-1', tags={}&gt;\n\n\nmlflow.set_experiment() is for setting the experiment name and we can see that a new experiment is created.\n\n!tree .\n\n.\n├── env.yaml\n├── experiment_tracking.ipynb\n├── imgs\n│   ├── mlflow_run1.png\n│   ├── mlflow_ui.png\n│   ├── model.png\n│   └── run1_metadata.png\n└── mlruns\n    ├── 0\n    │   └── meta.yaml\n    └── 280918629490989571\n        └── meta.yaml\n\n4 directories, 8 files\n\n\nWe can run mlflow ui in terminal to access, well, mlflow ui.\n:\n\n\n\nThe dataset that we are using for this tutorial is NYC TLC Trip Record dataset.\nWe are going to predict duration of a taxi ride.\n\ndata_path = Path('data')\nif not data_path.exists():\n    data_path.mkdir(exist_ok=True)\n\nFirst, we are creating a data folder data to which we are going to download the datasets.\n\ndef download_data(year : int, month : int, data_path : Path) -&gt; None:\n    '''download nyc green taxi data in parquet form and save it in data_path'''\n    url = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_{year}-{month:0&gt;2}.parquet\"\n    download_url(url, dest=data_path, show_progress=True)\n\ndownload_data(2023, 1, data_path)\ndownload_data(2023, 2, data_path)\n\n\n\n\n\n\n    \n      \n      100.46% [1433600/1427002 00:01&lt;00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.41% [1540096/1533740 00:01&lt;00:00]\n    \n    \n\n\nQuick side note for pyformatting,\n\nmonth = 2\nprint(f\"month == {month} : {month:0&gt;2}\")\n\nmonth = 11\nprint(f\"month == {month} : {month:0&gt;2}\")\n\nmonth == 2 : 02\nmonth == 11 : 11\n\n\n\n!tree .\n\n.\n├── data\n│   ├── green_tripdata_2023-01.parquet\n│   └── green_tripdata_2023-02.parquet\n├── env.yaml\n├── experiment_tracking.ipynb\n├── imgs\n│   ├── mlflow_run1.png\n│   ├── mlflow_ui.png\n│   ├── model.png\n│   └── run1_metadata.png\n└── mlruns\n    ├── 0\n    │   └── meta.yaml\n    └── 280918629490989571\n        └── meta.yaml\n\n5 directories, 10 files\n\n\nDatasets are downloaded using fastdownload library from fastai.\n\n\n\n\ndef read_dataframe(filename: Path)-&gt; pd.DataFrame:\n    df = pd.read_parquet(filename)\n    df[\"duration\"] = df[\"lpep_dropoff_datetime\"] - df[\"lpep_pickup_datetime\"]\n    df.duration = df[\"duration\"].apply(lambda td : td.total_seconds() / 60)\n    df = df[(df.duration  &gt;= 0) & (df.duration &lt;= 60) ]\n\n    categorical_data = [\"PULocationID\", \"DOLocationID\"]\n    df[categorical_data] = df[categorical_data].astype(str)\n    df[\"PU_DO\"] = df['PULocationID'] + \"_\" + df['DOLocationID']\n\n    return df\n\nSince the files are in parquet format, we use pd.read_parquet() method.\n\n# df[\"duration\"] = df[\"lpep_dropoff_datetime\"] - df[\"lpep_pickup_datetime\"]\n# df.duration = df[\"duration\"].apply(lambda td : td.total_seconds() / 60)\n\nWe want to calculate the duration of each trip. The trip duration is calculated by subtracting pickup datetime from dropoff datetime.\nWe also want to get the duration in minute. We get the total seconds and divided by 60.\n\ndf = read_dataframe(\"data/green_tripdata_2023-01.parquet\")\ndf_val = read_dataframe(\"data/green_tripdata_2023-02.parquet\")\n\n\ndef preprocess(df : pd.DataFrame) -&gt; tuple((pd.DataFrame, pd.DataFrame)):\n    categorical = [\"PU_DO\"]\n    numerical   = [\"trip_distance\", \"fare_amount\", \"total_amount\"]\n\n    X = df[categorical + numerical]\n    y = df.duration\n\n    return X, y\n    \n\n\nX, y = preprocess(df)\nX_val, y_val = preprocess(df_val)\n\nAfter loading train and validation dataset and performing preprocessing, we now have features X and targets y.\nNow we can start training the model.\n\n\n\n\ndef train(X, y):\n    model = LinearRegression()\n    model.fit(X,y)\n\n    return model\n\nWe will initialize mlflow run as\n\n# with mlflow.start_run() as run:\n\nand wrap up the training inside of it.\n- `set_tag` : for tracking metadata\n- `log_param` : for logging parameters\n- `log_metric` : for logging metric\nIn the example below, we use set_tag for tracking developer name, log_param for tracking data folder used for training and validation ,and log_metric for tracking validation rmse metric.\nSome other useful methods are :\n- `set_tags` : Log a batch of tags for the current run.\n- `log_params` : Log a batch of params for the current run.\n- `log_artifact` : Log a local file or directory as an artifact of the currently active run.\n- `log_artifacts` : Log all the contents of a local directory as artifacts of the run\n\nwith mlflow.start_run() as run:\n\n    mlflow.set_tag(\"Developer\", \"Shane\")\n    mlflow.log_param(\"Train-data-path\", \"data/green_tripdata_2023-01.parquet\")\n    mlflow.log_param(\"Valid-data-path\", \"data/green_tripdata_2023-02.parquet\")\n    \n    model = train(X, y)\n\n    preds = model.predict(X_val)\n\n    rmse = mean_squared_error(y_val, preds, squared=False)\n\n    mlflow.log_metric('RMSE', rmse)\n    signature = infer_signature(X_val, preds)\n    model_uri = mlflow.sklearn.log_model(model, artifact_path=\"model\", signature=signature).model_uri\n   \n\n\n\n\nfist run\n\n\nThe first run can be seen in the above picture with the name, shivering-panda-266. It is a random run name since we didn’t set a specific run name.\n\n\n\nrun1\n\n\nWe can see the Train-data-path, Valid-data-path in the Parameters section, RMSE in metrics section and Developer in the Tags section.\n\n\n\n\n# signature = infer_signature(X_val, preds)\n# mlflow.sklearn.log_model(model, artifact_path=\"model\", signature=signature)\n\nModel signatures define input and output schemas for MLflow models. Model signature is obtained here using infer_signature.\nWe can log a model using mlflow.&lt;framework&gt;.log_model. In this case, we are using mlflow.sklearn.log_model.\n\n\n\nmodel1\n\n\nSince we add signature parameter, we can see the model input and output schema here. We can also see two ways that we can load the model and make predictions.\n\n\n\nWe can also do auto logging by using mlflow.&lt;framework&gt;.autolog()\n\n# mlflow.sklearn.autolog()\n\nAutologging is known to be compatible with the following package versions: 0.22.1 &lt;= scikit-learn &lt;= 1.2.2. Autologging may not succeed when used with package versions outside of this range.\n\n\n\nWe have saved the model using log_model. Now, we are going to load that model for prediction.\n\nfrom mlflow import MlflowClient\n\nclient = MlflowClient(tracking_uri=\"mlruns\")\n\n\nexperiments = client.search_experiments()\n\nfor experiment in experiments:\n    print(f\"Experiment Name : {experiment.name}\")\n    print(f\"\\tExperiment id :{experiment.experiment_id}\")\n    print(f\"\\tArtifact Location :{experiment.artifact_location}\\n\")\n\nExperiment Name : Experiment-1\n    Experiment id :280918629490989571\n    Artifact Location :/home/shane/mlops-practice/experiment-tracking/mlruns/280918629490989571\n\nExperiment Name : Default\n    Experiment id :0\n    Artifact Location :/home/shane/mlops-practice/experiment-tracking/mlruns/0\n\n\n\nOur experiment’s name is “Experiment-1” and the Experiment id for that is “146920015920581846”.\n\nfor experiment in experiments:\n    if experiment.name == \"Experiment-1\":\n        exp = experiment\n\n\nexp_id = exp.experiment_id\nruns = client.search_runs(experiment_ids=[exp_id])\n\nfor run in runs:\n    print(f\"run name : {run.info.run_name}\")\n    print(f\"\\trun id : {run.info.run_id}\")\n    print(f\"\\trmse : {run.data.metrics['RMSE']}\")\n\nrun name : charming-toad-155\n    run id : 9ed0390d8a4b44dca36f36ee25d20ba4\n    rmse : 6.282692349434918\n\n\nLet’s pick a run_id of the model that we want to load.\n\nrun_id = runs[0].info.run_id\nlogged_model = f'runs:/{run_id}/model'\n\n# Load model as a PyFuncModel.\nloaded_model = mlflow.pyfunc.load_model(logged_model)\n\n# Predict on a Pandas DataFrame.\nimport pandas as pd\nloaded_model.predict(X_val)\n\n2023/07/17 18:59:06 WARNING mlflow.pyfunc: Detected one or more mismatches between the model's dependencies and the current Python environment:\n - mlflow (current: 2.4.2, required: mlflow==2.4)\nTo fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n\n\narray([26.09040939, 16.31353026, 22.14756441, ..., 15.42763737,\n       14.91915344, 12.24746033])\n\n\n\n\n\nModel Registry is a centralized model store, a set of APIs and UI. It provides\n    - model lineage, \n    - model versioning, \n    - stage transition, and \n    - annotations.\n\nmlflow.set_tracking_uri('mlruns/')\n\nmodel_uri = f\"runs:/{run_id}/models\"\nmlflow.register_model(model_uri=model_uri, name=\"nyc-taxi-regressor\")\n\nSuccessfully registered model 'nyc-taxi-regressor'.\n2023/07/17 18:59:10 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: nyc-taxi-regressor, version 1\nCreated version '1' of model 'nyc-taxi-regressor'.\n\n\n&lt;ModelVersion: aliases=[], creation_timestamp=1689596950866, current_stage='None', description=None, last_updated_timestamp=1689596950866, name='nyc-taxi-regressor', run_id='9ed0390d8a4b44dca36f36ee25d20ba4', run_link=None, source='/home/shane/mlops-practice/experiment-tracking/mlruns/280918629490989571/9ed0390d8a4b44dca36f36ee25d20ba4/artifacts/models', status='READY', status_message=None, tags={}, user_id=None, version=1&gt;\n\n\nWe have now registered the model under the name of “nyc-taxi-regressor’.\nWe can see that the version of the model is now 1. Since this is the first model registered under this name.\nBut It doesn’t have any staging information.\n\nmodel_name = \"nyc-taxi-regressor\"\nlatest_versions = client.get_latest_versions(name=model_name)\n\nfor version in latest_versions:\n    print(f\"version: {version.version}, stage: {version.current_stage}\")\n\nversion: 1, stage: None\n\n\nWe can transition the model version and stages using transition_model_version_stage.\n\nmodel_version = 1\nnew_stage = \"Staging\"\nclient.transition_model_version_stage(\n    name=model_name,\n    version=model_version,\n    stage=new_stage,\n    archive_existing_versions=False\n)\n\n&lt;ModelVersion: aliases=[], creation_timestamp=1689596950866, current_stage='Staging', description=None, last_updated_timestamp=1689596955099, name='nyc-taxi-regressor', run_id='9ed0390d8a4b44dca36f36ee25d20ba4', run_link=None, source='/home/shane/mlops-practice/experiment-tracking/mlruns/280918629490989571/9ed0390d8a4b44dca36f36ee25d20ba4/artifacts/models', status='READY', status_message=None, tags={}, user_id=None, version=1&gt;\n\n\n\nmodel_name = \"nyc-taxi-regressor\"\nlatest_versions = client.get_latest_versions(name=model_name)\n\nfor version in latest_versions:\n    print(f\"version: {version.version}, stage: {version.current_stage}\")\n\nversion: 1, stage: Staging\n\n\nNow, we can see that the stage of the model is now Staging.\n\nfrom datetime import datetime\n\ndate = datetime.today().date()\nclient.update_model_version(\n    name=model_name,\n    version=model_version,\n    description=f\"The model version {model_version} was transitioned to {new_stage} on {date}\"\n)\n\n&lt;ModelVersion: aliases=[], creation_timestamp=1689596950866, current_stage='Staging', description='The model version 1 was transitioned to Staging on 2023-07-17', last_updated_timestamp=1689596958723, name='nyc-taxi-regressor', run_id='9ed0390d8a4b44dca36f36ee25d20ba4', run_link=None, source='/home/shane/mlops-practice/experiment-tracking/mlruns/280918629490989571/9ed0390d8a4b44dca36f36ee25d20ba4/artifacts/models', status='READY', status_message=None, tags={}, user_id=None, version=1&gt;\n\n\nWe could also add description of the models like datetime information above."
  },
  {
    "objectID": "posts/mlflow/experiment_tracking.html#install",
    "href": "posts/mlflow/experiment_tracking.html#install",
    "title": "Experiment Tracking with MLflow",
    "section": "",
    "text": "We can install mlflow using pip or conda.\n\n# !pip install mlflow \n\n# or \n\n# !conda install -c conda-forge mlflow"
  },
  {
    "objectID": "posts/mlflow/experiment_tracking.html#imports",
    "href": "posts/mlflow/experiment_tracking.html#imports",
    "title": "Experiment Tracking with MLflow",
    "section": "",
    "text": "Here we are going to import a list of libraries that we need for this tutorial.\n\n!python --version\n\nPython 3.9.0\n\n\n\nfrom fastdownload import download_url\nfrom pathlib import Path\nimport pandas as pd\n\nfrom sklearn.linear_model import Lasso, LinearRegression\nfrom sklearn.metrics import mean_squared_error \n\nimport mlflow\nfrom mlflow.models.signature import infer_signature"
  },
  {
    "objectID": "posts/mlflow/experiment_tracking.html#tracking-uri",
    "href": "posts/mlflow/experiment_tracking.html#tracking-uri",
    "title": "Experiment Tracking with MLflow",
    "section": "",
    "text": "Before starting any training or data preprocessing, we start by setting tracking uri and experiment for this mlflow experiment. Tracking uri can be\n- localhost\n- localhost with SQlite\n- localhost with tracking server\n- remote tracking server, backend and artifact stores\nIn this tutorial, we will start with localhost option and we will also use remote tracking server option in the second half of this tutorial."
  },
  {
    "objectID": "posts/mlflow/experiment_tracking.html#localhost",
    "href": "posts/mlflow/experiment_tracking.html#localhost",
    "title": "Experiment Tracking with MLflow",
    "section": "",
    "text": "In a localhost setting, the backend and artifact store share a local folder called ./mlruns.\n\nmlflow.set_tracking_uri(\"mlruns\")\nmlflow.set_experiment(\"Experiment-1\")\n\n2023/07/17 18:58:07 INFO mlflow.tracking.fluent: Experiment with name 'Experiment-1' does not exist. Creating a new experiment.\n\n\n&lt;Experiment: artifact_location='/home/shane/mlops-practice/experiment-tracking/mlruns/280918629490989571', creation_time=1689596887482, experiment_id='280918629490989571', last_update_time=1689596887482, lifecycle_stage='active', name='Experiment-1', tags={}&gt;\n\n\nmlflow.set_experiment() is for setting the experiment name and we can see that a new experiment is created.\n\n!tree .\n\n.\n├── env.yaml\n├── experiment_tracking.ipynb\n├── imgs\n│   ├── mlflow_run1.png\n│   ├── mlflow_ui.png\n│   ├── model.png\n│   └── run1_metadata.png\n└── mlruns\n    ├── 0\n    │   └── meta.yaml\n    └── 280918629490989571\n        └── meta.yaml\n\n4 directories, 8 files\n\n\nWe can run mlflow ui in terminal to access, well, mlflow ui.\n:"
  },
  {
    "objectID": "posts/mlflow/experiment_tracking.html#downloading-dataset",
    "href": "posts/mlflow/experiment_tracking.html#downloading-dataset",
    "title": "Experiment Tracking with MLflow",
    "section": "",
    "text": "The dataset that we are using for this tutorial is NYC TLC Trip Record dataset.\nWe are going to predict duration of a taxi ride.\n\ndata_path = Path('data')\nif not data_path.exists():\n    data_path.mkdir(exist_ok=True)\n\nFirst, we are creating a data folder data to which we are going to download the datasets.\n\ndef download_data(year : int, month : int, data_path : Path) -&gt; None:\n    '''download nyc green taxi data in parquet form and save it in data_path'''\n    url = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_{year}-{month:0&gt;2}.parquet\"\n    download_url(url, dest=data_path, show_progress=True)\n\ndownload_data(2023, 1, data_path)\ndownload_data(2023, 2, data_path)\n\n\n\n\n\n\n    \n      \n      100.46% [1433600/1427002 00:01&lt;00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.41% [1540096/1533740 00:01&lt;00:00]\n    \n    \n\n\nQuick side note for pyformatting,\n\nmonth = 2\nprint(f\"month == {month} : {month:0&gt;2}\")\n\nmonth = 11\nprint(f\"month == {month} : {month:0&gt;2}\")\n\nmonth == 2 : 02\nmonth == 11 : 11\n\n\n\n!tree .\n\n.\n├── data\n│   ├── green_tripdata_2023-01.parquet\n│   └── green_tripdata_2023-02.parquet\n├── env.yaml\n├── experiment_tracking.ipynb\n├── imgs\n│   ├── mlflow_run1.png\n│   ├── mlflow_ui.png\n│   ├── model.png\n│   └── run1_metadata.png\n└── mlruns\n    ├── 0\n    │   └── meta.yaml\n    └── 280918629490989571\n        └── meta.yaml\n\n5 directories, 10 files\n\n\nDatasets are downloaded using fastdownload library from fastai."
  },
  {
    "objectID": "posts/mlflow/experiment_tracking.html#preprocessing",
    "href": "posts/mlflow/experiment_tracking.html#preprocessing",
    "title": "Experiment Tracking with MLflow",
    "section": "",
    "text": "def read_dataframe(filename: Path)-&gt; pd.DataFrame:\n    df = pd.read_parquet(filename)\n    df[\"duration\"] = df[\"lpep_dropoff_datetime\"] - df[\"lpep_pickup_datetime\"]\n    df.duration = df[\"duration\"].apply(lambda td : td.total_seconds() / 60)\n    df = df[(df.duration  &gt;= 0) & (df.duration &lt;= 60) ]\n\n    categorical_data = [\"PULocationID\", \"DOLocationID\"]\n    df[categorical_data] = df[categorical_data].astype(str)\n    df[\"PU_DO\"] = df['PULocationID'] + \"_\" + df['DOLocationID']\n\n    return df\n\nSince the files are in parquet format, we use pd.read_parquet() method.\n\n# df[\"duration\"] = df[\"lpep_dropoff_datetime\"] - df[\"lpep_pickup_datetime\"]\n# df.duration = df[\"duration\"].apply(lambda td : td.total_seconds() / 60)\n\nWe want to calculate the duration of each trip. The trip duration is calculated by subtracting pickup datetime from dropoff datetime.\nWe also want to get the duration in minute. We get the total seconds and divided by 60.\n\ndf = read_dataframe(\"data/green_tripdata_2023-01.parquet\")\ndf_val = read_dataframe(\"data/green_tripdata_2023-02.parquet\")\n\n\ndef preprocess(df : pd.DataFrame) -&gt; tuple((pd.DataFrame, pd.DataFrame)):\n    categorical = [\"PU_DO\"]\n    numerical   = [\"trip_distance\", \"fare_amount\", \"total_amount\"]\n\n    X = df[categorical + numerical]\n    y = df.duration\n\n    return X, y\n    \n\n\nX, y = preprocess(df)\nX_val, y_val = preprocess(df_val)\n\nAfter loading train and validation dataset and performing preprocessing, we now have features X and targets y.\nNow we can start training the model."
  },
  {
    "objectID": "posts/mlflow/experiment_tracking.html#model-training",
    "href": "posts/mlflow/experiment_tracking.html#model-training",
    "title": "Experiment Tracking with MLflow",
    "section": "",
    "text": "def train(X, y):\n    model = LinearRegression()\n    model.fit(X,y)\n\n    return model\n\nWe will initialize mlflow run as\n\n# with mlflow.start_run() as run:\n\nand wrap up the training inside of it.\n- `set_tag` : for tracking metadata\n- `log_param` : for logging parameters\n- `log_metric` : for logging metric\nIn the example below, we use set_tag for tracking developer name, log_param for tracking data folder used for training and validation ,and log_metric for tracking validation rmse metric.\nSome other useful methods are :\n- `set_tags` : Log a batch of tags for the current run.\n- `log_params` : Log a batch of params for the current run.\n- `log_artifact` : Log a local file or directory as an artifact of the currently active run.\n- `log_artifacts` : Log all the contents of a local directory as artifacts of the run\n\nwith mlflow.start_run() as run:\n\n    mlflow.set_tag(\"Developer\", \"Shane\")\n    mlflow.log_param(\"Train-data-path\", \"data/green_tripdata_2023-01.parquet\")\n    mlflow.log_param(\"Valid-data-path\", \"data/green_tripdata_2023-02.parquet\")\n    \n    model = train(X, y)\n\n    preds = model.predict(X_val)\n\n    rmse = mean_squared_error(y_val, preds, squared=False)\n\n    mlflow.log_metric('RMSE', rmse)\n    signature = infer_signature(X_val, preds)\n    model_uri = mlflow.sklearn.log_model(model, artifact_path=\"model\", signature=signature).model_uri\n   \n\n\n\n\nfist run\n\n\nThe first run can be seen in the above picture with the name, shivering-panda-266. It is a random run name since we didn’t set a specific run name.\n\n\n\nrun1\n\n\nWe can see the Train-data-path, Valid-data-path in the Parameters section, RMSE in metrics section and Developer in the Tags section."
  },
  {
    "objectID": "posts/mlflow/experiment_tracking.html#model-saving",
    "href": "posts/mlflow/experiment_tracking.html#model-saving",
    "title": "Experiment Tracking with MLflow",
    "section": "",
    "text": "# signature = infer_signature(X_val, preds)\n# mlflow.sklearn.log_model(model, artifact_path=\"model\", signature=signature)\n\nModel signatures define input and output schemas for MLflow models. Model signature is obtained here using infer_signature.\nWe can log a model using mlflow.&lt;framework&gt;.log_model. In this case, we are using mlflow.sklearn.log_model.\n\n\n\nmodel1\n\n\nSince we add signature parameter, we can see the model input and output schema here. We can also see two ways that we can load the model and make predictions."
  },
  {
    "objectID": "posts/mlflow/experiment_tracking.html#auto-logging",
    "href": "posts/mlflow/experiment_tracking.html#auto-logging",
    "title": "Experiment Tracking with MLflow",
    "section": "",
    "text": "We can also do auto logging by using mlflow.&lt;framework&gt;.autolog()\n\n# mlflow.sklearn.autolog()\n\nAutologging is known to be compatible with the following package versions: 0.22.1 &lt;= scikit-learn &lt;= 1.2.2. Autologging may not succeed when used with package versions outside of this range."
  },
  {
    "objectID": "posts/mlflow/experiment_tracking.html#model-loading",
    "href": "posts/mlflow/experiment_tracking.html#model-loading",
    "title": "Experiment Tracking with MLflow",
    "section": "",
    "text": "We have saved the model using log_model. Now, we are going to load that model for prediction.\n\nfrom mlflow import MlflowClient\n\nclient = MlflowClient(tracking_uri=\"mlruns\")\n\n\nexperiments = client.search_experiments()\n\nfor experiment in experiments:\n    print(f\"Experiment Name : {experiment.name}\")\n    print(f\"\\tExperiment id :{experiment.experiment_id}\")\n    print(f\"\\tArtifact Location :{experiment.artifact_location}\\n\")\n\nExperiment Name : Experiment-1\n    Experiment id :280918629490989571\n    Artifact Location :/home/shane/mlops-practice/experiment-tracking/mlruns/280918629490989571\n\nExperiment Name : Default\n    Experiment id :0\n    Artifact Location :/home/shane/mlops-practice/experiment-tracking/mlruns/0\n\n\n\nOur experiment’s name is “Experiment-1” and the Experiment id for that is “146920015920581846”.\n\nfor experiment in experiments:\n    if experiment.name == \"Experiment-1\":\n        exp = experiment\n\n\nexp_id = exp.experiment_id\nruns = client.search_runs(experiment_ids=[exp_id])\n\nfor run in runs:\n    print(f\"run name : {run.info.run_name}\")\n    print(f\"\\trun id : {run.info.run_id}\")\n    print(f\"\\trmse : {run.data.metrics['RMSE']}\")\n\nrun name : charming-toad-155\n    run id : 9ed0390d8a4b44dca36f36ee25d20ba4\n    rmse : 6.282692349434918\n\n\nLet’s pick a run_id of the model that we want to load.\n\nrun_id = runs[0].info.run_id\nlogged_model = f'runs:/{run_id}/model'\n\n# Load model as a PyFuncModel.\nloaded_model = mlflow.pyfunc.load_model(logged_model)\n\n# Predict on a Pandas DataFrame.\nimport pandas as pd\nloaded_model.predict(X_val)\n\n2023/07/17 18:59:06 WARNING mlflow.pyfunc: Detected one or more mismatches between the model's dependencies and the current Python environment:\n - mlflow (current: 2.4.2, required: mlflow==2.4)\nTo fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n\n\narray([26.09040939, 16.31353026, 22.14756441, ..., 15.42763737,\n       14.91915344, 12.24746033])"
  },
  {
    "objectID": "posts/mlflow/experiment_tracking.html#model-registry",
    "href": "posts/mlflow/experiment_tracking.html#model-registry",
    "title": "Experiment Tracking with MLflow",
    "section": "",
    "text": "Model Registry is a centralized model store, a set of APIs and UI. It provides\n    - model lineage, \n    - model versioning, \n    - stage transition, and \n    - annotations.\n\nmlflow.set_tracking_uri('mlruns/')\n\nmodel_uri = f\"runs:/{run_id}/models\"\nmlflow.register_model(model_uri=model_uri, name=\"nyc-taxi-regressor\")\n\nSuccessfully registered model 'nyc-taxi-regressor'.\n2023/07/17 18:59:10 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: nyc-taxi-regressor, version 1\nCreated version '1' of model 'nyc-taxi-regressor'.\n\n\n&lt;ModelVersion: aliases=[], creation_timestamp=1689596950866, current_stage='None', description=None, last_updated_timestamp=1689596950866, name='nyc-taxi-regressor', run_id='9ed0390d8a4b44dca36f36ee25d20ba4', run_link=None, source='/home/shane/mlops-practice/experiment-tracking/mlruns/280918629490989571/9ed0390d8a4b44dca36f36ee25d20ba4/artifacts/models', status='READY', status_message=None, tags={}, user_id=None, version=1&gt;\n\n\nWe have now registered the model under the name of “nyc-taxi-regressor’.\nWe can see that the version of the model is now 1. Since this is the first model registered under this name.\nBut It doesn’t have any staging information.\n\nmodel_name = \"nyc-taxi-regressor\"\nlatest_versions = client.get_latest_versions(name=model_name)\n\nfor version in latest_versions:\n    print(f\"version: {version.version}, stage: {version.current_stage}\")\n\nversion: 1, stage: None\n\n\nWe can transition the model version and stages using transition_model_version_stage.\n\nmodel_version = 1\nnew_stage = \"Staging\"\nclient.transition_model_version_stage(\n    name=model_name,\n    version=model_version,\n    stage=new_stage,\n    archive_existing_versions=False\n)\n\n&lt;ModelVersion: aliases=[], creation_timestamp=1689596950866, current_stage='Staging', description=None, last_updated_timestamp=1689596955099, name='nyc-taxi-regressor', run_id='9ed0390d8a4b44dca36f36ee25d20ba4', run_link=None, source='/home/shane/mlops-practice/experiment-tracking/mlruns/280918629490989571/9ed0390d8a4b44dca36f36ee25d20ba4/artifacts/models', status='READY', status_message=None, tags={}, user_id=None, version=1&gt;\n\n\n\nmodel_name = \"nyc-taxi-regressor\"\nlatest_versions = client.get_latest_versions(name=model_name)\n\nfor version in latest_versions:\n    print(f\"version: {version.version}, stage: {version.current_stage}\")\n\nversion: 1, stage: Staging\n\n\nNow, we can see that the stage of the model is now Staging.\n\nfrom datetime import datetime\n\ndate = datetime.today().date()\nclient.update_model_version(\n    name=model_name,\n    version=model_version,\n    description=f\"The model version {model_version} was transitioned to {new_stage} on {date}\"\n)\n\n&lt;ModelVersion: aliases=[], creation_timestamp=1689596950866, current_stage='Staging', description='The model version 1 was transitioned to Staging on 2023-07-17', last_updated_timestamp=1689596958723, name='nyc-taxi-regressor', run_id='9ed0390d8a4b44dca36f36ee25d20ba4', run_link=None, source='/home/shane/mlops-practice/experiment-tracking/mlruns/280918629490989571/9ed0390d8a4b44dca36f36ee25d20ba4/artifacts/models', status='READY', status_message=None, tags={}, user_id=None, version=1&gt;\n\n\nWe could also add description of the models like datetime information above."
  },
  {
    "objectID": "posts/data_manipulation_d2l/2.1-Data_Manipulation.html",
    "href": "posts/data_manipulation_d2l/2.1-Data_Manipulation.html",
    "title": "Data Manipulation with Pytorch",
    "section": "",
    "text": "Notes for chapter 2 section 1 of Dive into deep learning."
  },
  {
    "objectID": "posts/data_manipulation_d2l/2.1-Data_Manipulation.html#getting-started",
    "href": "posts/data_manipulation_d2l/2.1-Data_Manipulation.html#getting-started",
    "title": "Data Manipulation with Pytorch",
    "section": "Getting Started",
    "text": "Getting Started\n\nimport torch\n\n\ntensor : (possibly multi dimensional) array of numbers\ntensor with one axis : vector\ntensor with two axes : matrix\ntensor with more than two axes k &gt; 2 : kth order tensor\ncreate new tensors populated with values.\narange(n) : create a vector of evenly spaced values, starting at 0 ( included ) and ending at n ( not included ).\nBy default, the interval size is 1.\nstored in main memory for CPU based computations.\n\n\nx = torch.arange(12, dtype=torch.float32)\nx\n\ntensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])\n\n\n\nEach value is called element of tensor.\ncan inspect total number of elements using numel method.\n\n\nx.numel()\n\n12\n\n\n\ncan inspect shape of a tensor using shape attribute\n\n\nx.shape\n\ntorch.Size([12])\n\n\n\ncan inspect size of each dim or axis using size method\n\n\nx.size(dim=0)\n\n12\n\n\n\ncan change the shape of a tensor without altering its value using reshape method\nnew tensor becomes a matrix\n\n\nX = x.reshape(4,3)\nX, X.shape\n\n(tensor([[ 0.,  1.,  2.],\n         [ 3.,  4.,  5.],\n         [ 6.,  7.,  8.],\n         [ 9., 10., 11.]]),\n torch.Size([4, 3]))\n\n\n\nwe don’t need to specify all component of reshape.\nsince we already know the size of the tensor, we only need to specify one component.\n\n\nx.reshape(-1, 3), x.reshape(4, -1)\n\n(tensor([[ 0.,  1.,  2.],\n         [ 3.,  4.,  5.],\n         [ 6.,  7.,  8.],\n         [ 9., 10., 11.]]),\n tensor([[ 0.,  1.,  2.],\n         [ 3.,  4.,  5.],\n         [ 6.,  7.,  8.],\n         [ 9., 10., 11.]]))\n\n\n\ntensor initialized to contain all zeros or ones\n\n\ntorch.zeros((2,3,4)), torch.ones((2,3,4))\n\n(tensor([[[0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.]],\n \n         [[0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.]]]),\n tensor([[[1., 1., 1., 1.],\n          [1., 1., 1., 1.],\n          [1., 1., 1., 1.]],\n \n         [[1., 1., 1., 1.],\n          [1., 1., 1., 1.],\n          [1., 1., 1., 1.]]]))\n\n\n\ncan also initialzed tensor with elements drawn from a standard Gaussina (normal) distribution with mean 0 and standard deviation 1.\n\n\ntorch.randn(3, 4)\n\ntensor([[ 2.3716, -0.2123, -1.0150,  1.1659],\n        [-1.8146, -0.1514,  0.0144,  0.0529],\n        [-0.0288,  0.2103,  2.7825, -1.1707]])\n\n\n\nFinally, can also construct tensors by giving exact values using (nested) python lists\n\n\ntorch.tensor([[2, 3, 4], [2,4,5]])\n\ntensor([[2, 3, 4],\n        [2, 4, 5]])"
  },
  {
    "objectID": "posts/data_manipulation_d2l/2.1-Data_Manipulation.html#indexing-and-slicing",
    "href": "posts/data_manipulation_d2l/2.1-Data_Manipulation.html#indexing-and-slicing",
    "title": "Data Manipulation with Pytorch",
    "section": "Indexing and Slicing",
    "text": "Indexing and Slicing\n\nlike python list, we can access tensor elements by indexing (starting with 0)\nnegative indexing to access element based on its position relative to the end\ncan also access a whole range of elements by slicing (X [ start: end] ) , including start and not including end\nFinally, when only one index ( or slice ) is specified for a kth order tensor, it is applied along axis 0.\n\n\nX\n\ntensor([[ 0.,  1.,  2.],\n        [ 3.,  4.,  5.],\n        [ 6.,  7.,  8.],\n        [ 9., 10., 11.]])\n\n\n\nX[-1]\n\ntensor([ 9., 10., 11.])\n\n\n\nX[2:4]\n\ntensor([[ 6.,  7.,  8.],\n        [ 9., 10., 11.]])\n\n\n\nX[2,2]\n\ntensor(8.)\n\n\n\nX[2, :2]\n\ntensor([6., 7.])\n\n\n\ncan also write elements of a matrix by specifying indices\n\n\nX[2,2] = 34\n\n\nX\n\ntensor([[ 0.,  1.,  2.],\n        [ 3.,  4.,  5.],\n        [ 6.,  7., 34.],\n        [ 9., 10., 11.]])\n\n\n\nto assign multiple elements with the same value, we can use indexing on the left hand side\n\n\nX[:2, :] = 22\nX\n\ntensor([[22., 22., 22.],\n        [22., 22., 22.],\n        [ 6.,  7., 34.],\n        [ 9., 10., 11.]])"
  },
  {
    "objectID": "posts/data_manipulation_d2l/2.1-Data_Manipulation.html#operations",
    "href": "posts/data_manipulation_d2l/2.1-Data_Manipulation.html#operations",
    "title": "Data Manipulation with Pytorch",
    "section": "Operations",
    "text": "Operations\n\nelementwise operations : these apply a standard scalar operation to each element of the tensor\nunary scalar operator : taking one input\n\n\ntorch.exp(x)\n\ntensor([3.5849e+09, 3.5849e+09, 3.5849e+09, 3.5849e+09, 3.5849e+09, 3.5849e+09,\n        4.0343e+02, 1.0966e+03, 5.8346e+14, 8.1031e+03, 2.2026e+04, 5.9874e+04])\n\n\n\nbinary scalar operator : taking two inputs\n\n\nx = torch.tensor([1., 2., 4. , 6.])\ny = torch.tensor([2, 2, 2, 2])\n\nx + y, x - y, x * y, x / y, x ** y\n\n(tensor([3., 4., 6., 8.]),\n tensor([-1.,  0.,  2.,  4.]),\n tensor([ 2.,  4.,  8., 12.]),\n tensor([0.5000, 1.0000, 2.0000, 3.0000]),\n tensor([ 1.,  4., 16., 36.]))\n\n\n\nconcatenate multiple tensors together : stacking them end to end to form a larger tensor.\n\n\nX = torch.arange(12, dtype=torch.float32).reshape((3,4))\nY = torch.tensor([[2., 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\ntorch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)\n\n(tensor([[ 0.,  1.,  2.,  3.],\n         [ 4.,  5.,  6.,  7.],\n         [ 8.,  9., 10., 11.],\n         [ 2.,  1.,  4.,  3.],\n         [ 1.,  2.,  3.,  4.],\n         [ 4.,  3.,  2.,  1.]]),\n tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))\n\n\n\nconstruct binary tensor via logical statements\n\n\nX == Y\n\ntensor([[False,  True, False,  True],\n        [False, False, False, False],\n        [False, False, False, False]])\n\n\n\nX &lt; Y\n\ntensor([[ True, False,  True, False],\n        [False, False, False, False],\n        [False, False, False, False]])\n\n\n\nX &gt; Y\n\ntensor([[False, False, False, False],\n        [ True,  True,  True,  True],\n        [ True,  True,  True,  True]])\n\n\n\nSumming all the elements in the tensor using sum method\n\n\nX.sum()\n\ntensor(66.)"
  },
  {
    "objectID": "posts/data_manipulation_d2l/2.1-Data_Manipulation.html#broadcasting",
    "href": "posts/data_manipulation_d2l/2.1-Data_Manipulation.html#broadcasting",
    "title": "Data Manipulation with Pytorch",
    "section": "Broadcasting",
    "text": "Broadcasting\n\ncan now do elementwise binary operations on two tensors with the same shape\nBroadcasting\n\nunder certain conditions, can still perform elementwise binary operations on two tensors with different shapes\n\n\nexpand one or both arrays by copying elements along axes with length 1 so that after the transformation, the two tensors have the same shape.\nperform elementwise operations on the resulting arrays\n\n\n\n\n\na = torch.arange(3).reshape((3, 1))\nb = torch.arange(2).reshape((1,2))\na, b\n\n(tensor([[0],\n         [1],\n         [2]]),\n tensor([[0, 1]]))\n\n\n\na + b\n\ntensor([[0, 1],\n        [1, 2],\n        [2, 3]])\n\n\n\na = torch.arange(6).reshape((3, 2))\nb = torch.arange(9).reshape((3,3))\na, b\n\n(tensor([[0, 1],\n         [2, 3],\n         [4, 5]]),\n tensor([[0, 1, 2],\n         [3, 4, 5],\n         [6, 7, 8]]))\n\n\n\na + b\n\nRuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1\n\n\n\nsingleton dimensions are dimensions of size 1.\nWhat is singleton dimension of a tensor\n\n\na = torch.ones((2,3,3))\nb = torch.ones((1,1,3))\n\na, b\n\n(tensor([[[1., 1., 1.],\n          [1., 1., 1.],\n          [1., 1., 1.]],\n \n         [[1., 1., 1.],\n          [1., 1., 1.],\n          [1., 1., 1.]]]),\n tensor([[[1., 1., 1.]]]))\n\n\n\na + b\n\ntensor([[[2., 2., 2.],\n         [2., 2., 2.],\n         [2., 2., 2.]],\n\n        [[2., 2., 2.],\n         [2., 2., 2.],\n         [2., 2., 2.]]])\n\n\n\nhow-does-pytorch-broadcasting-work\nPytorch broadcasting semantics\nNumpy Broadcasting Rules"
  },
  {
    "objectID": "posts/data_manipulation_d2l/2.1-Data_Manipulation.html#saving-memory",
    "href": "posts/data_manipulation_d2l/2.1-Data_Manipulation.html#saving-memory",
    "title": "Data Manipulation with Pytorch",
    "section": "Saving Memory",
    "text": "Saving Memory\n\nrunning operations can cause new memories to be allocated to host results.\nby running Y = Y + X, python dereference the tensor that Y used to point to and instead point Y at the newly allocated memory.\n\n\nbefore = id(Y)\nY = Y + X\nid(Y) == before\n\nFalse\n\n\n\nThis is how to do operations in place.\nno unnecessary memeory allocations\nwe can avoid a memory leak or referring to stale parameters\n\n\nZ = torch.zeros_like(Y)\nprint(f'id(Z) : {id(Z)}')\nZ[:] = X + Y\nprint(f'id(Z) : {id(Z)}')\n\nid(Z) : 140466157769232\nid(Z) : 140466157769232\n\n\n\nbefore = id(X)\nX += Y\nid(X) == before\n\nTrue"
  },
  {
    "objectID": "posts/data_manipulation_d2l/2.1-Data_Manipulation.html#conversion-to-other-python-objects",
    "href": "posts/data_manipulation_d2l/2.1-Data_Manipulation.html#conversion-to-other-python-objects",
    "title": "Data Manipulation with Pytorch",
    "section": "Conversion to other python objects",
    "text": "Conversion to other python objects\n\nnumpy to torch , torch to numpy\n\n\nA = X.numpy()\nB = torch.from_numpy(A)\ntype(A), type(B)\n\n(numpy.ndarray, torch.Tensor)\n\n\n\nto convert size one torch tensor to python scalar\n\nitem function\npython built-in functions\n\n\n\na = torch.tensor([3.5])\na, a.item(), float(a), int(a)\n\n(tensor([3.5000]), 3.5, 3.5, 3)"
  }
]