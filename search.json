[
  {
    "objectID": "posts/pandas/2.2-Data_Preprocessing.html",
    "href": "posts/pandas/2.2-Data_Preprocessing.html",
    "title": "Data Preprocessing with Pandas",
    "section": "",
    "text": "Notes for D2L Chapter 2 Section 2"
  },
  {
    "objectID": "posts/pandas/2.2-Data_Preprocessing.html#data-preparation",
    "href": "posts/pandas/2.2-Data_Preprocessing.html#data-preparation",
    "title": "Data Preprocessing with Pandas",
    "section": "Data Preparation",
    "text": "Data Preparation\nIn supervised learning, we train models to predict a target value, given some sets of input values. Our first step in preprocessing the dataset is to separate out columns corresponding to input and columns corresponding to target values. We can select columns either\n- by name or\n- by using integer-location based indexing (iloc)\n\ninputs, targets = data.iloc[:, 0:2], data.iloc[:, 2]\nprint(f\"Inputs \\n {inputs}\\n\")\nprint(f\"Targets \\n {targets}\")\n\nInputs \n    NumRooms RoofType\n0       NaN      NaN\n1       2.0      NaN\n2       4.0    Slate\n3       NaN      NaN\n\nTargets \n 0    127500\n1    106000\n2    178100\n3    140000\nName: Price, dtype: int64\n\n\nPandas replaced all CSV entries with value NA with a special NaN ( not a number ) value. These are called “missing values”. Depending on the context, missing values can be handled either via imputation or deletion.\n\nImputation : replace missing values with estimates of their values\nDeletion : simply discard the columns or rows that contain missing values\n\nFor categorical input fields like “RoofType”, we can treat NaN as a category.\nWe can use pd.get_dummies() to convert this column into two separate columns.”RootType” column is now separated into “RoofType_Slate” and “RootType_nan”.\n\nprint('Before Imputation')\nprint(inputs)\n\nprint(\"\\nAfter Category Imputation\")\ninputs = pd.get_dummies(inputs, columns=['RoofType'], dummy_na=True, dtype=int)\nprint(inputs)\n\nBefore Imputation\n   NumRooms RoofType\n0       NaN      NaN\n1       2.0      NaN\n2       4.0    Slate\n3       NaN      NaN\n\nAfter Category Imputation\n   NumRooms  RoofType_Slate  RoofType_nan\n0       NaN               0             1\n1       2.0               0             1\n2       4.0               1             0\n3       NaN               0             1\n\n\nFor numerical missing value, one common way is to replace NaN entries with the mean value of the corresponding column.\n\ninputs = inputs.fillna(inputs.mean())\nprint(inputs)\n\n   NumRooms  RoofType_Slate  RoofType_nan\n0       3.0               0             1\n1       2.0               0             1\n2       4.0               1             0\n3       3.0               0             1"
  },
  {
    "objectID": "posts/pandas/2.2-Data_Preprocessing.html#conversion-to-the-tensor-format",
    "href": "posts/pandas/2.2-Data_Preprocessing.html#conversion-to-the-tensor-format",
    "title": "Data Preprocessing with Pandas",
    "section": "Conversion to the tensor format",
    "text": "Conversion to the tensor format\nNow that we have separated inputs and targets columns, we can load them into a tensor.\n\nimport torch\n\nX, y = torch.tensor(inputs.values), torch.tensor(targets.values)\nX, y\n\n(tensor([[3., 0., 1.],\n         [2., 0., 1.],\n         [4., 1., 0.],\n         [3., 0., 1.]], dtype=torch.float64),\n tensor([127500, 106000, 178100, 140000]))"
  },
  {
    "objectID": "posts/pandas/2.2-Data_Preprocessing.html#object-creation",
    "href": "posts/pandas/2.2-Data_Preprocessing.html#object-creation",
    "title": "Data Preprocessing with Pandas",
    "section": "Object Creation",
    "text": "Object Creation\nCreating a Series by passing a list of values\n\ns = pd.Series([1, 3, 5, np.nan, 6, 8])\ns\n\n0    1.0\n1    3.0\n2    5.0\n3    NaN\n4    6.0\n5    8.0\ndtype: float64\n\n\nCreating a DataFrame by passing a NumPy array, with a datetime index using date_range()\n\ndates = pd.date_range(\"20230719\", periods=6)\n\nprint(dates)\n\nDatetimeIndex(['2023-07-19', '2023-07-20', '2023-07-21', '2023-07-22',\n               '2023-07-23', '2023-07-24'],\n              dtype='datetime64[ns]', freq='D')\n\n\n\ndf = pd.DataFrame(np.random.randn(6,4), index=dates, columns=list(\"ABCD\"))\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2023-07-19\n-0.111161\n0.166914\n0.193785\n1.071966\n\n\n2023-07-20\n0.527643\n0.850008\n-2.145393\n-1.092856\n\n\n2023-07-21\n-0.295973\n0.681027\n-1.032201\n-1.090513\n\n\n2023-07-22\n0.163666\n-0.607460\n0.301047\n-1.192986\n\n\n2023-07-23\n-1.079115\n0.877211\n0.023358\n0.027880\n\n\n2023-07-24\n1.443937\n-1.912616\n0.219415\n0.500132\n\n\n\n\n\n\n\nCreating a DataFrame by passing a dictionary of objects\n\ndf2 = pd.DataFrame(\n    {\n        \"A\" : 1.,\n        \"B\" : pd.Timestamp(\"20230721\"),\n        \"C\" : pd.Series(1, index=list(range(6)), dtype=\"float32\"),\n        \"D\" : np.array([3] * 6, dtype=\"int32\"),\n        \"E\" : pd.Categorical([\"test\", \"train\",\"test\", \"train\", \"test\", \"train\"]),\n        \"F\" : \"foo\",\n    }\n)\n\ndf2\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\n\n\n\n\n0\n1.0\n2023-07-21\n1.0\n3\ntest\nfoo\n\n\n1\n1.0\n2023-07-21\n1.0\n3\ntrain\nfoo\n\n\n2\n1.0\n2023-07-21\n1.0\n3\ntest\nfoo\n\n\n3\n1.0\n2023-07-21\n1.0\n3\ntrain\nfoo\n\n\n4\n1.0\n2023-07-21\n1.0\n3\ntest\nfoo\n\n\n5\n1.0\n2023-07-21\n1.0\n3\ntrain\nfoo\n\n\n\n\n\n\n\n\ndf2.dtypes\n\nA           float64\nB    datetime64[ns]\nC           float32\nD             int32\nE          category\nF            object\ndtype: object"
  },
  {
    "objectID": "posts/pandas/2.2-Data_Preprocessing.html#viewing-data",
    "href": "posts/pandas/2.2-Data_Preprocessing.html#viewing-data",
    "title": "Data Preprocessing with Pandas",
    "section": "Viewing Data",
    "text": "Viewing Data\n\nDataFrame.head() : to view top rows\nDataFrame.tail() : to view bottom rows\n\n\ndf.head()\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2023-07-19\n-0.111161\n0.166914\n0.193785\n1.071966\n\n\n2023-07-20\n0.527643\n0.850008\n-2.145393\n-1.092856\n\n\n2023-07-21\n-0.295973\n0.681027\n-1.032201\n-1.090513\n\n\n2023-07-22\n0.163666\n-0.607460\n0.301047\n-1.192986\n\n\n2023-07-23\n-1.079115\n0.877211\n0.023358\n0.027880\n\n\n\n\n\n\n\n\ndf.tail(3)\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2023-07-22\n0.163666\n-0.607460\n0.301047\n-1.192986\n\n\n2023-07-23\n-1.079115\n0.877211\n0.023358\n0.027880\n\n\n2023-07-24\n1.443937\n-1.912616\n0.219415\n0.500132\n\n\n\n\n\n\n\n\ndf.index\n\nDatetimeIndex(['2023-07-19', '2023-07-20', '2023-07-21', '2023-07-22',\n               '2023-07-23', '2023-07-24'],\n              dtype='datetime64[ns]', freq='D')\n\n\n\ndf.columns\n\nIndex(['A', 'B', 'C', 'D'], dtype='object')\n\n\nDataFrame.to_numpy() gives NumPy representation of the underlying data.\nIt can be expensive operation if DataFrame has columns with different data types.\nNumPy arrays have only one dtype for entire array, while Pandas DataFrames have one dtype per column.\nWhen we call DataFrame.to_numpy(), pandas will find the Numpy dtype that can hold all of the dtypes in the DataFrame, which probably might end up being object.\n\n\n\n\n\n\nSide Note about %time and %%time\n\n\n\n\n\n%time measures execution time of the next line.\n%%time measures execution time of the whole cell.\nDifference between %time and %%time\n\n\n\n\n%%time\ndf.to_numpy()\n\nCPU times: user 20 µs, sys: 19 µs, total: 39 µs\nWall time: 41 µs\n\n\narray([[-0.11116127,  0.16691402,  0.19378469,  1.07196565],\n       [ 0.52764337,  0.85000843, -2.1453925 , -1.09285636],\n       [-0.29597326,  0.68102652, -1.03220127, -1.09051285],\n       [ 0.16366561, -0.60746034,  0.30104705, -1.19298591],\n       [-1.07911538,  0.87721105,  0.02335771,  0.02788001],\n       [ 1.44393719, -1.91261559,  0.2194151 ,  0.50013158]])\n\n\n\n%%time\ndf2.to_numpy()\n\nCPU times: user 350 µs, sys: 319 µs, total: 669 µs\nWall time: 642 µs\n\n\narray([[1.0, Timestamp('2023-07-21 00:00:00'), 1.0, 3, 'test', 'foo'],\n       [1.0, Timestamp('2023-07-21 00:00:00'), 1.0, 3, 'train', 'foo'],\n       [1.0, Timestamp('2023-07-21 00:00:00'), 1.0, 3, 'test', 'foo'],\n       [1.0, Timestamp('2023-07-21 00:00:00'), 1.0, 3, 'train', 'foo'],\n       [1.0, Timestamp('2023-07-21 00:00:00'), 1.0, 3, 'test', 'foo'],\n       [1.0, Timestamp('2023-07-21 00:00:00'), 1.0, 3, 'train', 'foo']],\n      dtype=object)\n\n\nDataFrame.to_numpy() does not include the index or column labels in the output.\ndescribe() shows a quick statistics summary of data.\n\ndf.describe()\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\ncount\n6.000000\n6.000000\n6.000000\n6.000000\n\n\nmean\n0.108166\n0.009181\n-0.406665\n-0.296063\n\n\nstd\n0.846687\n1.096382\n0.985166\n0.967561\n\n\nmin\n-1.079115\n-1.912616\n-2.145393\n-1.192986\n\n\n25%\n-0.249770\n-0.413867\n-0.768312\n-1.092270\n\n\n50%\n0.026252\n0.423970\n0.108571\n-0.531316\n\n\n75%\n0.436649\n0.807763\n0.213007\n0.382069\n\n\nmax\n1.443937\n0.877211\n0.301047\n1.071966\n\n\n\n\n\n\n\nTransposing data\n\ndf.T\n\n\n\n\n\n\n\n\n2023-07-19\n2023-07-20\n2023-07-21\n2023-07-22\n2023-07-23\n2023-07-24\n\n\n\n\nA\n-0.111161\n0.527643\n-0.295973\n0.163666\n-1.079115\n1.443937\n\n\nB\n0.166914\n0.850008\n0.681027\n-0.607460\n0.877211\n-1.912616\n\n\nC\n0.193785\n-2.145393\n-1.032201\n0.301047\n0.023358\n0.219415\n\n\nD\n1.071966\n-1.092856\n-1.090513\n-1.192986\n0.027880\n0.500132\n\n\n\n\n\n\n\nDataFrame.sort_index() sorts by an index.\n\ndf.sort_index(axis=1, ascending=False)\n\n\n\n\n\n\n\n\nD\nC\nB\nA\n\n\n\n\n2023-07-19\n1.071966\n0.193785\n0.166914\n-0.111161\n\n\n2023-07-20\n-1.092856\n-2.145393\n0.850008\n0.527643\n\n\n2023-07-21\n-1.090513\n-1.032201\n0.681027\n-0.295973\n\n\n2023-07-22\n-1.192986\n0.301047\n-0.607460\n0.163666\n\n\n2023-07-23\n0.027880\n0.023358\n0.877211\n-1.079115\n\n\n2023-07-24\n0.500132\n0.219415\n-1.912616\n1.443937\n\n\n\n\n\n\n\nDataFrame.sort_values() sorts by values.\n\ndf.sort_values(by=\"D\")\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2023-07-22\n0.163666\n-0.607460\n0.301047\n-1.192986\n\n\n2023-07-20\n0.527643\n0.850008\n-2.145393\n-1.092856\n\n\n2023-07-21\n-0.295973\n0.681027\n-1.032201\n-1.090513\n\n\n2023-07-23\n-1.079115\n0.877211\n0.023358\n0.027880\n\n\n2023-07-24\n1.443937\n-1.912616\n0.219415\n0.500132\n\n\n2023-07-19\n-0.111161\n0.166914\n0.193785\n1.071966"
  },
  {
    "objectID": "posts/pandas/2.2-Data_Preprocessing.html#selection",
    "href": "posts/pandas/2.2-Data_Preprocessing.html#selection",
    "title": "Data Preprocessing with Pandas",
    "section": "Selection",
    "text": "Selection\n\nGetting\nSelecting a single column - df[“A”] - df.A\n\ndf[\"A\"]\n\n2023-07-19   -0.111161\n2023-07-20    0.527643\n2023-07-21   -0.295973\n2023-07-22    0.163666\n2023-07-23   -1.079115\n2023-07-24    1.443937\nFreq: D, Name: A, dtype: float64\n\n\nSelecting via [] (__getitem__)\n\ndf[0:3]\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2023-07-19\n-0.111161\n0.166914\n0.193785\n1.071966\n\n\n2023-07-20\n0.527643\n0.850008\n-2.145393\n-1.092856\n\n\n2023-07-21\n-0.295973\n0.681027\n-1.032201\n-1.090513\n\n\n\n\n\n\n\n\ndf[\"20230719\":\"20230722\"]\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2023-07-19\n-0.111161\n0.166914\n0.193785\n1.071966\n\n\n2023-07-20\n0.527643\n0.850008\n-2.145393\n-1.092856\n\n\n2023-07-21\n-0.295973\n0.681027\n-1.032201\n-1.090513\n\n\n2023-07-22\n0.163666\n-0.607460\n0.301047\n-1.192986\n\n\n\n\n\n\n\n\n\nSelection by label\nFor getting a cross section using a label:\n\ndf.loc[dates[0]]\n\nA   -0.111161\nB    0.166914\nC    0.193785\nD    1.071966\nName: 2023-07-19 00:00:00, dtype: float64\n\n\nSelecting on a multi-axis by label\n\ndf.loc[:, [\"A\", \"B\"]]\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n2023-07-19\n-0.111161\n0.166914\n\n\n2023-07-20\n0.527643\n0.850008\n\n\n2023-07-21\n-0.295973\n0.681027\n\n\n2023-07-22\n0.163666\n-0.607460\n\n\n2023-07-23\n-1.079115\n0.877211\n\n\n2023-07-24\n1.443937\n-1.912616\n\n\n\n\n\n\n\nshowing label slicing, both endpoints are included\n\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2023-07-19\n-0.111161\n0.166914\n0.193785\n1.071966\n\n\n2023-07-20\n0.527643\n0.850008\n-2.145393\n-1.092856\n\n\n2023-07-21\n-0.295973\n0.681027\n-1.032201\n-1.090513\n\n\n2023-07-22\n0.163666\n-0.607460\n0.301047\n-1.192986\n\n\n2023-07-23\n-1.079115\n0.877211\n0.023358\n0.027880\n\n\n2023-07-24\n1.443937\n-1.912616\n0.219415\n0.500132\n\n\n\n\n\n\n\n\ndf.loc[\"20230719\" : \"20230722\", [\"A\", \"B\"]]\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n2023-07-19\n-0.111161\n0.166914\n\n\n2023-07-20\n0.527643\n0.850008\n\n\n2023-07-21\n-0.295973\n0.681027\n\n\n2023-07-22\n0.163666\n-0.607460\n\n\n\n\n\n\n\nReduction in the dimensions of the returned object\n\ndf.loc[\"20230722\", [\"A\", \"B\"]]\n\nA    0.163666\nB   -0.607460\nName: 2023-07-22 00:00:00, dtype: float64\n\n\nFor getting a scalar value\n\n%%time\ndf.loc[\"20230722\", \"A\"]\n\nCPU times: user 159 µs, sys: 140 µs, total: 299 µs\nWall time: 303 µs\n\n\n0.16366560935804825\n\n\nFor getting fast access to scalar, same as the previous method\n\n%%time\ndf.at[\"20230722\", \"A\"]\n\nCPU times: user 166 µs, sys: 0 ns, total: 166 µs\nWall time: 169 µs\n\n\n0.16366560935804825\n\n\n\n\nSelection by position\nSelect via the position of the passed integers\n\ndf.iloc[3]\n\nA    0.163666\nB   -0.607460\nC    0.301047\nD   -1.192986\nName: 2023-07-22 00:00:00, dtype: float64\n\n\n\ndf.iloc[3:5, 0:2]\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n2023-07-22\n0.163666\n-0.607460\n\n\n2023-07-23\n-1.079115\n0.877211\n\n\n\n\n\n\n\nby list of integer positions, similar to NumPy\n\ndf.iloc[[1,2,4], [0,2]]\n\n\n\n\n\n\n\n\nA\nC\n\n\n\n\n2023-07-20\n0.527643\n-2.145393\n\n\n2023-07-21\n-0.295973\n-1.032201\n\n\n2023-07-23\n-1.079115\n0.023358\n\n\n\n\n\n\n\nFor Slicing rows explicitly\n\ndf.iloc[1:3, :]\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2023-07-20\n0.527643\n0.850008\n-2.145393\n-1.092856\n\n\n2023-07-21\n-0.295973\n0.681027\n-1.032201\n-1.090513\n\n\n\n\n\n\n\nFor slicing columns, explicitly\n\ndf.iloc[:, 1:3]\n\n\n\n\n\n\n\n\nB\nC\n\n\n\n\n2023-07-19\n0.166914\n0.193785\n\n\n2023-07-20\n0.850008\n-2.145393\n\n\n2023-07-21\n0.681027\n-1.032201\n\n\n2023-07-22\n-0.607460\n0.301047\n\n\n2023-07-23\n0.877211\n0.023358\n\n\n2023-07-24\n-1.912616\n0.219415\n\n\n\n\n\n\n\nFor getting a value explicitly\n\n%time df.iloc[1,1]\n\nCPU times: user 62 µs, sys: 53 µs, total: 115 µs\nWall time: 118 µs\n\n\n0.8500084309052217\n\n\nFor getting fast access,\n\n%time df.iat[1,1]\n\nCPU times: user 52 µs, sys: 44 µs, total: 96 µs\nWall time: 98.9 µs\n\n\n0.8500084309052217\n\n\n\n\nBoolean Indexing\nUsing single column’s values to select data\n\ndf[df[\"B\"] &gt; 0]\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2023-07-19\n-0.111161\n0.166914\n0.193785\n1.071966\n\n\n2023-07-20\n0.527643\n0.850008\n-2.145393\n-1.092856\n\n\n2023-07-21\n-0.295973\n0.681027\n-1.032201\n-1.090513\n\n\n2023-07-23\n-1.079115\n0.877211\n0.023358\n0.027880\n\n\n\n\n\n\n\nSelecting values from a DataFrame where a boolean condition is met:\n\ndf[df &gt; 0]\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2023-07-19\nNaN\n0.166914\n0.193785\n1.071966\n\n\n2023-07-20\n0.527643\n0.850008\nNaN\nNaN\n\n\n2023-07-21\nNaN\n0.681027\nNaN\nNaN\n\n\n2023-07-22\n0.163666\nNaN\n0.301047\nNaN\n\n\n2023-07-23\nNaN\n0.877211\n0.023358\n0.027880\n\n\n2023-07-24\n1.443937\nNaN\n0.219415\n0.500132\n\n\n\n\n\n\n\nUsing isin() method for filtering\n\ndf2 = df.copy()\n\ndf2[\"E\"] = [\"one\", \"one\", \"two\", \"three\", \"four\", \"three\"]\n\ndf2\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n2023-07-19\n-0.111161\n0.166914\n0.193785\n1.071966\none\n\n\n2023-07-20\n0.527643\n0.850008\n-2.145393\n-1.092856\none\n\n\n2023-07-21\n-0.295973\n0.681027\n-1.032201\n-1.090513\ntwo\n\n\n2023-07-22\n0.163666\n-0.607460\n0.301047\n-1.192986\nthree\n\n\n2023-07-23\n-1.079115\n0.877211\n0.023358\n0.027880\nfour\n\n\n2023-07-24\n1.443937\n-1.912616\n0.219415\n0.500132\nthree\n\n\n\n\n\n\n\n\ndf2[df2[\"E\"].isin([\"two\", \"four\"])]\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n2023-07-21\n-0.295973\n0.681027\n-1.032201\n-1.090513\ntwo\n\n\n2023-07-23\n-1.079115\n0.877211\n0.023358\n0.027880\nfour\n\n\n\n\n\n\n\n\n\nSetting\nSetting a new column automatically aligns the data by the indexes\n\ns1 = pd.Series([1,2,3,4,5,6], index=pd.date_range(\"20230719\", periods=6))\ns1\n\n2023-07-19    1\n2023-07-20    2\n2023-07-21    3\n2023-07-22    4\n2023-07-23    5\n2023-07-24    6\nFreq: D, dtype: int64\n\n\n\ndf[\"F\"] = s1\n\nSetting values by label\n\ndf.at[dates[0], \"A\"] = 0\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nF\n\n\n\n\n2023-07-19\n0.000000\n0.166914\n0.193785\n1.071966\n1\n\n\n2023-07-20\n0.527643\n0.850008\n-2.145393\n-1.092856\n2\n\n\n2023-07-21\n-0.295973\n0.681027\n-1.032201\n-1.090513\n3\n\n\n2023-07-22\n0.163666\n-0.607460\n0.301047\n-1.192986\n4\n\n\n2023-07-23\n-1.079115\n0.877211\n0.023358\n0.027880\n5\n\n\n2023-07-24\n1.443937\n-1.912616\n0.219415\n0.500132\n6\n\n\n\n\n\n\n\nSetting values by position\n\ndf.iat[0, 1] = 0\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nF\n\n\n\n\n2023-07-19\n0.000000\n0.000000\n0.193785\n1.071966\n1\n\n\n2023-07-20\n0.527643\n0.850008\n-2.145393\n-1.092856\n2\n\n\n2023-07-21\n-0.295973\n0.681027\n-1.032201\n-1.090513\n3\n\n\n2023-07-22\n0.163666\n-0.607460\n0.301047\n-1.192986\n4\n\n\n2023-07-23\n-1.079115\n0.877211\n0.023358\n0.027880\n5\n\n\n2023-07-24\n1.443937\n-1.912616\n0.219415\n0.500132\n6\n\n\n\n\n\n\n\nSetting by assigning with a NumPy array\n\ndf.loc[:, \"D\"] = np.array([5] * len(df))\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nF\n\n\n\n\n2023-07-19\n0.000000\n0.000000\n0.193785\n5.0\n1\n\n\n2023-07-20\n0.527643\n0.850008\n-2.145393\n5.0\n2\n\n\n2023-07-21\n-0.295973\n0.681027\n-1.032201\n5.0\n3\n\n\n2023-07-22\n0.163666\n-0.607460\n0.301047\n5.0\n4\n\n\n2023-07-23\n-1.079115\n0.877211\n0.023358\n5.0\n5\n\n\n2023-07-24\n1.443937\n-1.912616\n0.219415\n5.0\n6\n\n\n\n\n\n\n\nA where operation with setting\n\ndf2 = df.copy()\n\ndf2[df2 &gt; 0] = -df2\n\ndf2\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nF\n\n\n\n\n2023-07-19\n0.000000\n0.000000\n-0.193785\n-5.0\n-1\n\n\n2023-07-20\n-0.527643\n-0.850008\n-2.145393\n-5.0\n-2\n\n\n2023-07-21\n-0.295973\n-0.681027\n-1.032201\n-5.0\n-3\n\n\n2023-07-22\n-0.163666\n-0.607460\n-0.301047\n-5.0\n-4\n\n\n2023-07-23\n-1.079115\n-0.877211\n-0.023358\n-5.0\n-5\n\n\n2023-07-24\n-1.443937\n-1.912616\n-0.219415\n-5.0\n-6"
  },
  {
    "objectID": "posts/pandas/2.2-Data_Preprocessing.html#missing-data",
    "href": "posts/pandas/2.2-Data_Preprocessing.html#missing-data",
    "title": "Data Preprocessing with Pandas",
    "section": "Missing data",
    "text": "Missing data\npandas primarily uses the value np.nan to represent missing data.\nReindexing allows us to change/add/delete the index on a specified axis.\n\ndf1 = df.reindex(index=dates[:4], columns=list(df.columns) + [\"E\"])\ndf1.loc[dates[0]: dates[1], \"E\"] = 1\n\ndf1\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nF\nE\n\n\n\n\n2023-07-19\n0.000000\n0.000000\n0.193785\n5.0\n1\n1.0\n\n\n2023-07-20\n0.527643\n0.850008\n-2.145393\n5.0\n2\n1.0\n\n\n2023-07-21\n-0.295973\n0.681027\n-1.032201\n5.0\n3\nNaN\n\n\n2023-07-22\n0.163666\n-0.607460\n0.301047\n5.0\n4\nNaN\n\n\n\n\n\n\n\nDataFrame.dropna() drops any rows that have missing data\n\ndf1.dropna(how=\"any\")\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nF\nE\n\n\n\n\n2023-07-19\n0.000000\n0.000000\n0.193785\n5.0\n1\n1.0\n\n\n2023-07-20\n0.527643\n0.850008\n-2.145393\n5.0\n2\n1.0\n\n\n\n\n\n\n\nDataFrame.fillna() fills missing data\n\ndf1.fillna(value=5)\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nF\nE\n\n\n\n\n2023-07-19\n0.000000\n0.000000\n0.193785\n5.0\n1\n1.0\n\n\n2023-07-20\n0.527643\n0.850008\n-2.145393\n5.0\n2\n1.0\n\n\n2023-07-21\n-0.295973\n0.681027\n-1.032201\n5.0\n3\n5.0\n\n\n2023-07-22\n0.163666\n-0.607460\n0.301047\n5.0\n4\n5.0\n\n\n\n\n\n\n\nisna() gets the boolean mask where values are nan.\n\npd.isna(df1)\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nF\nE\n\n\n\n\n2023-07-19\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2023-07-20\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2023-07-21\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n2023-07-22\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue"
  },
  {
    "objectID": "posts/pandas/2.2-Data_Preprocessing.html#operations",
    "href": "posts/pandas/2.2-Data_Preprocessing.html#operations",
    "title": "Data Preprocessing with Pandas",
    "section": "Operations",
    "text": "Operations\n\nStats\nOperations in general exclude missing data.\n\ndf.mean()\n\nA    0.126693\nB   -0.018638\nC   -0.406665\nD    5.000000\nF    3.500000\ndtype: float64\n\n\n\ndf.mean(1)\n\n2023-07-19    1.238757\n2023-07-20    1.246452\n2023-07-21    1.470570\n2023-07-22    1.771450\n2023-07-23    1.964291\n2023-07-24    2.150147\nFreq: D, dtype: float64\n\n\nFor opearting with objects that have different dimensionality and need alignment, pandas automatically broadcasts along the specified dimension.\n\ndates\n\nDatetimeIndex(['2023-07-19', '2023-07-20', '2023-07-21', '2023-07-22',\n               '2023-07-23', '2023-07-24'],\n              dtype='datetime64[ns]', freq='D')\n\n\nshift : Shift index by desired number of time frequency increments.\nThis method is for shifting the values of datetime-like indexes by a specified time increment a given number of times.\n\ns = pd.Series([1, 3, 5, np.nan, 6, 8], index=dates).shift(2)\n\ns\n\n2023-07-19    NaN\n2023-07-20    NaN\n2023-07-21    1.0\n2023-07-22    3.0\n2023-07-23    5.0\n2023-07-24    NaN\nFreq: D, dtype: float64\n\n\n\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nF\n\n\n\n\n2023-07-19\n0.000000\n0.000000\n0.193785\n5.0\n1\n\n\n2023-07-20\n0.527643\n0.850008\n-2.145393\n5.0\n2\n\n\n2023-07-21\n-0.295973\n0.681027\n-1.032201\n5.0\n3\n\n\n2023-07-22\n0.163666\n-0.607460\n0.301047\n5.0\n4\n\n\n2023-07-23\n-1.079115\n0.877211\n0.023358\n5.0\n5\n\n\n2023-07-24\n1.443937\n-1.912616\n0.219415\n5.0\n6\n\n\n\n\n\n\n\n\ndf.sub(s, axis=\"index\")\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nF\n\n\n\n\n2023-07-19\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2023-07-20\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2023-07-21\n-1.295973\n-0.318973\n-2.032201\n4.0\n2.0\n\n\n2023-07-22\n-2.836334\n-3.607460\n-2.698953\n2.0\n1.0\n\n\n2023-07-23\n-6.079115\n-4.122789\n-4.976642\n0.0\n0.0\n\n\n2023-07-24\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n\nApply\n\ndf.apply(np.cumsum)\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nF\n\n\n\n\n2023-07-19\n0.000000\n0.000000\n0.193785\n5.0\n1\n\n\n2023-07-20\n0.527643\n0.850008\n-1.951608\n10.0\n3\n\n\n2023-07-21\n0.231670\n1.531035\n-2.983809\n15.0\n6\n\n\n2023-07-22\n0.395336\n0.923575\n-2.682762\n20.0\n10\n\n\n2023-07-23\n-0.683780\n1.800786\n-2.659404\n25.0\n15\n\n\n2023-07-24\n0.760158\n-0.111830\n-2.439989\n30.0\n21\n\n\n\n\n\n\n\n\n\nHistogramming\n\ns = pd.Series(np.random.randint(0, 7, size=10))\n\ns \n\n0    3\n1    0\n2    1\n3    5\n4    5\n5    2\n6    1\n7    1\n8    4\n9    2\ndtype: int64\n\n\n\ns.value_counts()\n\n1    3\n5    2\n2    2\n3    1\n0    1\n4    1\nName: count, dtype: int64\n\n\n\n\nString Methods\n\ns = pd.Series([\"A\", \"B\", \"C\", \"Aaba\", \"Baca\", np.nan, \"CABA\", \"dog\", \"cat\"])\n\ns.str.lower()\n\n0       a\n1       b\n2       c\n3    aaba\n4    baca\n5     NaN\n6    caba\n7     dog\n8     cat\ndtype: object"
  },
  {
    "objectID": "posts/pandas/2.2-Data_Preprocessing.html#merge",
    "href": "posts/pandas/2.2-Data_Preprocessing.html#merge",
    "title": "Data Preprocessing with Pandas",
    "section": "Merge",
    "text": "Merge\n\nConcat\n\ndf = pd.DataFrame(np.random.randn(10, 4))\ndf\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n0.153445\n-1.008416\n-0.505299\n0.025258\n\n\n1\n0.277799\n0.930983\n-0.367915\n-1.232603\n\n\n2\n0.469069\n-1.270812\n2.911260\n-1.190650\n\n\n3\n0.067151\n-0.616638\n-1.448431\n0.316322\n\n\n4\n-0.398150\n-0.282550\n0.085388\n0.825427\n\n\n5\n1.911145\n-0.163210\n0.014488\n0.059030\n\n\n6\n2.202431\n-0.210953\n-0.603069\n1.214237\n\n\n7\n-0.503438\n0.378269\n-0.917014\n0.189364\n\n\n8\n0.847221\n0.674547\n-3.213680\n-1.436244\n\n\n9\n0.117996\n-0.596137\n1.411912\n0.254441\n\n\n\n\n\n\n\n\n# break it into pieces\npieces = [df[:3], df[3:7], df[7:]]\npd.concat(pieces)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n0.153445\n-1.008416\n-0.505299\n0.025258\n\n\n1\n0.277799\n0.930983\n-0.367915\n-1.232603\n\n\n2\n0.469069\n-1.270812\n2.911260\n-1.190650\n\n\n3\n0.067151\n-0.616638\n-1.448431\n0.316322\n\n\n4\n-0.398150\n-0.282550\n0.085388\n0.825427\n\n\n5\n1.911145\n-0.163210\n0.014488\n0.059030\n\n\n6\n2.202431\n-0.210953\n-0.603069\n1.214237\n\n\n7\n-0.503438\n0.378269\n-0.917014\n0.189364\n\n\n8\n0.847221\n0.674547\n-3.213680\n-1.436244\n\n\n9\n0.117996\n-0.596137\n1.411912\n0.254441\n\n\n\n\n\n\n\n\n\n\n\n\n\nSide Note about Adding\n\n\n\n\n\nAdding a column to a DataFrame is relatively fast. However, adding a row requires a copy and may be expensive.\nRecommand passing a pre-built list of records to the DataFrame constructor instead of building a DataFrame by iteratively appending records to it.\n\n\n\n\n\nJoin\nmerge enables SQL style join types along specific columns.\n\nleft = pd.DataFrame({\"key\" : [\"foo\", \"foo\"], \"lval\" : [1, 2]})\nright = pd.DataFrame({\"key\" : [\"foo\", \"foo\"], \"rval\" : [4, 5]})\n\n\nleft\n\n\n\n\n\n\n\n\nkey\nlval\n\n\n\n\n0\nfoo\n1\n\n\n1\nfoo\n2\n\n\n\n\n\n\n\n\nright\n\n\n\n\n\n\n\n\nkey\nrval\n\n\n\n\n0\nfoo\n4\n\n\n1\nfoo\n5\n\n\n\n\n\n\n\n\npd.merge(left, right, on=\"key\")\n\n\n\n\n\n\n\n\nkey\nlval\nrval\n\n\n\n\n0\nfoo\n1\n4\n\n\n1\nfoo\n1\n5\n\n\n2\nfoo\n2\n4\n\n\n3\nfoo\n2\n5\n\n\n\n\n\n\n\n\nleft = pd.DataFrame({\"key\" : [\"foo\", \"bar\"], \"lval\" : [1, 2]})\nright = pd.DataFrame({\"key\" : [\"foo\", \"bar\"], \"rval\" : [4, 5]})\n\n\nleft\n\n\n\n\n\n\n\n\nkey\nlval\n\n\n\n\n0\nfoo\n1\n\n\n1\nbar\n2\n\n\n\n\n\n\n\n\nright\n\n\n\n\n\n\n\n\nkey\nrval\n\n\n\n\n0\nfoo\n4\n\n\n1\nbar\n5\n\n\n\n\n\n\n\n\npd.merge(left, right, on=\"key\")\n\n\n\n\n\n\n\n\nkey\nlval\nrval\n\n\n\n\n0\nfoo\n1\n4\n\n\n1\nbar\n2\n5"
  },
  {
    "objectID": "posts/pandas/2.2-Data_Preprocessing.html#grouping",
    "href": "posts/pandas/2.2-Data_Preprocessing.html#grouping",
    "title": "Data Preprocessing with Pandas",
    "section": "Grouping",
    "text": "Grouping\nGroup by : a process involving one or more of the following steps: - Splitting - Applying - Combining\n\ndf = pd.DataFrame(\n{\n    \"A\": [\"foo\", \"bar\", \"foo\", \"bar\", \"foo\", \"bar\", \"foo\", \"foo\"],\n    \"B\": [\"one\", \"one\", \"two\", \"three\", \"two\", \"two\", \"one\", \"three\"],\n    \"C\": np.random.randn(8),\n    \"D\": np.random.randn(8),\n})\n\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\nfoo\none\n2.062276\n-1.707116\n\n\n1\nbar\none\n2.239865\n0.517498\n\n\n2\nfoo\ntwo\n0.030384\n1.126816\n\n\n3\nbar\nthree\n0.962275\n-2.151044\n\n\n4\nfoo\ntwo\n-0.259296\n-0.056988\n\n\n5\nbar\ntwo\n0.270636\n-0.328337\n\n\n6\nfoo\none\n0.808535\n-0.625021\n\n\n7\nfoo\nthree\n-0.406189\n-0.006188\n\n\n\n\n\n\n\n\ndf.groupby(\"A\")[[\"C\", \"D\"]].sum()\n\n\n\n\n\n\n\n\nC\nD\n\n\nA\n\n\n\n\n\n\nbar\n3.472776\n-1.961884\n\n\nfoo\n2.235710\n-1.268498\n\n\n\n\n\n\n\n\ndf.groupby([\"B\", \"A\"]).sum()\n\n\n\n\n\n\n\n\n\nC\nD\n\n\nB\nA\n\n\n\n\n\n\none\nbar\n2.239865\n0.517498\n\n\nfoo\n2.870811\n-2.332137\n\n\nthree\nbar\n0.962275\n-2.151044\n\n\nfoo\n-0.406189\n-0.006188\n\n\ntwo\nbar\n0.270636\n-0.328337\n\n\nfoo\n-0.228912\n1.069828"
  },
  {
    "objectID": "posts/pandas/2.2-Data_Preprocessing.html#reshaping",
    "href": "posts/pandas/2.2-Data_Preprocessing.html#reshaping",
    "title": "Data Preprocessing with Pandas",
    "section": "Reshaping",
    "text": "Reshaping\n\ntuples = list(\n    zip(\n        [\"bar\", \"bar\", \"baz\", \"baz\", \"foo\", \"foo\", \"qux\", \"qux\"],\n        [\"one\", \"two\", \"one\", \"two\", \"one\", \"two\", \"one\", \"two\"],\n    )\n)\n\nindex = pd.MultiIndex.from_tuples(tuples, names=[\"first\", \"second\"])\n\ndf = pd.DataFrame(np.random.randn(8, 2), index=index, columns=[\"A\", \"B\"])\n\ndf2 = df[:4]\n\ndf2\n\n\n\n\n\n\n\n\n\nA\nB\n\n\nfirst\nsecond\n\n\n\n\n\n\nbar\none\n1.733548\n0.643470\n\n\ntwo\n1.266536\n1.682044\n\n\nbaz\none\n-0.773087\n-0.106262\n\n\ntwo\n-0.143492\n-0.695517\n\n\n\n\n\n\n\nstack() method compresses a level in the DataFrame’s columns\n\nstacked = df2.stack()\nstacked\n\nfirst  second   \nbar    one     A    1.733548\n               B    0.643470\n       two     A    1.266536\n               B    1.682044\nbaz    one     A   -0.773087\n               B   -0.106262\n       two     A   -0.143492\n               B   -0.695517\ndtype: float64\n\n\n\nstacked.unstack()\n\n\n\n\n\n\n\n\n\nA\nB\n\n\nfirst\nsecond\n\n\n\n\n\n\nbar\none\n1.733548\n0.643470\n\n\ntwo\n1.266536\n1.682044\n\n\nbaz\none\n-0.773087\n-0.106262\n\n\ntwo\n-0.143492\n-0.695517\n\n\n\n\n\n\n\n\nstacked.unstack(1)\n\n\n\n\n\n\n\n\nsecond\none\ntwo\n\n\nfirst\n\n\n\n\n\n\n\nbar\nA\n1.733548\n1.266536\n\n\nB\n0.643470\n1.682044\n\n\nbaz\nA\n-0.773087\n-0.143492\n\n\nB\n-0.106262\n-0.695517\n\n\n\n\n\n\n\n\nstacked.unstack(0)\n\n\n\n\n\n\n\n\nfirst\nbar\nbaz\n\n\nsecond\n\n\n\n\n\n\n\none\nA\n1.733548\n-0.773087\n\n\nB\n0.643470\n-0.106262\n\n\ntwo\nA\n1.266536\n-0.143492\n\n\nB\n1.682044\n-0.695517\n\n\n\n\n\n\n\n\nPivot tables\n\ndf = pd.DataFrame(\n{\n    \"A\": [\"one\", \"one\", \"two\", \"three\"] * 3,\n    \"B\": [\"A\", \"B\", \"C\"] * 4,\n    \"C\": [\"foo\", \"foo\", \"foo\", \"bar\", \"bar\", \"bar\"] * 2,\n    \"D\": np.random.randn(12),\n    \"E\": np.random.randn(12),\n}\n)\n\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n0\none\nA\nfoo\n0.043140\n-1.331849\n\n\n1\none\nB\nfoo\n-0.476268\n0.664963\n\n\n2\ntwo\nC\nfoo\n-1.018212\n1.279238\n\n\n3\nthree\nA\nbar\n-0.473956\n1.173957\n\n\n4\none\nB\nbar\n0.309105\n2.049337\n\n\n5\none\nC\nbar\n0.338232\n0.813336\n\n\n6\ntwo\nA\nfoo\n-0.167626\n-1.301410\n\n\n7\nthree\nB\nfoo\n1.412266\n0.991458\n\n\n8\none\nC\nfoo\n-0.507957\n0.678728\n\n\n9\none\nA\nbar\n-0.304315\n-0.625889\n\n\n10\ntwo\nB\nbar\n0.518126\n1.607797\n\n\n11\nthree\nC\nbar\n0.888700\n1.490596\n\n\n\n\n\n\n\n\npd.pivot_table(df, values=\"D\", index=[\"A\", \"B\"], columns=\"C\")\n\n\n\n\n\n\n\n\nC\nbar\nfoo\n\n\nA\nB\n\n\n\n\n\n\none\nA\n-0.304315\n0.043140\n\n\nB\n0.309105\n-0.476268\n\n\nC\n0.338232\n-0.507957\n\n\nthree\nA\n-0.473956\nNaN\n\n\nB\nNaN\n1.412266\n\n\nC\n0.888700\nNaN\n\n\ntwo\nA\nNaN\n-0.167626\n\n\nB\n0.518126\nNaN\n\n\nC\nNaN\n-1.018212"
  },
  {
    "objectID": "posts/pandas/2.2-Data_Preprocessing.html#time-series",
    "href": "posts/pandas/2.2-Data_Preprocessing.html#time-series",
    "title": "Data Preprocessing with Pandas",
    "section": "Time series",
    "text": "Time series\n\nrng = pd.date_range(\"1/1/2012\", periods=100, freq=\"S\")\n\nts = pd.Series(np.random.randint(0, 500, len(rng)), index=rng)\n\nts.resample(\"5Min\").sum()\n\n2012-01-01    24080\nFreq: 5T, dtype: int64\n\n\n\nts_utc = ts.tz_localize(\"UTC\")\nts_utc\n\n2012-01-01 00:00:00+00:00    349\n2012-01-01 00:00:01+00:00    369\n2012-01-01 00:00:02+00:00    312\n2012-01-01 00:00:03+00:00    215\n2012-01-01 00:00:04+00:00    202\n                            ... \n2012-01-01 00:01:35+00:00    195\n2012-01-01 00:01:36+00:00    186\n2012-01-01 00:01:37+00:00    368\n2012-01-01 00:01:38+00:00     43\n2012-01-01 00:01:39+00:00    354\nFreq: S, Length: 100, dtype: int64\n\n\n\nts_utc.tz_convert(\"US/Eastern\")\n\n2011-12-31 19:00:00-05:00    349\n2011-12-31 19:00:01-05:00    369\n2011-12-31 19:00:02-05:00    312\n2011-12-31 19:00:03-05:00    215\n2011-12-31 19:00:04-05:00    202\n                            ... \n2011-12-31 19:01:35-05:00    195\n2011-12-31 19:01:36-05:00    186\n2011-12-31 19:01:37-05:00    368\n2011-12-31 19:01:38-05:00     43\n2011-12-31 19:01:39-05:00    354\nFreq: S, Length: 100, dtype: int64\n\n\n\nrng = pd.date_range(\"1/1/2012\", periods=5, freq=\"M\")\n\nts = pd.Series(np.random.randn(len(rng)) , index=rng)\nts\n\n2012-01-31    0.711697\n2012-02-29   -0.309698\n2012-03-31   -0.076134\n2012-04-30    0.422104\n2012-05-31   -2.403484\nFreq: M, dtype: float64\n\n\n\nps = ts.to_period()\nps\n\n2012-01    0.711697\n2012-02   -0.309698\n2012-03   -0.076134\n2012-04    0.422104\n2012-05   -2.403484\nFreq: M, dtype: float64\n\n\n\nps.to_timestamp()\n\n2012-01-01    0.711697\n2012-02-01   -0.309698\n2012-03-01   -0.076134\n2012-04-01    0.422104\n2012-05-01   -2.403484\nFreq: MS, dtype: float64\n\n\n\nprng = pd.period_range(\"1990Q1\", \"2000Q4\", freq=\"Q-NOV\")\nts = pd.Series(np.random.randn(len(prng)), prng)\n\nts.index = (prng.asfreq('M', 'e') + 1).asfreq('H', 's') + 9\n\nts.head()\n\n1990-03-01 09:00    0.789300\n1990-06-01 09:00   -1.827196\n1990-09-01 09:00   -0.662090\n1990-12-01 09:00    0.810399\n1991-03-01 09:00    0.658745\nFreq: H, dtype: float64"
  },
  {
    "objectID": "posts/pandas/2.2-Data_Preprocessing.html#categoricals",
    "href": "posts/pandas/2.2-Data_Preprocessing.html#categoricals",
    "title": "Data Preprocessing with Pandas",
    "section": "Categoricals",
    "text": "Categoricals\n\ndf = pd.DataFrame(\n    {\"id\": [1, 2, 3, 4, 5, 6], \"raw_grade\": [\"a\", \"b\", \"b\", \"a\", \"a\", \"e\"]}\n)\n\n\ndf[\"grade\"] = df[\"raw_grade\"].astype(\"category\")\ndf[\"grade\"]\n\n0    a\n1    b\n2    b\n3    a\n4    a\n5    e\nName: grade, dtype: category\nCategories (3, object): ['a', 'b', 'e']\n\n\n\nnew_categories = [\"very good\", \"good\", \"very bad\"]\n\ndf[\"grade\"] = df[\"grade\"].cat.rename_categories(new_categories)\ndf[\"grade\"]\n\n0    very good\n1         good\n2         good\n3    very good\n4    very good\n5     very bad\nName: grade, dtype: category\nCategories (3, object): ['very good', 'good', 'very bad']\n\n\n\ndf[\"grade\"] = df[\"grade\"].cat.set_categories(\n    [\"very bad\", \"bad\", \"medium\", \"good\", \"very good\"]\n)\n\ndf[\"grade\"]\n\n0    very good\n1         good\n2         good\n3    very good\n4    very good\n5     very bad\nName: grade, dtype: category\nCategories (5, object): ['very bad', 'bad', 'medium', 'good', 'very good']\n\n\n\ndf.sort_values(by=\"grade\")\n\n\n\n\n\n\n\n\nid\nraw_grade\ngrade\n\n\n\n\n5\n6\ne\nvery bad\n\n\n1\n2\nb\ngood\n\n\n2\n3\nb\ngood\n\n\n0\n1\na\nvery good\n\n\n3\n4\na\nvery good\n\n\n4\n5\na\nvery good\n\n\n\n\n\n\n\n\ndf.groupby(\"grade\").size()\n\ngrade\nvery bad     1\nbad          0\nmedium       0\ngood         2\nvery good    3\ndtype: int64"
  },
  {
    "objectID": "posts/pandas/2.2-Data_Preprocessing.html#plotting",
    "href": "posts/pandas/2.2-Data_Preprocessing.html#plotting",
    "title": "Data Preprocessing with Pandas",
    "section": "Plotting",
    "text": "Plotting\n\nimport matplotlib.pyplot as plt\n\nplt.close(\"all\")\n\n\nts = pd.Series(np.random.randn(1000), index=pd.date_range(\"1/1/2000\", periods=1000))\n\nts = ts.cumsum()\n\nts.plot()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\ndf = pd.DataFrame(\n    np.random.randn(1000, 4), index=ts.index, columns=[\"A\", \"B\", \"C\", \"D\"]\n)\n\ndf = df.cumsum()\n\nplt.figure()\n\ndf.plot()\n\nplt.legend(loc=\"best\")\n\n&lt;matplotlib.legend.Legend at 0x7ff509ff9a20&gt;\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;"
  },
  {
    "objectID": "posts/pandas/2.2-Data_Preprocessing.html#importing-and-exporting-data",
    "href": "posts/pandas/2.2-Data_Preprocessing.html#importing-and-exporting-data",
    "title": "Data Preprocessing with Pandas",
    "section": "Importing and exporting data",
    "text": "Importing and exporting data\n\ndf.to_csv(\"foo.csv\")\n\n\npd.read_csv(\"foo.csv\")\n\n\n\n\n\n\n\n\nUnnamed: 0\nA\nB\nC\nD\n\n\n\n\n0\n2000-01-01\n0.119829\n0.256524\n-0.021635\n-0.553218\n\n\n1\n2000-01-02\n-0.519971\n-0.113309\n0.710283\n0.269571\n\n\n2\n2000-01-03\n-1.643742\n-0.387483\n0.172760\n0.842172\n\n\n3\n2000-01-04\n-1.505119\n0.233312\n-1.027990\n1.072677\n\n\n4\n2000-01-05\n-0.515360\n-0.130559\n-2.820043\n0.949913\n\n\n...\n...\n...\n...\n...\n...\n\n\n995\n2002-09-22\n-12.310366\n41.116491\n38.013362\n39.133126\n\n\n996\n2002-09-23\n-12.070904\n43.043589\n38.085873\n39.285890\n\n\n997\n2002-09-24\n-11.423160\n42.627345\n39.041164\n40.857578\n\n\n998\n2002-09-25\n-11.291670\n41.719354\n37.736052\n39.475829\n\n\n999\n2002-09-26\n-10.932869\n39.653972\n36.900024\n40.507333\n\n\n\n\n1000 rows × 5 columns\n\n\n\n\nHDF5\n\ndf.to_hdf(\"foo.h5\", \"df\")\n\n\npd.read_hdf(\"foo.h5\", \"df\")\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2000-01-01\n0.119829\n0.256524\n-0.021635\n-0.553218\n\n\n2000-01-02\n-0.519971\n-0.113309\n0.710283\n0.269571\n\n\n2000-01-03\n-1.643742\n-0.387483\n0.172760\n0.842172\n\n\n2000-01-04\n-1.505119\n0.233312\n-1.027990\n1.072677\n\n\n2000-01-05\n-0.515360\n-0.130559\n-2.820043\n0.949913\n\n\n...\n...\n...\n...\n...\n\n\n2002-09-22\n-12.310366\n41.116491\n38.013362\n39.133126\n\n\n2002-09-23\n-12.070904\n43.043589\n38.085873\n39.285890\n\n\n2002-09-24\n-11.423160\n42.627345\n39.041164\n40.857578\n\n\n2002-09-25\n-11.291670\n41.719354\n37.736052\n39.475829\n\n\n2002-09-26\n-10.932869\n39.653972\n36.900024\n40.507333\n\n\n\n\n1000 rows × 4 columns\n\n\n\n\n\nExcel\n\ndf.to_excel(\"foo.xlsx\", sheet_name=\"Sheet1\")\n\n\npd.read_excel(\"foo.xlsx\", \"Sheet1\", index_col=None, na_values=[\"NA\"])\n\n\n\n\n\n\n\n\nUnnamed: 0\nA\nB\nC\nD\n\n\n\n\n0\n2000-01-01\n0.119829\n0.256524\n-0.021635\n-0.553218\n\n\n1\n2000-01-02\n-0.519971\n-0.113309\n0.710283\n0.269571\n\n\n2\n2000-01-03\n-1.643742\n-0.387483\n0.172760\n0.842172\n\n\n3\n2000-01-04\n-1.505119\n0.233312\n-1.027990\n1.072677\n\n\n4\n2000-01-05\n-0.515360\n-0.130559\n-2.820043\n0.949913\n\n\n...\n...\n...\n...\n...\n...\n\n\n995\n2002-09-22\n-12.310366\n41.116491\n38.013362\n39.133126\n\n\n996\n2002-09-23\n-12.070904\n43.043589\n38.085873\n39.285890\n\n\n997\n2002-09-24\n-11.423160\n42.627345\n39.041164\n40.857578\n\n\n998\n2002-09-25\n-11.291670\n41.719354\n37.736052\n39.475829\n\n\n999\n2002-09-26\n-10.932869\n39.653972\n36.900024\n40.507333\n\n\n\n\n1000 rows × 5 columns"
  },
  {
    "objectID": "posts/mlflow/experiment_tracking.html",
    "href": "posts/mlflow/experiment_tracking.html",
    "title": "Experiment Tracking with MLflow",
    "section": "",
    "text": "In this tutorial, we are going to train a simple regression model. While training a model, we are going to use an experiment tracking tool called mlflow.\nWhat is mlflow?\nMLflow is a open-source experiment tracking tool. We can use mlflow to track experiments, experiment runs, artifacts related to experiment runs. MLflow has five components:\n- MLflow Tracking\n- MLflow Models\n- MLflow Model Registry\n- MLflow Projects\n- MLflow Recipes\nWe are only going to use Tracking, Models and Model Registry here. You can see the rest here in the mlflow docs.\nThis blog or notebook is the notes for week 2 ( experiment tracking ) lectures of the datatalks mlops zoomcamp.\n\n\nWe can install mlflow using pip or conda.\n\n# !pip install mlflow \n\n# or \n\n# !conda install -c conda-forge mlflow\n\n\n\n\nHere we are going to import a list of libraries that we need for this tutorial.\n\n!python --version\n\nPython 3.9.0\n\n\n\nfrom fastdownload import download_url\nfrom pathlib import Path\nimport pandas as pd\n\nfrom sklearn.linear_model import Lasso, LinearRegression\nfrom sklearn.metrics import mean_squared_error \n\nimport mlflow\nfrom mlflow.models.signature import infer_signature\n\n\n\n\nBefore starting any training or data preprocessing, we start by setting tracking uri and experiment for this mlflow experiment. Tracking uri can be\n- localhost\n- localhost with SQlite\n- localhost with tracking server\n- remote tracking server, backend and artifact stores\nIn this tutorial, we will start with localhost option and we will also use remote tracking server option in the second half of this tutorial.\n\n\n\nIn a localhost setting, the backend and artifact store share a local folder called ./mlruns.\n\nmlflow.set_tracking_uri(\"mlruns\")\nmlflow.set_experiment(\"Experiment-1\")\n\n2023/07/17 18:58:07 INFO mlflow.tracking.fluent: Experiment with name 'Experiment-1' does not exist. Creating a new experiment.\n\n\n&lt;Experiment: artifact_location='/home/shane/mlops-practice/experiment-tracking/mlruns/280918629490989571', creation_time=1689596887482, experiment_id='280918629490989571', last_update_time=1689596887482, lifecycle_stage='active', name='Experiment-1', tags={}&gt;\n\n\nmlflow.set_experiment() is for setting the experiment name and we can see that a new experiment is created.\n\n!tree .\n\n.\n├── env.yaml\n├── experiment_tracking.ipynb\n├── imgs\n│   ├── mlflow_run1.png\n│   ├── mlflow_ui.png\n│   ├── model.png\n│   └── run1_metadata.png\n└── mlruns\n    ├── 0\n    │   └── meta.yaml\n    └── 280918629490989571\n        └── meta.yaml\n\n4 directories, 8 files\n\n\nWe can run mlflow ui in terminal to access, well, mlflow ui.\n:\n\n\n\nThe dataset that we are using for this tutorial is NYC TLC Trip Record dataset.\nWe are going to predict duration of a taxi ride.\n\ndata_path = Path('data')\nif not data_path.exists():\n    data_path.mkdir(exist_ok=True)\n\nFirst, we are creating a data folder data to which we are going to download the datasets.\n\ndef download_data(year : int, month : int, data_path : Path) -&gt; None:\n    '''download nyc green taxi data in parquet form and save it in data_path'''\n    url = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_{year}-{month:0&gt;2}.parquet\"\n    download_url(url, dest=data_path, show_progress=True)\n\ndownload_data(2023, 1, data_path)\ndownload_data(2023, 2, data_path)\n\n\n\n\n\n\n    \n      \n      100.46% [1433600/1427002 00:01&lt;00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.41% [1540096/1533740 00:01&lt;00:00]\n    \n    \n\n\nQuick side note for pyformatting,\n\nmonth = 2\nprint(f\"month == {month} : {month:0&gt;2}\")\n\nmonth = 11\nprint(f\"month == {month} : {month:0&gt;2}\")\n\nmonth == 2 : 02\nmonth == 11 : 11\n\n\n\n!tree .\n\n.\n├── data\n│   ├── green_tripdata_2023-01.parquet\n│   └── green_tripdata_2023-02.parquet\n├── env.yaml\n├── experiment_tracking.ipynb\n├── imgs\n│   ├── mlflow_run1.png\n│   ├── mlflow_ui.png\n│   ├── model.png\n│   └── run1_metadata.png\n└── mlruns\n    ├── 0\n    │   └── meta.yaml\n    └── 280918629490989571\n        └── meta.yaml\n\n5 directories, 10 files\n\n\nDatasets are downloaded using fastdownload library from fastai.\n\n\n\n\ndef read_dataframe(filename: Path)-&gt; pd.DataFrame:\n    df = pd.read_parquet(filename)\n    df[\"duration\"] = df[\"lpep_dropoff_datetime\"] - df[\"lpep_pickup_datetime\"]\n    df.duration = df[\"duration\"].apply(lambda td : td.total_seconds() / 60)\n    df = df[(df.duration  &gt;= 0) & (df.duration &lt;= 60) ]\n\n    categorical_data = [\"PULocationID\", \"DOLocationID\"]\n    df[categorical_data] = df[categorical_data].astype(str)\n    df[\"PU_DO\"] = df['PULocationID'] + \"_\" + df['DOLocationID']\n\n    return df\n\nSince the files are in parquet format, we use pd.read_parquet() method.\n\n# df[\"duration\"] = df[\"lpep_dropoff_datetime\"] - df[\"lpep_pickup_datetime\"]\n# df.duration = df[\"duration\"].apply(lambda td : td.total_seconds() / 60)\n\nWe want to calculate the duration of each trip. The trip duration is calculated by subtracting pickup datetime from dropoff datetime.\nWe also want to get the duration in minute. We get the total seconds and divided by 60.\n\ndf = read_dataframe(\"data/green_tripdata_2023-01.parquet\")\ndf_val = read_dataframe(\"data/green_tripdata_2023-02.parquet\")\n\n\ndef preprocess(df : pd.DataFrame) -&gt; tuple((pd.DataFrame, pd.DataFrame)):\n    categorical = [\"PU_DO\"]\n    numerical   = [\"trip_distance\", \"fare_amount\", \"total_amount\"]\n\n    X = df[categorical + numerical]\n    y = df.duration\n\n    return X, y\n    \n\n\nX, y = preprocess(df)\nX_val, y_val = preprocess(df_val)\n\nAfter loading train and validation dataset and performing preprocessing, we now have features X and targets y.\nNow we can start training the model.\n\n\n\n\ndef train(X, y):\n    model = LinearRegression()\n    model.fit(X,y)\n\n    return model\n\nWe will initialize mlflow run as\n\n# with mlflow.start_run() as run:\n\nand wrap up the training inside of it.\n- `set_tag` : for tracking metadata\n- `log_param` : for logging parameters\n- `log_metric` : for logging metric\nIn the example below, we use set_tag for tracking developer name, log_param for tracking data folder used for training and validation ,and log_metric for tracking validation rmse metric.\nSome other useful methods are :\n- `set_tags` : Log a batch of tags for the current run.\n- `log_params` : Log a batch of params for the current run.\n- `log_artifact` : Log a local file or directory as an artifact of the currently active run.\n- `log_artifacts` : Log all the contents of a local directory as artifacts of the run\n\nwith mlflow.start_run() as run:\n\n    mlflow.set_tag(\"Developer\", \"Shane\")\n    mlflow.log_param(\"Train-data-path\", \"data/green_tripdata_2023-01.parquet\")\n    mlflow.log_param(\"Valid-data-path\", \"data/green_tripdata_2023-02.parquet\")\n    \n    model = train(X, y)\n\n    preds = model.predict(X_val)\n\n    rmse = mean_squared_error(y_val, preds, squared=False)\n\n    mlflow.log_metric('RMSE', rmse)\n    signature = infer_signature(X_val, preds)\n    model_uri = mlflow.sklearn.log_model(model, artifact_path=\"model\", signature=signature).model_uri\n   \n\n\n\n\nfist run\n\n\nThe first run can be seen in the above picture with the name, shivering-panda-266. It is a random run name since we didn’t set a specific run name.\n\n\n\nrun1\n\n\nWe can see the Train-data-path, Valid-data-path in the Parameters section, RMSE in metrics section and Developer in the Tags section.\n\n\n\n\n# signature = infer_signature(X_val, preds)\n# mlflow.sklearn.log_model(model, artifact_path=\"model\", signature=signature)\n\nModel signatures define input and output schemas for MLflow models. Model signature is obtained here using infer_signature.\nWe can log a model using mlflow.&lt;framework&gt;.log_model. In this case, we are using mlflow.sklearn.log_model.\n\n\n\nmodel1\n\n\nSince we add signature parameter, we can see the model input and output schema here. We can also see two ways that we can load the model and make predictions.\n\n\n\nWe can also do auto logging by using mlflow.&lt;framework&gt;.autolog()\n\n# mlflow.sklearn.autolog()\n\nAutologging is known to be compatible with the following package versions: 0.22.1 &lt;= scikit-learn &lt;= 1.2.2. Autologging may not succeed when used with package versions outside of this range.\n\n\n\nWe have saved the model using log_model. Now, we are going to load that model for prediction.\n\nfrom mlflow import MlflowClient\n\nclient = MlflowClient(tracking_uri=\"mlruns\")\n\n\nexperiments = client.search_experiments()\n\nfor experiment in experiments:\n    print(f\"Experiment Name : {experiment.name}\")\n    print(f\"\\tExperiment id :{experiment.experiment_id}\")\n    print(f\"\\tArtifact Location :{experiment.artifact_location}\\n\")\n\nExperiment Name : Experiment-1\n    Experiment id :280918629490989571\n    Artifact Location :/home/shane/mlops-practice/experiment-tracking/mlruns/280918629490989571\n\nExperiment Name : Default\n    Experiment id :0\n    Artifact Location :/home/shane/mlops-practice/experiment-tracking/mlruns/0\n\n\n\nOur experiment’s name is “Experiment-1” and the Experiment id for that is “146920015920581846”.\n\nfor experiment in experiments:\n    if experiment.name == \"Experiment-1\":\n        exp = experiment\n\n\nexp_id = exp.experiment_id\nruns = client.search_runs(experiment_ids=[exp_id])\n\nfor run in runs:\n    print(f\"run name : {run.info.run_name}\")\n    print(f\"\\trun id : {run.info.run_id}\")\n    print(f\"\\trmse : {run.data.metrics['RMSE']}\")\n\nrun name : charming-toad-155\n    run id : 9ed0390d8a4b44dca36f36ee25d20ba4\n    rmse : 6.282692349434918\n\n\nLet’s pick a run_id of the model that we want to load.\n\nrun_id = runs[0].info.run_id\nlogged_model = f'runs:/{run_id}/model'\n\n# Load model as a PyFuncModel.\nloaded_model = mlflow.pyfunc.load_model(logged_model)\n\n# Predict on a Pandas DataFrame.\nimport pandas as pd\nloaded_model.predict(X_val)\n\n2023/07/17 18:59:06 WARNING mlflow.pyfunc: Detected one or more mismatches between the model's dependencies and the current Python environment:\n - mlflow (current: 2.4.2, required: mlflow==2.4)\nTo fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n\n\narray([26.09040939, 16.31353026, 22.14756441, ..., 15.42763737,\n       14.91915344, 12.24746033])\n\n\n\n\n\nModel Registry is a centralized model store, a set of APIs and UI. It provides\n    - model lineage, \n    - model versioning, \n    - stage transition, and \n    - annotations.\n\nmlflow.set_tracking_uri('mlruns/')\n\nmodel_uri = f\"runs:/{run_id}/models\"\nmlflow.register_model(model_uri=model_uri, name=\"nyc-taxi-regressor\")\n\nSuccessfully registered model 'nyc-taxi-regressor'.\n2023/07/17 18:59:10 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: nyc-taxi-regressor, version 1\nCreated version '1' of model 'nyc-taxi-regressor'.\n\n\n&lt;ModelVersion: aliases=[], creation_timestamp=1689596950866, current_stage='None', description=None, last_updated_timestamp=1689596950866, name='nyc-taxi-regressor', run_id='9ed0390d8a4b44dca36f36ee25d20ba4', run_link=None, source='/home/shane/mlops-practice/experiment-tracking/mlruns/280918629490989571/9ed0390d8a4b44dca36f36ee25d20ba4/artifacts/models', status='READY', status_message=None, tags={}, user_id=None, version=1&gt;\n\n\nWe have now registered the model under the name of “nyc-taxi-regressor’.\nWe can see that the version of the model is now 1. Since this is the first model registered under this name.\nBut It doesn’t have any staging information.\n\nmodel_name = \"nyc-taxi-regressor\"\nlatest_versions = client.get_latest_versions(name=model_name)\n\nfor version in latest_versions:\n    print(f\"version: {version.version}, stage: {version.current_stage}\")\n\nversion: 1, stage: None\n\n\nWe can transition the model version and stages using transition_model_version_stage.\n\nmodel_version = 1\nnew_stage = \"Staging\"\nclient.transition_model_version_stage(\n    name=model_name,\n    version=model_version,\n    stage=new_stage,\n    archive_existing_versions=False\n)\n\n&lt;ModelVersion: aliases=[], creation_timestamp=1689596950866, current_stage='Staging', description=None, last_updated_timestamp=1689596955099, name='nyc-taxi-regressor', run_id='9ed0390d8a4b44dca36f36ee25d20ba4', run_link=None, source='/home/shane/mlops-practice/experiment-tracking/mlruns/280918629490989571/9ed0390d8a4b44dca36f36ee25d20ba4/artifacts/models', status='READY', status_message=None, tags={}, user_id=None, version=1&gt;\n\n\n\nmodel_name = \"nyc-taxi-regressor\"\nlatest_versions = client.get_latest_versions(name=model_name)\n\nfor version in latest_versions:\n    print(f\"version: {version.version}, stage: {version.current_stage}\")\n\nversion: 1, stage: Staging\n\n\nNow, we can see that the stage of the model is now Staging.\n\nfrom datetime import datetime\n\ndate = datetime.today().date()\nclient.update_model_version(\n    name=model_name,\n    version=model_version,\n    description=f\"The model version {model_version} was transitioned to {new_stage} on {date}\"\n)\n\n&lt;ModelVersion: aliases=[], creation_timestamp=1689596950866, current_stage='Staging', description='The model version 1 was transitioned to Staging on 2023-07-17', last_updated_timestamp=1689596958723, name='nyc-taxi-regressor', run_id='9ed0390d8a4b44dca36f36ee25d20ba4', run_link=None, source='/home/shane/mlops-practice/experiment-tracking/mlruns/280918629490989571/9ed0390d8a4b44dca36f36ee25d20ba4/artifacts/models', status='READY', status_message=None, tags={}, user_id=None, version=1&gt;\n\n\nWe could also add description of the models like datetime information above."
  },
  {
    "objectID": "posts/mlflow/experiment_tracking.html#install",
    "href": "posts/mlflow/experiment_tracking.html#install",
    "title": "Experiment Tracking with MLflow",
    "section": "",
    "text": "We can install mlflow using pip or conda.\n\n# !pip install mlflow \n\n# or \n\n# !conda install -c conda-forge mlflow"
  },
  {
    "objectID": "posts/mlflow/experiment_tracking.html#imports",
    "href": "posts/mlflow/experiment_tracking.html#imports",
    "title": "Experiment Tracking with MLflow",
    "section": "",
    "text": "Here we are going to import a list of libraries that we need for this tutorial.\n\n!python --version\n\nPython 3.9.0\n\n\n\nfrom fastdownload import download_url\nfrom pathlib import Path\nimport pandas as pd\n\nfrom sklearn.linear_model import Lasso, LinearRegression\nfrom sklearn.metrics import mean_squared_error \n\nimport mlflow\nfrom mlflow.models.signature import infer_signature"
  },
  {
    "objectID": "posts/mlflow/experiment_tracking.html#tracking-uri",
    "href": "posts/mlflow/experiment_tracking.html#tracking-uri",
    "title": "Experiment Tracking with MLflow",
    "section": "",
    "text": "Before starting any training or data preprocessing, we start by setting tracking uri and experiment for this mlflow experiment. Tracking uri can be\n- localhost\n- localhost with SQlite\n- localhost with tracking server\n- remote tracking server, backend and artifact stores\nIn this tutorial, we will start with localhost option and we will also use remote tracking server option in the second half of this tutorial."
  },
  {
    "objectID": "posts/mlflow/experiment_tracking.html#localhost",
    "href": "posts/mlflow/experiment_tracking.html#localhost",
    "title": "Experiment Tracking with MLflow",
    "section": "",
    "text": "In a localhost setting, the backend and artifact store share a local folder called ./mlruns.\n\nmlflow.set_tracking_uri(\"mlruns\")\nmlflow.set_experiment(\"Experiment-1\")\n\n2023/07/17 18:58:07 INFO mlflow.tracking.fluent: Experiment with name 'Experiment-1' does not exist. Creating a new experiment.\n\n\n&lt;Experiment: artifact_location='/home/shane/mlops-practice/experiment-tracking/mlruns/280918629490989571', creation_time=1689596887482, experiment_id='280918629490989571', last_update_time=1689596887482, lifecycle_stage='active', name='Experiment-1', tags={}&gt;\n\n\nmlflow.set_experiment() is for setting the experiment name and we can see that a new experiment is created.\n\n!tree .\n\n.\n├── env.yaml\n├── experiment_tracking.ipynb\n├── imgs\n│   ├── mlflow_run1.png\n│   ├── mlflow_ui.png\n│   ├── model.png\n│   └── run1_metadata.png\n└── mlruns\n    ├── 0\n    │   └── meta.yaml\n    └── 280918629490989571\n        └── meta.yaml\n\n4 directories, 8 files\n\n\nWe can run mlflow ui in terminal to access, well, mlflow ui.\n:"
  },
  {
    "objectID": "posts/mlflow/experiment_tracking.html#downloading-dataset",
    "href": "posts/mlflow/experiment_tracking.html#downloading-dataset",
    "title": "Experiment Tracking with MLflow",
    "section": "",
    "text": "The dataset that we are using for this tutorial is NYC TLC Trip Record dataset.\nWe are going to predict duration of a taxi ride.\n\ndata_path = Path('data')\nif not data_path.exists():\n    data_path.mkdir(exist_ok=True)\n\nFirst, we are creating a data folder data to which we are going to download the datasets.\n\ndef download_data(year : int, month : int, data_path : Path) -&gt; None:\n    '''download nyc green taxi data in parquet form and save it in data_path'''\n    url = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_{year}-{month:0&gt;2}.parquet\"\n    download_url(url, dest=data_path, show_progress=True)\n\ndownload_data(2023, 1, data_path)\ndownload_data(2023, 2, data_path)\n\n\n\n\n\n\n    \n      \n      100.46% [1433600/1427002 00:01&lt;00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.41% [1540096/1533740 00:01&lt;00:00]\n    \n    \n\n\nQuick side note for pyformatting,\n\nmonth = 2\nprint(f\"month == {month} : {month:0&gt;2}\")\n\nmonth = 11\nprint(f\"month == {month} : {month:0&gt;2}\")\n\nmonth == 2 : 02\nmonth == 11 : 11\n\n\n\n!tree .\n\n.\n├── data\n│   ├── green_tripdata_2023-01.parquet\n│   └── green_tripdata_2023-02.parquet\n├── env.yaml\n├── experiment_tracking.ipynb\n├── imgs\n│   ├── mlflow_run1.png\n│   ├── mlflow_ui.png\n│   ├── model.png\n│   └── run1_metadata.png\n└── mlruns\n    ├── 0\n    │   └── meta.yaml\n    └── 280918629490989571\n        └── meta.yaml\n\n5 directories, 10 files\n\n\nDatasets are downloaded using fastdownload library from fastai."
  },
  {
    "objectID": "posts/mlflow/experiment_tracking.html#preprocessing",
    "href": "posts/mlflow/experiment_tracking.html#preprocessing",
    "title": "Experiment Tracking with MLflow",
    "section": "",
    "text": "def read_dataframe(filename: Path)-&gt; pd.DataFrame:\n    df = pd.read_parquet(filename)\n    df[\"duration\"] = df[\"lpep_dropoff_datetime\"] - df[\"lpep_pickup_datetime\"]\n    df.duration = df[\"duration\"].apply(lambda td : td.total_seconds() / 60)\n    df = df[(df.duration  &gt;= 0) & (df.duration &lt;= 60) ]\n\n    categorical_data = [\"PULocationID\", \"DOLocationID\"]\n    df[categorical_data] = df[categorical_data].astype(str)\n    df[\"PU_DO\"] = df['PULocationID'] + \"_\" + df['DOLocationID']\n\n    return df\n\nSince the files are in parquet format, we use pd.read_parquet() method.\n\n# df[\"duration\"] = df[\"lpep_dropoff_datetime\"] - df[\"lpep_pickup_datetime\"]\n# df.duration = df[\"duration\"].apply(lambda td : td.total_seconds() / 60)\n\nWe want to calculate the duration of each trip. The trip duration is calculated by subtracting pickup datetime from dropoff datetime.\nWe also want to get the duration in minute. We get the total seconds and divided by 60.\n\ndf = read_dataframe(\"data/green_tripdata_2023-01.parquet\")\ndf_val = read_dataframe(\"data/green_tripdata_2023-02.parquet\")\n\n\ndef preprocess(df : pd.DataFrame) -&gt; tuple((pd.DataFrame, pd.DataFrame)):\n    categorical = [\"PU_DO\"]\n    numerical   = [\"trip_distance\", \"fare_amount\", \"total_amount\"]\n\n    X = df[categorical + numerical]\n    y = df.duration\n\n    return X, y\n    \n\n\nX, y = preprocess(df)\nX_val, y_val = preprocess(df_val)\n\nAfter loading train and validation dataset and performing preprocessing, we now have features X and targets y.\nNow we can start training the model."
  },
  {
    "objectID": "posts/mlflow/experiment_tracking.html#model-training",
    "href": "posts/mlflow/experiment_tracking.html#model-training",
    "title": "Experiment Tracking with MLflow",
    "section": "",
    "text": "def train(X, y):\n    model = LinearRegression()\n    model.fit(X,y)\n\n    return model\n\nWe will initialize mlflow run as\n\n# with mlflow.start_run() as run:\n\nand wrap up the training inside of it.\n- `set_tag` : for tracking metadata\n- `log_param` : for logging parameters\n- `log_metric` : for logging metric\nIn the example below, we use set_tag for tracking developer name, log_param for tracking data folder used for training and validation ,and log_metric for tracking validation rmse metric.\nSome other useful methods are :\n- `set_tags` : Log a batch of tags for the current run.\n- `log_params` : Log a batch of params for the current run.\n- `log_artifact` : Log a local file or directory as an artifact of the currently active run.\n- `log_artifacts` : Log all the contents of a local directory as artifacts of the run\n\nwith mlflow.start_run() as run:\n\n    mlflow.set_tag(\"Developer\", \"Shane\")\n    mlflow.log_param(\"Train-data-path\", \"data/green_tripdata_2023-01.parquet\")\n    mlflow.log_param(\"Valid-data-path\", \"data/green_tripdata_2023-02.parquet\")\n    \n    model = train(X, y)\n\n    preds = model.predict(X_val)\n\n    rmse = mean_squared_error(y_val, preds, squared=False)\n\n    mlflow.log_metric('RMSE', rmse)\n    signature = infer_signature(X_val, preds)\n    model_uri = mlflow.sklearn.log_model(model, artifact_path=\"model\", signature=signature).model_uri\n   \n\n\n\n\nfist run\n\n\nThe first run can be seen in the above picture with the name, shivering-panda-266. It is a random run name since we didn’t set a specific run name.\n\n\n\nrun1\n\n\nWe can see the Train-data-path, Valid-data-path in the Parameters section, RMSE in metrics section and Developer in the Tags section."
  },
  {
    "objectID": "posts/mlflow/experiment_tracking.html#model-saving",
    "href": "posts/mlflow/experiment_tracking.html#model-saving",
    "title": "Experiment Tracking with MLflow",
    "section": "",
    "text": "# signature = infer_signature(X_val, preds)\n# mlflow.sklearn.log_model(model, artifact_path=\"model\", signature=signature)\n\nModel signatures define input and output schemas for MLflow models. Model signature is obtained here using infer_signature.\nWe can log a model using mlflow.&lt;framework&gt;.log_model. In this case, we are using mlflow.sklearn.log_model.\n\n\n\nmodel1\n\n\nSince we add signature parameter, we can see the model input and output schema here. We can also see two ways that we can load the model and make predictions."
  },
  {
    "objectID": "posts/mlflow/experiment_tracking.html#auto-logging",
    "href": "posts/mlflow/experiment_tracking.html#auto-logging",
    "title": "Experiment Tracking with MLflow",
    "section": "",
    "text": "We can also do auto logging by using mlflow.&lt;framework&gt;.autolog()\n\n# mlflow.sklearn.autolog()\n\nAutologging is known to be compatible with the following package versions: 0.22.1 &lt;= scikit-learn &lt;= 1.2.2. Autologging may not succeed when used with package versions outside of this range."
  },
  {
    "objectID": "posts/mlflow/experiment_tracking.html#model-loading",
    "href": "posts/mlflow/experiment_tracking.html#model-loading",
    "title": "Experiment Tracking with MLflow",
    "section": "",
    "text": "We have saved the model using log_model. Now, we are going to load that model for prediction.\n\nfrom mlflow import MlflowClient\n\nclient = MlflowClient(tracking_uri=\"mlruns\")\n\n\nexperiments = client.search_experiments()\n\nfor experiment in experiments:\n    print(f\"Experiment Name : {experiment.name}\")\n    print(f\"\\tExperiment id :{experiment.experiment_id}\")\n    print(f\"\\tArtifact Location :{experiment.artifact_location}\\n\")\n\nExperiment Name : Experiment-1\n    Experiment id :280918629490989571\n    Artifact Location :/home/shane/mlops-practice/experiment-tracking/mlruns/280918629490989571\n\nExperiment Name : Default\n    Experiment id :0\n    Artifact Location :/home/shane/mlops-practice/experiment-tracking/mlruns/0\n\n\n\nOur experiment’s name is “Experiment-1” and the Experiment id for that is “146920015920581846”.\n\nfor experiment in experiments:\n    if experiment.name == \"Experiment-1\":\n        exp = experiment\n\n\nexp_id = exp.experiment_id\nruns = client.search_runs(experiment_ids=[exp_id])\n\nfor run in runs:\n    print(f\"run name : {run.info.run_name}\")\n    print(f\"\\trun id : {run.info.run_id}\")\n    print(f\"\\trmse : {run.data.metrics['RMSE']}\")\n\nrun name : charming-toad-155\n    run id : 9ed0390d8a4b44dca36f36ee25d20ba4\n    rmse : 6.282692349434918\n\n\nLet’s pick a run_id of the model that we want to load.\n\nrun_id = runs[0].info.run_id\nlogged_model = f'runs:/{run_id}/model'\n\n# Load model as a PyFuncModel.\nloaded_model = mlflow.pyfunc.load_model(logged_model)\n\n# Predict on a Pandas DataFrame.\nimport pandas as pd\nloaded_model.predict(X_val)\n\n2023/07/17 18:59:06 WARNING mlflow.pyfunc: Detected one or more mismatches between the model's dependencies and the current Python environment:\n - mlflow (current: 2.4.2, required: mlflow==2.4)\nTo fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n\n\narray([26.09040939, 16.31353026, 22.14756441, ..., 15.42763737,\n       14.91915344, 12.24746033])"
  },
  {
    "objectID": "posts/mlflow/experiment_tracking.html#model-registry",
    "href": "posts/mlflow/experiment_tracking.html#model-registry",
    "title": "Experiment Tracking with MLflow",
    "section": "",
    "text": "Model Registry is a centralized model store, a set of APIs and UI. It provides\n    - model lineage, \n    - model versioning, \n    - stage transition, and \n    - annotations.\n\nmlflow.set_tracking_uri('mlruns/')\n\nmodel_uri = f\"runs:/{run_id}/models\"\nmlflow.register_model(model_uri=model_uri, name=\"nyc-taxi-regressor\")\n\nSuccessfully registered model 'nyc-taxi-regressor'.\n2023/07/17 18:59:10 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: nyc-taxi-regressor, version 1\nCreated version '1' of model 'nyc-taxi-regressor'.\n\n\n&lt;ModelVersion: aliases=[], creation_timestamp=1689596950866, current_stage='None', description=None, last_updated_timestamp=1689596950866, name='nyc-taxi-regressor', run_id='9ed0390d8a4b44dca36f36ee25d20ba4', run_link=None, source='/home/shane/mlops-practice/experiment-tracking/mlruns/280918629490989571/9ed0390d8a4b44dca36f36ee25d20ba4/artifacts/models', status='READY', status_message=None, tags={}, user_id=None, version=1&gt;\n\n\nWe have now registered the model under the name of “nyc-taxi-regressor’.\nWe can see that the version of the model is now 1. Since this is the first model registered under this name.\nBut It doesn’t have any staging information.\n\nmodel_name = \"nyc-taxi-regressor\"\nlatest_versions = client.get_latest_versions(name=model_name)\n\nfor version in latest_versions:\n    print(f\"version: {version.version}, stage: {version.current_stage}\")\n\nversion: 1, stage: None\n\n\nWe can transition the model version and stages using transition_model_version_stage.\n\nmodel_version = 1\nnew_stage = \"Staging\"\nclient.transition_model_version_stage(\n    name=model_name,\n    version=model_version,\n    stage=new_stage,\n    archive_existing_versions=False\n)\n\n&lt;ModelVersion: aliases=[], creation_timestamp=1689596950866, current_stage='Staging', description=None, last_updated_timestamp=1689596955099, name='nyc-taxi-regressor', run_id='9ed0390d8a4b44dca36f36ee25d20ba4', run_link=None, source='/home/shane/mlops-practice/experiment-tracking/mlruns/280918629490989571/9ed0390d8a4b44dca36f36ee25d20ba4/artifacts/models', status='READY', status_message=None, tags={}, user_id=None, version=1&gt;\n\n\n\nmodel_name = \"nyc-taxi-regressor\"\nlatest_versions = client.get_latest_versions(name=model_name)\n\nfor version in latest_versions:\n    print(f\"version: {version.version}, stage: {version.current_stage}\")\n\nversion: 1, stage: Staging\n\n\nNow, we can see that the stage of the model is now Staging.\n\nfrom datetime import datetime\n\ndate = datetime.today().date()\nclient.update_model_version(\n    name=model_name,\n    version=model_version,\n    description=f\"The model version {model_version} was transitioned to {new_stage} on {date}\"\n)\n\n&lt;ModelVersion: aliases=[], creation_timestamp=1689596950866, current_stage='Staging', description='The model version 1 was transitioned to Staging on 2023-07-17', last_updated_timestamp=1689596958723, name='nyc-taxi-regressor', run_id='9ed0390d8a4b44dca36f36ee25d20ba4', run_link=None, source='/home/shane/mlops-practice/experiment-tracking/mlruns/280918629490989571/9ed0390d8a4b44dca36f36ee25d20ba4/artifacts/models', status='READY', status_message=None, tags={}, user_id=None, version=1&gt;\n\n\nWe could also add description of the models like datetime information above."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "These are just my notes about MLOps, Machine Learning, Deep Learning and Python.\nMy Name is Shane Ko Naung.I am a machine learning engineer from Myanmar."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "Data Preprocessing with Pandas\n\n\n\n\n\n\n\npandas\n\n\nd2l\n\n\n\n\n\n\n\n\n\n\n\nJul 20, 2023\n\n\nShane Ko Naung\n\n\n\n\n\n\n  \n\n\n\n\nData Manipulation with Pytorch\n\n\n\n\n\n\n\npytorch\n\n\nd2l\n\n\n\n\n\n\n\n\n\n\n\nJul 19, 2023\n\n\nShane Ko Naung\n\n\n\n\n\n\n  \n\n\n\n\nExperiment Tracking with MLflow\n\n\n\n\n\n\n\nmlops\n\n\nmlflow\n\n\n\n\n\n\n\n\n\n\n\nJul 17, 2023\n\n\nShane Ko Naung\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/data_manipulation_d2l/2.1-Data_Manipulation.html",
    "href": "posts/data_manipulation_d2l/2.1-Data_Manipulation.html",
    "title": "Data Manipulation with Pytorch",
    "section": "",
    "text": "Notes for chapter 2 section 1 of Dive into deep learning."
  },
  {
    "objectID": "posts/data_manipulation_d2l/2.1-Data_Manipulation.html#getting-started",
    "href": "posts/data_manipulation_d2l/2.1-Data_Manipulation.html#getting-started",
    "title": "Data Manipulation with Pytorch",
    "section": "Getting Started",
    "text": "Getting Started\n\nimport torch\n\n\ntensor : (possibly multi dimensional) array of numbers\ntensor with one axis : vector\ntensor with two axes : matrix\ntensor with more than two axes k &gt; 2 : kth order tensor\ncreate new tensors populated with values.\narange(n) : create a vector of evenly spaced values, starting at 0 ( included ) and ending at n ( not included ).\nBy default, the interval size is 1.\nstored in main memory for CPU based computations.\n\n\nx = torch.arange(12, dtype=torch.float32)\nx\n\ntensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])\n\n\n\nEach value is called element of tensor.\ncan inspect total number of elements using numel method.\n\n\nx.numel()\n\n12\n\n\n\ncan inspect shape of a tensor using shape attribute\n\n\nx.shape\n\ntorch.Size([12])\n\n\n\ncan inspect size of each dim or axis using size method\n\n\nx.size(dim=0)\n\n12\n\n\n\ncan change the shape of a tensor without altering its value using reshape method\nnew tensor becomes a matrix\n\n\nX = x.reshape(4,3)\nX, X.shape\n\n(tensor([[ 0.,  1.,  2.],\n         [ 3.,  4.,  5.],\n         [ 6.,  7.,  8.],\n         [ 9., 10., 11.]]),\n torch.Size([4, 3]))\n\n\n\nwe don’t need to specify all component of reshape.\nsince we already know the size of the tensor, we only need to specify one component.\n\n\nx.reshape(-1, 3), x.reshape(4, -1)\n\n(tensor([[ 0.,  1.,  2.],\n         [ 3.,  4.,  5.],\n         [ 6.,  7.,  8.],\n         [ 9., 10., 11.]]),\n tensor([[ 0.,  1.,  2.],\n         [ 3.,  4.,  5.],\n         [ 6.,  7.,  8.],\n         [ 9., 10., 11.]]))\n\n\n\ntensor initialized to contain all zeros or ones\n\n\ntorch.zeros((2,3,4)), torch.ones((2,3,4))\n\n(tensor([[[0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.]],\n \n         [[0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.]]]),\n tensor([[[1., 1., 1., 1.],\n          [1., 1., 1., 1.],\n          [1., 1., 1., 1.]],\n \n         [[1., 1., 1., 1.],\n          [1., 1., 1., 1.],\n          [1., 1., 1., 1.]]]))\n\n\n\ncan also initialzed tensor with elements drawn from a standard Gaussina (normal) distribution with mean 0 and standard deviation 1.\n\n\ntorch.randn(3, 4)\n\ntensor([[ 2.3716, -0.2123, -1.0150,  1.1659],\n        [-1.8146, -0.1514,  0.0144,  0.0529],\n        [-0.0288,  0.2103,  2.7825, -1.1707]])\n\n\n\nFinally, can also construct tensors by giving exact values using (nested) python lists\n\n\ntorch.tensor([[2, 3, 4], [2,4,5]])\n\ntensor([[2, 3, 4],\n        [2, 4, 5]])"
  },
  {
    "objectID": "posts/data_manipulation_d2l/2.1-Data_Manipulation.html#indexing-and-slicing",
    "href": "posts/data_manipulation_d2l/2.1-Data_Manipulation.html#indexing-and-slicing",
    "title": "Data Manipulation with Pytorch",
    "section": "Indexing and Slicing",
    "text": "Indexing and Slicing\n\nlike python list, we can access tensor elements by indexing (starting with 0)\nnegative indexing to access element based on its position relative to the end\ncan also access a whole range of elements by slicing (X [ start: end] ) , including start and not including end\nFinally, when only one index ( or slice ) is specified for a kth order tensor, it is applied along axis 0.\n\n\nX\n\ntensor([[ 0.,  1.,  2.],\n        [ 3.,  4.,  5.],\n        [ 6.,  7.,  8.],\n        [ 9., 10., 11.]])\n\n\n\nX[-1]\n\ntensor([ 9., 10., 11.])\n\n\n\nX[2:4]\n\ntensor([[ 6.,  7.,  8.],\n        [ 9., 10., 11.]])\n\n\n\nX[2,2]\n\ntensor(8.)\n\n\n\nX[2, :2]\n\ntensor([6., 7.])\n\n\n\ncan also write elements of a matrix by specifying indices\n\n\nX[2,2] = 34\n\n\nX\n\ntensor([[ 0.,  1.,  2.],\n        [ 3.,  4.,  5.],\n        [ 6.,  7., 34.],\n        [ 9., 10., 11.]])\n\n\n\nto assign multiple elements with the same value, we can use indexing on the left hand side\n\n\nX[:2, :] = 22\nX\n\ntensor([[22., 22., 22.],\n        [22., 22., 22.],\n        [ 6.,  7., 34.],\n        [ 9., 10., 11.]])"
  },
  {
    "objectID": "posts/data_manipulation_d2l/2.1-Data_Manipulation.html#operations",
    "href": "posts/data_manipulation_d2l/2.1-Data_Manipulation.html#operations",
    "title": "Data Manipulation with Pytorch",
    "section": "Operations",
    "text": "Operations\n\nelementwise operations : these apply a standard scalar operation to each element of the tensor\nunary scalar operator : taking one input\n\n\ntorch.exp(x)\n\ntensor([3.5849e+09, 3.5849e+09, 3.5849e+09, 3.5849e+09, 3.5849e+09, 3.5849e+09,\n        4.0343e+02, 1.0966e+03, 5.8346e+14, 8.1031e+03, 2.2026e+04, 5.9874e+04])\n\n\n\nbinary scalar operator : taking two inputs\n\n\nx = torch.tensor([1., 2., 4. , 6.])\ny = torch.tensor([2, 2, 2, 2])\n\nx + y, x - y, x * y, x / y, x ** y\n\n(tensor([3., 4., 6., 8.]),\n tensor([-1.,  0.,  2.,  4.]),\n tensor([ 2.,  4.,  8., 12.]),\n tensor([0.5000, 1.0000, 2.0000, 3.0000]),\n tensor([ 1.,  4., 16., 36.]))\n\n\n\nconcatenate multiple tensors together : stacking them end to end to form a larger tensor.\n\n\nX = torch.arange(12, dtype=torch.float32).reshape((3,4))\nY = torch.tensor([[2., 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\ntorch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)\n\n(tensor([[ 0.,  1.,  2.,  3.],\n         [ 4.,  5.,  6.,  7.],\n         [ 8.,  9., 10., 11.],\n         [ 2.,  1.,  4.,  3.],\n         [ 1.,  2.,  3.,  4.],\n         [ 4.,  3.,  2.,  1.]]),\n tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))\n\n\n\nconstruct binary tensor via logical statements\n\n\nX == Y\n\ntensor([[False,  True, False,  True],\n        [False, False, False, False],\n        [False, False, False, False]])\n\n\n\nX &lt; Y\n\ntensor([[ True, False,  True, False],\n        [False, False, False, False],\n        [False, False, False, False]])\n\n\n\nX &gt; Y\n\ntensor([[False, False, False, False],\n        [ True,  True,  True,  True],\n        [ True,  True,  True,  True]])\n\n\n\nSumming all the elements in the tensor using sum method\n\n\nX.sum()\n\ntensor(66.)"
  },
  {
    "objectID": "posts/data_manipulation_d2l/2.1-Data_Manipulation.html#broadcasting",
    "href": "posts/data_manipulation_d2l/2.1-Data_Manipulation.html#broadcasting",
    "title": "Data Manipulation with Pytorch",
    "section": "Broadcasting",
    "text": "Broadcasting\n\ncan now do elementwise binary operations on two tensors with the same shape\nBroadcasting\n\nunder certain conditions, can still perform elementwise binary operations on two tensors with different shapes\n\n\nexpand one or both arrays by copying elements along axes with length 1 so that after the transformation, the two tensors have the same shape.\nperform elementwise operations on the resulting arrays\n\n\n\n\n\na = torch.arange(3).reshape((3, 1))\nb = torch.arange(2).reshape((1,2))\na, b\n\n(tensor([[0],\n         [1],\n         [2]]),\n tensor([[0, 1]]))\n\n\n\na + b\n\ntensor([[0, 1],\n        [1, 2],\n        [2, 3]])\n\n\n\na = torch.arange(6).reshape((3, 2))\nb = torch.arange(9).reshape((3,3))\na, b\n\n(tensor([[0, 1],\n         [2, 3],\n         [4, 5]]),\n tensor([[0, 1, 2],\n         [3, 4, 5],\n         [6, 7, 8]]))\n\n\n\na + b\n\nRuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1\n\n\n\nsingleton dimensions are dimensions of size 1.\nWhat is singleton dimension of a tensor\n\n\na = torch.ones((2,3,3))\nb = torch.ones((1,1,3))\n\na, b\n\n(tensor([[[1., 1., 1.],\n          [1., 1., 1.],\n          [1., 1., 1.]],\n \n         [[1., 1., 1.],\n          [1., 1., 1.],\n          [1., 1., 1.]]]),\n tensor([[[1., 1., 1.]]]))\n\n\n\na + b\n\ntensor([[[2., 2., 2.],\n         [2., 2., 2.],\n         [2., 2., 2.]],\n\n        [[2., 2., 2.],\n         [2., 2., 2.],\n         [2., 2., 2.]]])\n\n\n\nhow-does-pytorch-broadcasting-work\nPytorch broadcasting semantics\nNumpy Broadcasting Rules"
  },
  {
    "objectID": "posts/data_manipulation_d2l/2.1-Data_Manipulation.html#saving-memory",
    "href": "posts/data_manipulation_d2l/2.1-Data_Manipulation.html#saving-memory",
    "title": "Data Manipulation with Pytorch",
    "section": "Saving Memory",
    "text": "Saving Memory\n\nrunning operations can cause new memories to be allocated to host results.\nby running Y = Y + X, python dereference the tensor that Y used to point to and instead point Y at the newly allocated memory.\n\n\nbefore = id(Y)\nY = Y + X\nid(Y) == before\n\nFalse\n\n\n\nThis is how to do operations in place.\nno unnecessary memeory allocations\nwe can avoid a memory leak or referring to stale parameters\n\n\nZ = torch.zeros_like(Y)\nprint(f'id(Z) : {id(Z)}')\nZ[:] = X + Y\nprint(f'id(Z) : {id(Z)}')\n\nid(Z) : 140466157769232\nid(Z) : 140466157769232\n\n\n\nbefore = id(X)\nX += Y\nid(X) == before\n\nTrue"
  },
  {
    "objectID": "posts/data_manipulation_d2l/2.1-Data_Manipulation.html#conversion-to-other-python-objects",
    "href": "posts/data_manipulation_d2l/2.1-Data_Manipulation.html#conversion-to-other-python-objects",
    "title": "Data Manipulation with Pytorch",
    "section": "Conversion to other python objects",
    "text": "Conversion to other python objects\n\nnumpy to torch , torch to numpy\n\n\nA = X.numpy()\nB = torch.from_numpy(A)\ntype(A), type(B)\n\n(numpy.ndarray, torch.Tensor)\n\n\n\nto convert size one torch tensor to python scalar\n\nitem function\npython built-in functions\n\n\n\na = torch.tensor([3.5])\na, a.item(), float(a), int(a)\n\n(tensor([3.5000]), 3.5, 3.5, 3)"
  }
]